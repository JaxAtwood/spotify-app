{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spotify_NN_TuningGridCV.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQZK1Q-wHMhr",
        "colab_type": "text"
      },
      "source": [
        "# NeuralNetwork."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po5At9QnVfo4",
        "colab_type": "text"
      },
      "source": [
        "### Data Preparation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xd6cgX-TTUV_",
        "colab_type": "code",
        "outputId": "02ede541-01ec-49ca-e7e9-641641ab129d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 61
        }
      },
      "source": [
        "# imports.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.metrics import top_k_categorical_accuracy\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW-fT6lVT9Tc",
        "colab_type": "code",
        "outputId": "6456fbe1-574d-4c64-b9af-117684e20bcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        }
      },
      "source": [
        "# read in the data.\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/spotify-song-suggester-1/spotify-app/master/ds/Data/SpotifyTracks_doubleforloop_genre_year.csv',index_col=[0])\n",
        "# show the data frame.\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(164449, 20)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>artist_name</th>\n",
              "      <th>track_name</th>\n",
              "      <th>track_id</th>\n",
              "      <th>popularity</th>\n",
              "      <th>year</th>\n",
              "      <th>genre</th>\n",
              "      <th>id</th>\n",
              "      <th>acousticness</th>\n",
              "      <th>danceability</th>\n",
              "      <th>duration_ms</th>\n",
              "      <th>energy</th>\n",
              "      <th>instrumentalness</th>\n",
              "      <th>key</th>\n",
              "      <th>liveness</th>\n",
              "      <th>loudness</th>\n",
              "      <th>mode</th>\n",
              "      <th>speechiness</th>\n",
              "      <th>tempo</th>\n",
              "      <th>time_signature</th>\n",
              "      <th>valence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Gorillaz</td>\n",
              "      <td>On Melancholy Hill</td>\n",
              "      <td>0q6LuUqGLUiCPP1cbdwFs3</td>\n",
              "      <td>75</td>\n",
              "      <td>2010</td>\n",
              "      <td>alternative</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.689</td>\n",
              "      <td>233867</td>\n",
              "      <td>0.739</td>\n",
              "      <td>0.509000</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0640</td>\n",
              "      <td>-5.810</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0260</td>\n",
              "      <td>120.423</td>\n",
              "      <td>4</td>\n",
              "      <td>0.578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Avenged Sevenfold</td>\n",
              "      <td>Nightmare</td>\n",
              "      <td>4UEo1b0wWrtHMC8bVqPiH8</td>\n",
              "      <td>70</td>\n",
              "      <td>2010</td>\n",
              "      <td>alternative</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000318</td>\n",
              "      <td>0.554</td>\n",
              "      <td>374453</td>\n",
              "      <td>0.949</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2000</td>\n",
              "      <td>-4.928</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0787</td>\n",
              "      <td>129.984</td>\n",
              "      <td>4</td>\n",
              "      <td>0.233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The Black Keys</td>\n",
              "      <td>Howlin' for You</td>\n",
              "      <td>0grFc6klR3hxoHLcgCYsF4</td>\n",
              "      <td>66</td>\n",
              "      <td>2010</td>\n",
              "      <td>alternative</td>\n",
              "      <td>2</td>\n",
              "      <td>0.028000</td>\n",
              "      <td>0.705</td>\n",
              "      <td>191800</td>\n",
              "      <td>0.735</td>\n",
              "      <td>0.078300</td>\n",
              "      <td>11</td>\n",
              "      <td>0.1120</td>\n",
              "      <td>-6.646</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0931</td>\n",
              "      <td>132.627</td>\n",
              "      <td>4</td>\n",
              "      <td>0.448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>My Darkest Days</td>\n",
              "      <td>Porn Star Dancing</td>\n",
              "      <td>3Q8zopc4ABXhysDb1sgLVW</td>\n",
              "      <td>65</td>\n",
              "      <td>2010</td>\n",
              "      <td>alternative</td>\n",
              "      <td>3</td>\n",
              "      <td>0.013900</td>\n",
              "      <td>0.477</td>\n",
              "      <td>199013</td>\n",
              "      <td>0.917</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0756</td>\n",
              "      <td>-3.399</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0837</td>\n",
              "      <td>160.044</td>\n",
              "      <td>4</td>\n",
              "      <td>0.271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Volbeat</td>\n",
              "      <td>A Warrior's Call</td>\n",
              "      <td>0hTiTU0yqthnByyZDD3bcc</td>\n",
              "      <td>62</td>\n",
              "      <td>2010</td>\n",
              "      <td>alternative</td>\n",
              "      <td>4</td>\n",
              "      <td>0.000750</td>\n",
              "      <td>0.374</td>\n",
              "      <td>263080</td>\n",
              "      <td>0.903</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>10</td>\n",
              "      <td>0.2440</td>\n",
              "      <td>-4.490</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0825</td>\n",
              "      <td>109.118</td>\n",
              "      <td>3</td>\n",
              "      <td>0.429</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         artist_name          track_name  ... time_signature  valence\n",
              "0           Gorillaz  On Melancholy Hill  ...              4    0.578\n",
              "1  Avenged Sevenfold           Nightmare  ...              4    0.233\n",
              "2     The Black Keys     Howlin' for You  ...              4    0.448\n",
              "3    My Darkest Days   Porn Star Dancing  ...              4    0.271\n",
              "4            Volbeat    A Warrior's Call  ...              3    0.429\n",
              "\n",
              "[5 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7cYOHfLykeM",
        "colab_type": "code",
        "outputId": "02ebead4-e0a0-4f74-c7c8-184a2167471b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        }
      },
      "source": [
        "# new data frame with encoded genre column.\n",
        "genreframe = pd.concat([df,pd.get_dummies(df['genre'], prefix='genre')],axis=1)\n",
        "# show the data frame.\n",
        "print(genreframe.shape)\n",
        "genreframe.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(164449, 32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>artist_name</th>\n",
              "      <th>track_name</th>\n",
              "      <th>track_id</th>\n",
              "      <th>popularity</th>\n",
              "      <th>year</th>\n",
              "      <th>genre</th>\n",
              "      <th>id</th>\n",
              "      <th>acousticness</th>\n",
              "      <th>danceability</th>\n",
              "      <th>duration_ms</th>\n",
              "      <th>energy</th>\n",
              "      <th>instrumentalness</th>\n",
              "      <th>key</th>\n",
              "      <th>liveness</th>\n",
              "      <th>loudness</th>\n",
              "      <th>mode</th>\n",
              "      <th>speechiness</th>\n",
              "      <th>tempo</th>\n",
              "      <th>time_signature</th>\n",
              "      <th>valence</th>\n",
              "      <th>genre_alternative</th>\n",
              "      <th>genre_country</th>\n",
              "      <th>genre_dance</th>\n",
              "      <th>genre_folk</th>\n",
              "      <th>genre_grunge</th>\n",
              "      <th>genre_indie</th>\n",
              "      <th>genre_jazz</th>\n",
              "      <th>genre_metal</th>\n",
              "      <th>genre_pop</th>\n",
              "      <th>genre_punk</th>\n",
              "      <th>genre_rap</th>\n",
              "      <th>genre_rock</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Gorillaz</td>\n",
              "      <td>On Melancholy Hill</td>\n",
              "      <td>0q6LuUqGLUiCPP1cbdwFs3</td>\n",
              "      <td>75</td>\n",
              "      <td>2010</td>\n",
              "      <td>alternative</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.689</td>\n",
              "      <td>233867</td>\n",
              "      <td>0.739</td>\n",
              "      <td>0.509000</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0640</td>\n",
              "      <td>-5.810</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0260</td>\n",
              "      <td>120.423</td>\n",
              "      <td>4</td>\n",
              "      <td>0.578</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Avenged Sevenfold</td>\n",
              "      <td>Nightmare</td>\n",
              "      <td>4UEo1b0wWrtHMC8bVqPiH8</td>\n",
              "      <td>70</td>\n",
              "      <td>2010</td>\n",
              "      <td>alternative</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000318</td>\n",
              "      <td>0.554</td>\n",
              "      <td>374453</td>\n",
              "      <td>0.949</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2000</td>\n",
              "      <td>-4.928</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0787</td>\n",
              "      <td>129.984</td>\n",
              "      <td>4</td>\n",
              "      <td>0.233</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The Black Keys</td>\n",
              "      <td>Howlin' for You</td>\n",
              "      <td>0grFc6klR3hxoHLcgCYsF4</td>\n",
              "      <td>66</td>\n",
              "      <td>2010</td>\n",
              "      <td>alternative</td>\n",
              "      <td>2</td>\n",
              "      <td>0.028000</td>\n",
              "      <td>0.705</td>\n",
              "      <td>191800</td>\n",
              "      <td>0.735</td>\n",
              "      <td>0.078300</td>\n",
              "      <td>11</td>\n",
              "      <td>0.1120</td>\n",
              "      <td>-6.646</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0931</td>\n",
              "      <td>132.627</td>\n",
              "      <td>4</td>\n",
              "      <td>0.448</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>My Darkest Days</td>\n",
              "      <td>Porn Star Dancing</td>\n",
              "      <td>3Q8zopc4ABXhysDb1sgLVW</td>\n",
              "      <td>65</td>\n",
              "      <td>2010</td>\n",
              "      <td>alternative</td>\n",
              "      <td>3</td>\n",
              "      <td>0.013900</td>\n",
              "      <td>0.477</td>\n",
              "      <td>199013</td>\n",
              "      <td>0.917</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0756</td>\n",
              "      <td>-3.399</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0837</td>\n",
              "      <td>160.044</td>\n",
              "      <td>4</td>\n",
              "      <td>0.271</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Volbeat</td>\n",
              "      <td>A Warrior's Call</td>\n",
              "      <td>0hTiTU0yqthnByyZDD3bcc</td>\n",
              "      <td>62</td>\n",
              "      <td>2010</td>\n",
              "      <td>alternative</td>\n",
              "      <td>4</td>\n",
              "      <td>0.000750</td>\n",
              "      <td>0.374</td>\n",
              "      <td>263080</td>\n",
              "      <td>0.903</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>10</td>\n",
              "      <td>0.2440</td>\n",
              "      <td>-4.490</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0825</td>\n",
              "      <td>109.118</td>\n",
              "      <td>3</td>\n",
              "      <td>0.429</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         artist_name          track_name  ... genre_rap  genre_rock\n",
              "0           Gorillaz  On Melancholy Hill  ...         0           0\n",
              "1  Avenged Sevenfold           Nightmare  ...         0           0\n",
              "2     The Black Keys     Howlin' for You  ...         0           0\n",
              "3    My Darkest Days   Porn Star Dancing  ...         0           0\n",
              "4            Volbeat    A Warrior's Call  ...         0           0\n",
              "\n",
              "[5 rows x 32 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLXpKFMn21FC",
        "colab_type": "code",
        "outputId": "f60f59f4-4cb0-42aa-dee3-47b93054c320",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "# show only the audio features.\n",
        "genreframe.iloc[0,7:20]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "acousticness        1.51e-05\n",
              "danceability           0.689\n",
              "duration_ms           233867\n",
              "energy                 0.739\n",
              "instrumentalness       0.509\n",
              "key                        2\n",
              "liveness               0.064\n",
              "loudness               -5.81\n",
              "mode                       1\n",
              "speechiness            0.026\n",
              "tempo                120.423\n",
              "time_signature             4\n",
              "valence                0.578\n",
              "Name: 0, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XxUNwzqq2sk",
        "colab_type": "code",
        "outputId": "1ee397f3-8729-445f-f80e-858aebe0e5cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "# set the X_train data on the audio features.\n",
        "X_train = genreframe.iloc[:,7:20].to_numpy()\n",
        "# set the standard scalar \n",
        "scaler = StandardScaler()\n",
        "# fit the scalar on the X_train data.\n",
        "X_scaled = scaler.fit_transform(X_train)\n",
        "# show the array.\n",
        "print(X_scaled.shape)\n",
        "X_scaled"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(164449, 13)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.81353437,  0.86273319,  0.01422737, ..., -0.07457545,\n",
              "         0.21919423,  0.52020022],\n",
              "       [-0.81253793,  0.02764277,  1.61395879, ...,  0.24680209,\n",
              "         0.21919423, -0.95936143],\n",
              "       [-0.72147348,  0.96170687, -0.46445403, ...,  0.33564226,\n",
              "         0.21919423, -0.03731576],\n",
              "       ...,\n",
              "       [ 0.35424623,  1.02975127,  0.17080273, ..., -0.0196848 ,\n",
              "         0.21919423,  1.42080297],\n",
              "       [-0.80624809, -0.62187201, -1.57989421, ..., -1.07440547,\n",
              "         0.21919423,  1.22781667],\n",
              "       [-0.42211418,  1.23388449, -0.24267685, ..., -0.99696024,\n",
              "         0.21919423, -0.50048289]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRCoXukYv3bI",
        "colab_type": "code",
        "outputId": "cf0a60bb-5b3d-47bb-e413-fe9ac04ef141",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        }
      },
      "source": [
        "# create a list of the genres.\n",
        "genres = genreframe.genre.unique().tolist()\n",
        "genres"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['alternative',\n",
              " 'country',\n",
              " 'dance',\n",
              " 'folk',\n",
              " 'grunge',\n",
              " 'indie',\n",
              " 'jazz',\n",
              " 'metal',\n",
              " 'pop',\n",
              " 'punk',\n",
              " 'rap',\n",
              " 'rock']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fohOUtlQs-0H",
        "colab_type": "code",
        "outputId": "f9e0ab61-c8ef-4259-f699-ffdbbb9a2648",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "source": [
        "# set the y_train as one hots.\n",
        "onehots = genreframe.iloc[:,20:]\n",
        "y_train = onehots.to_numpy()\n",
        "# show the array.\n",
        "print(y_train.shape)\n",
        "y_train"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(164449, 12)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0, ..., 0, 0, 0],\n",
              "       [1, 0, 0, ..., 0, 0, 0],\n",
              "       [1, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 1],\n",
              "       [0, 0, 0, ..., 0, 0, 1],\n",
              "       [0, 0, 0, ..., 0, 0, 1]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngIrkO9Lxjpa",
        "colab_type": "text"
      },
      "source": [
        "### Base Model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XFBrQTyUejP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function for NN model.\n",
        "def create_model():\n",
        "    # set the model.\n",
        "    model = Sequential()\n",
        "    # hidden layers.\n",
        "    model.add(Dense(48, input_dim=13, activation='relu'))\n",
        "    model.add(Dense(24, activation='relu'))\n",
        "    model.add(Dense(12,activation='softmax'))\n",
        "    # function for top 3.\n",
        "    def top_3_accuracy(y_true, y_pred):\n",
        "        return top_k_categorical_accuracy(y_true, y_pred, k=3)\n",
        "\n",
        "    # set the optimizer.\n",
        "    optimizer = Adam()\n",
        "    # compile the model.\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                        optimizer=optimizer,\n",
        "                        metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ygl6RH8CL2KN",
        "colab_type": "code",
        "outputId": "e06b48d6-c1c0-4340-9df0-840b561c337f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "# set the model.\n",
        "model = create_model()\n",
        "# show the model summary.\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_31 (Dense)             (None, 48)                672       \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 24)                1176      \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 12)                300       \n",
            "=================================================================\n",
            "Total params: 2,148\n",
            "Trainable params: 2,148\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eE8a9-JTL9Sj",
        "colab_type": "code",
        "outputId": "5389bacd-7de5-4ee6-e47a-70c884954e21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 713
        }
      },
      "source": [
        "# set the model.\n",
        "model = create_model()\n",
        "# fit the model and create history.\n",
        "history = model.fit(X_scaled, y_train, validation_split=0.2, batch_size=200, epochs=20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 131559 samples, validate on 32890 samples\n",
            "Epoch 1/20\n",
            "131559/131559 [==============================] - 2s 16us/sample - loss: 1.9870 - acc: 0.3157 - val_loss: 1.8878 - val_acc: 0.3702\n",
            "Epoch 2/20\n",
            "131559/131559 [==============================] - 2s 14us/sample - loss: 1.8758 - acc: 0.3520 - val_loss: 1.8723 - val_acc: 0.3759\n",
            "Epoch 3/20\n",
            "131559/131559 [==============================] - 2s 15us/sample - loss: 1.8548 - acc: 0.3587 - val_loss: 1.8591 - val_acc: 0.3787\n",
            "Epoch 4/20\n",
            "131559/131559 [==============================] - 2s 14us/sample - loss: 1.8418 - acc: 0.3628 - val_loss: 1.8454 - val_acc: 0.3843\n",
            "Epoch 5/20\n",
            "131559/131559 [==============================] - 2s 14us/sample - loss: 1.8338 - acc: 0.3656 - val_loss: 1.8537 - val_acc: 0.3771\n",
            "Epoch 6/20\n",
            "131559/131559 [==============================] - 2s 14us/sample - loss: 1.8275 - acc: 0.3676 - val_loss: 1.8480 - val_acc: 0.3808\n",
            "Epoch 7/20\n",
            "131559/131559 [==============================] - 2s 14us/sample - loss: 1.8230 - acc: 0.3687 - val_loss: 1.8445 - val_acc: 0.3834\n",
            "Epoch 8/20\n",
            "131559/131559 [==============================] - 2s 14us/sample - loss: 1.8190 - acc: 0.3706 - val_loss: 1.8429 - val_acc: 0.3811\n",
            "Epoch 9/20\n",
            "131559/131559 [==============================] - 2s 14us/sample - loss: 1.8158 - acc: 0.3711 - val_loss: 1.8307 - val_acc: 0.3847\n",
            "Epoch 10/20\n",
            "131559/131559 [==============================] - 2s 14us/sample - loss: 1.8133 - acc: 0.3719 - val_loss: 1.8471 - val_acc: 0.3791\n",
            "Epoch 11/20\n",
            "131559/131559 [==============================] - 2s 14us/sample - loss: 1.8107 - acc: 0.3728 - val_loss: 1.8405 - val_acc: 0.3819\n",
            "Epoch 12/20\n",
            "131559/131559 [==============================] - 2s 15us/sample - loss: 1.8080 - acc: 0.3742 - val_loss: 1.8415 - val_acc: 0.3812\n",
            "Epoch 13/20\n",
            "131559/131559 [==============================] - 2s 14us/sample - loss: 1.8063 - acc: 0.3750 - val_loss: 1.8336 - val_acc: 0.3863\n",
            "Epoch 14/20\n",
            "131559/131559 [==============================] - 2s 14us/sample - loss: 1.8046 - acc: 0.3738 - val_loss: 1.8249 - val_acc: 0.3876\n",
            "Epoch 15/20\n",
            "131559/131559 [==============================] - 2s 14us/sample - loss: 1.8030 - acc: 0.3753 - val_loss: 1.8294 - val_acc: 0.3861\n",
            "Epoch 16/20\n",
            "131559/131559 [==============================] - 2s 14us/sample - loss: 1.8014 - acc: 0.3750 - val_loss: 1.8196 - val_acc: 0.3889\n",
            "Epoch 17/20\n",
            "131559/131559 [==============================] - 2s 14us/sample - loss: 1.8003 - acc: 0.3756 - val_loss: 1.8289 - val_acc: 0.3859\n",
            "Epoch 18/20\n",
            "131559/131559 [==============================] - 2s 14us/sample - loss: 1.7988 - acc: 0.3762 - val_loss: 1.8201 - val_acc: 0.3919\n",
            "Epoch 19/20\n",
            "131559/131559 [==============================] - 2s 15us/sample - loss: 1.7976 - acc: 0.3765 - val_loss: 1.8208 - val_acc: 0.3919\n",
            "Epoch 20/20\n",
            "131559/131559 [==============================] - 2s 14us/sample - loss: 1.7962 - acc: 0.3774 - val_loss: 1.8207 - val_acc: 0.3900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UVyOXb-oePS",
        "colab_type": "code",
        "outputId": "37d25a00-6bc8-4145-ca99-cb569a31bfe2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# base model accuracy score.\n",
        "scores = model.evaluate(X_scaled, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "164449/164449 [==============================] - 9s 54us/sample - loss: 1.7989 - acc: 0.3808\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2EXMgzwVZ40",
        "colab_type": "text"
      },
      "source": [
        "#### Base Model Visuals."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWwPaekerboJ",
        "colab_type": "code",
        "outputId": "c6305e79-753d-4349-c0f8-87d2794ee7aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 795
        }
      },
      "source": [
        "# plot the model loss.\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
        "ax.plot(np.sqrt(history.history['loss']), 'r', label='train')\n",
        "ax.plot(np.sqrt(history.history['val_loss']), 'b' ,label='val')\n",
        "ax.set_xlabel(r'Epoch', fontsize=20)\n",
        "ax.set_ylabel(r'Loss', fontsize=20)\n",
        "ax.legend()\n",
        "ax.tick_params(labelsize=20)\n",
        "\n",
        "# plot the model accuracy.\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
        "ax.plot(np.sqrt(history.history['acc']), 'r', label='train')\n",
        "ax.plot(np.sqrt(history.history['val_acc']), 'b' ,label='val')\n",
        "ax.set_xlabel(r'Epoch', fontsize=20)\n",
        "ax.set_ylabel(r'Accuracy', fontsize=20)\n",
        "ax.legend()\n",
        "ax.tick_params(labelsize=20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoYAAAGFCAYAAABg9jJKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMzMzAxwYmpmZmVn2fyjEsicJXw6vAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAocAAAGFCAYAAACPNFl0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPaHbCdJPEF3",
        "colab_type": "text"
      },
      "source": [
        "### Optimizer. *(adam)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipA7pcOZQ92J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function for NN model.\n",
        "def create_model(optimizer='adam'):\n",
        "    # set the model.\n",
        "    model = Sequential()\n",
        "    # hidden layers.\n",
        "    model.add(Dense(48, input_dim=13, activation='relu'))\n",
        "    model.add(Dense(24, activation='relu'))\n",
        "    model.add(Dense(12,activation='softmax'))\n",
        "    # function for top 3.\n",
        "    def top_3_accuracy(y_true, y_pred):\n",
        "        return top_k_categorical_accuracy(y_true, y_pred, k=3)\n",
        "\n",
        "    # compile the model.\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                        optimizer='adam',\n",
        "                        metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gURgxs91Q94_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set the model with KerasClassifier.\n",
        "model = KerasClassifier(build_fn=create_model, epochs=20, batch_size=200, validation_split=0.2, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A77OZ4Q3Q98W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set the parameters for optimizer.\n",
        "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
        "param_grid = dict(optimizer=optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OU3wcEbKREzp",
        "colab_type": "code",
        "outputId": "86c8f2ef-0ff6-4f35-ca35-942ea716e196",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# set the gridsearch.\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
        "grid_result = grid.fit(X_scaled, y_train)\n",
        "# show the results.\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 2.0175 - acc: 0.3156 - val_loss: 1.8565 - val_acc: 0.3758\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 15us/sample - loss: 1.8808 - acc: 0.3525 - val_loss: 1.8409 - val_acc: 0.3745\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 15us/sample - loss: 1.8595 - acc: 0.3581 - val_loss: 1.8244 - val_acc: 0.3846\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8456 - acc: 0.3642 - val_loss: 1.8191 - val_acc: 0.3829\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 15us/sample - loss: 1.8363 - acc: 0.3666 - val_loss: 1.8120 - val_acc: 0.3878\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8293 - acc: 0.3689 - val_loss: 1.8102 - val_acc: 0.3876\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 15us/sample - loss: 1.8236 - acc: 0.3706 - val_loss: 1.8054 - val_acc: 0.3889\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 15us/sample - loss: 1.8191 - acc: 0.3722 - val_loss: 1.7917 - val_acc: 0.3936\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8153 - acc: 0.3734 - val_loss: 1.8030 - val_acc: 0.3921\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 15us/sample - loss: 1.8123 - acc: 0.3744 - val_loss: 1.7947 - val_acc: 0.3940\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 15us/sample - loss: 1.8094 - acc: 0.3762 - val_loss: 1.7908 - val_acc: 0.3941\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8069 - acc: 0.3770 - val_loss: 1.7905 - val_acc: 0.3958\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 15us/sample - loss: 1.8053 - acc: 0.3779 - val_loss: 1.8004 - val_acc: 0.3910\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 15us/sample - loss: 1.8034 - acc: 0.3771 - val_loss: 1.7889 - val_acc: 0.3973\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 15us/sample - loss: 1.8022 - acc: 0.3783 - val_loss: 1.7939 - val_acc: 0.3946\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 15us/sample - loss: 1.8006 - acc: 0.3783 - val_loss: 1.7907 - val_acc: 0.3975\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 15us/sample - loss: 1.7990 - acc: 0.3799 - val_loss: 1.7944 - val_acc: 0.3925\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.7976 - acc: 0.3796 - val_loss: 1.7920 - val_acc: 0.3953\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.7964 - acc: 0.3801 - val_loss: 1.7933 - val_acc: 0.3975\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 15us/sample - loss: 1.7955 - acc: 0.3807 - val_loss: 1.7923 - val_acc: 0.3970\n",
            "32890/32890 [==============================] - 0s 10us/sample - loss: 1.8459 - acc: 0.3614\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.9951 - acc: 0.3162 - val_loss: 1.8584 - val_acc: 0.3754\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 15us/sample - loss: 1.8619 - acc: 0.3625 - val_loss: 1.8368 - val_acc: 0.3823\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8402 - acc: 0.3695 - val_loss: 1.8500 - val_acc: 0.3736\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 15us/sample - loss: 1.8290 - acc: 0.3736 - val_loss: 1.8161 - val_acc: 0.3878\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8208 - acc: 0.3756 - val_loss: 1.8149 - val_acc: 0.3873\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 15us/sample - loss: 1.8145 - acc: 0.3778 - val_loss: 1.8129 - val_acc: 0.3894\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8096 - acc: 0.3796 - val_loss: 1.8040 - val_acc: 0.3886\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8056 - acc: 0.3800 - val_loss: 1.7932 - val_acc: 0.3920\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8017 - acc: 0.3824 - val_loss: 1.7955 - val_acc: 0.3943\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.7982 - acc: 0.3824 - val_loss: 1.7904 - val_acc: 0.3952\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 15us/sample - loss: 1.7953 - acc: 0.3839 - val_loss: 1.8005 - val_acc: 0.3903\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 15us/sample - loss: 1.7928 - acc: 0.3837 - val_loss: 1.7949 - val_acc: 0.3935\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.7906 - acc: 0.3853 - val_loss: 1.8075 - val_acc: 0.3870\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.7887 - acc: 0.3860 - val_loss: 1.8018 - val_acc: 0.3902\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.7862 - acc: 0.3867 - val_loss: 1.7899 - val_acc: 0.3969\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.7850 - acc: 0.3867 - val_loss: 1.7914 - val_acc: 0.3962\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.7830 - acc: 0.3875 - val_loss: 1.7986 - val_acc: 0.3927\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.7819 - acc: 0.3888 - val_loss: 1.8042 - val_acc: 0.3855\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.7807 - acc: 0.3884 - val_loss: 1.8062 - val_acc: 0.3856\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.7792 - acc: 0.3894 - val_loss: 1.7862 - val_acc: 0.3970\n",
            "32890/32890 [==============================] - 0s 10us/sample - loss: 1.8829 - acc: 0.3412\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 2.0255 - acc: 0.3048 - val_loss: 1.9008 - val_acc: 0.3485\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8922 - acc: 0.3475 - val_loss: 1.8538 - val_acc: 0.3695\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8690 - acc: 0.3543 - val_loss: 1.8534 - val_acc: 0.3714\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8561 - acc: 0.3591 - val_loss: 1.8419 - val_acc: 0.3772\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8470 - acc: 0.3617 - val_loss: 1.8234 - val_acc: 0.3822\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8401 - acc: 0.3648 - val_loss: 1.8202 - val_acc: 0.3820\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8342 - acc: 0.3654 - val_loss: 1.8300 - val_acc: 0.3769\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8299 - acc: 0.3676 - val_loss: 1.8214 - val_acc: 0.3810\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8264 - acc: 0.3677 - val_loss: 1.8294 - val_acc: 0.3781\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8235 - acc: 0.3697 - val_loss: 1.8091 - val_acc: 0.3900\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8209 - acc: 0.3697 - val_loss: 1.8204 - val_acc: 0.3832\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8187 - acc: 0.3709 - val_loss: 1.8131 - val_acc: 0.3881\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8167 - acc: 0.3713 - val_loss: 1.8204 - val_acc: 0.3824\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8148 - acc: 0.3720 - val_loss: 1.8183 - val_acc: 0.3838\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8131 - acc: 0.3727 - val_loss: 1.8099 - val_acc: 0.3861\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8116 - acc: 0.3733 - val_loss: 1.8231 - val_acc: 0.3820\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8104 - acc: 0.3747 - val_loss: 1.8118 - val_acc: 0.3845\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8088 - acc: 0.3745 - val_loss: 1.8151 - val_acc: 0.3848\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8078 - acc: 0.3748 - val_loss: 1.8200 - val_acc: 0.3822\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8068 - acc: 0.3758 - val_loss: 1.8153 - val_acc: 0.3823\n",
            "32890/32890 [==============================] - 0s 11us/sample - loss: 1.7962 - acc: 0.3854\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 2.0212 - acc: 0.3075 - val_loss: 1.9093 - val_acc: 0.3556\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8872 - acc: 0.3504 - val_loss: 1.8681 - val_acc: 0.3718\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8631 - acc: 0.3573 - val_loss: 1.8556 - val_acc: 0.3736\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8481 - acc: 0.3626 - val_loss: 1.8577 - val_acc: 0.3704\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8382 - acc: 0.3667 - val_loss: 1.8358 - val_acc: 0.3819\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8313 - acc: 0.3690 - val_loss: 1.8508 - val_acc: 0.3699\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8256 - acc: 0.3713 - val_loss: 1.8250 - val_acc: 0.3856\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8208 - acc: 0.3733 - val_loss: 1.8278 - val_acc: 0.3836\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8179 - acc: 0.3742 - val_loss: 1.8296 - val_acc: 0.3801\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8147 - acc: 0.3748 - val_loss: 1.8366 - val_acc: 0.3797\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8118 - acc: 0.3762 - val_loss: 1.8282 - val_acc: 0.3814\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8099 - acc: 0.3765 - val_loss: 1.8432 - val_acc: 0.3763\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8075 - acc: 0.3774 - val_loss: 1.8251 - val_acc: 0.3837\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8059 - acc: 0.3785 - val_loss: 1.8219 - val_acc: 0.3853\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8044 - acc: 0.3779 - val_loss: 1.8091 - val_acc: 0.3898\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8027 - acc: 0.3786 - val_loss: 1.8136 - val_acc: 0.3880\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8015 - acc: 0.3803 - val_loss: 1.8170 - val_acc: 0.3863\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8000 - acc: 0.3805 - val_loss: 1.8200 - val_acc: 0.3850\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.7987 - acc: 0.3808 - val_loss: 1.8252 - val_acc: 0.3807\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.7975 - acc: 0.3804 - val_loss: 1.8222 - val_acc: 0.3814\n",
            "32890/32890 [==============================] - 0s 10us/sample - loss: 1.8295 - acc: 0.3654\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 2s 23us/sample - loss: 2.0103 - acc: 0.3148 - val_loss: 1.8972 - val_acc: 0.3419\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 2s 16us/sample - loss: 1.8846 - acc: 0.3516 - val_loss: 1.8668 - val_acc: 0.3441\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 2s 16us/sample - loss: 1.8644 - acc: 0.3560 - val_loss: 1.8698 - val_acc: 0.3412\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8525 - acc: 0.3592 - val_loss: 1.8317 - val_acc: 0.3572\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 2s 16us/sample - loss: 1.8439 - acc: 0.3624 - val_loss: 1.8485 - val_acc: 0.3474\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 2s 16us/sample - loss: 1.8375 - acc: 0.3634 - val_loss: 1.8289 - val_acc: 0.3573\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 2s 16us/sample - loss: 1.8329 - acc: 0.3651 - val_loss: 1.8277 - val_acc: 0.3539\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 2s 16us/sample - loss: 1.8281 - acc: 0.3668 - val_loss: 1.8274 - val_acc: 0.3572\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 2s 16us/sample - loss: 1.8240 - acc: 0.3678 - val_loss: 1.8212 - val_acc: 0.3609\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 2s 16us/sample - loss: 1.8208 - acc: 0.3691 - val_loss: 1.8350 - val_acc: 0.3531\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 2s 16us/sample - loss: 1.8182 - acc: 0.3712 - val_loss: 1.8299 - val_acc: 0.3526\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 2s 16us/sample - loss: 1.8156 - acc: 0.3716 - val_loss: 1.8134 - val_acc: 0.3620\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 2s 16us/sample - loss: 1.8134 - acc: 0.3714 - val_loss: 1.8071 - val_acc: 0.3679\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 2s 18us/sample - loss: 1.8116 - acc: 0.3730 - val_loss: 1.8034 - val_acc: 0.3647\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 2s 16us/sample - loss: 1.8093 - acc: 0.3739 - val_loss: 1.8148 - val_acc: 0.3626\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 2s 16us/sample - loss: 1.8082 - acc: 0.3745 - val_loss: 1.8029 - val_acc: 0.3680\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 2s 16us/sample - loss: 1.8060 - acc: 0.3753 - val_loss: 1.8170 - val_acc: 0.3578\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 2s 16us/sample - loss: 1.8047 - acc: 0.3766 - val_loss: 1.8044 - val_acc: 0.3669\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 2s 16us/sample - loss: 1.8035 - acc: 0.3764 - val_loss: 1.7975 - val_acc: 0.3724\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 2s 16us/sample - loss: 1.8022 - acc: 0.3777 - val_loss: 1.8035 - val_acc: 0.3674\n",
            "32889/32889 [==============================] - 0s 10us/sample - loss: 1.8280 - acc: 0.3864\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 2.0159 - acc: 0.3067 - val_loss: 1.8682 - val_acc: 0.3706\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8805 - acc: 0.3549 - val_loss: 1.8405 - val_acc: 0.3760\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8596 - acc: 0.3603 - val_loss: 1.8191 - val_acc: 0.3835\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8470 - acc: 0.3635 - val_loss: 1.8124 - val_acc: 0.3884\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8386 - acc: 0.3660 - val_loss: 1.8169 - val_acc: 0.3843\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8321 - acc: 0.3675 - val_loss: 1.8052 - val_acc: 0.3880\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8272 - acc: 0.3694 - val_loss: 1.8087 - val_acc: 0.3873\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8223 - acc: 0.3712 - val_loss: 1.8015 - val_acc: 0.3887\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8195 - acc: 0.3724 - val_loss: 1.8001 - val_acc: 0.3897\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8156 - acc: 0.3735 - val_loss: 1.7920 - val_acc: 0.3944\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8130 - acc: 0.3740 - val_loss: 1.7947 - val_acc: 0.3910\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8104 - acc: 0.3744 - val_loss: 1.8052 - val_acc: 0.3881\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8083 - acc: 0.3758 - val_loss: 1.7886 - val_acc: 0.3963\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8060 - acc: 0.3767 - val_loss: 1.7827 - val_acc: 0.3963\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8038 - acc: 0.3773 - val_loss: 1.7829 - val_acc: 0.3992\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8022 - acc: 0.3781 - val_loss: 1.7760 - val_acc: 0.4035\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8005 - acc: 0.3791 - val_loss: 1.7911 - val_acc: 0.3940\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.7992 - acc: 0.3785 - val_loss: 1.7882 - val_acc: 0.3954\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.7978 - acc: 0.3803 - val_loss: 1.7944 - val_acc: 0.3947\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.7965 - acc: 0.3802 - val_loss: 1.7860 - val_acc: 0.3978\n",
            "32890/32890 [==============================] - 0s 10us/sample - loss: 1.8415 - acc: 0.3630\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 2s 24us/sample - loss: 2.0031 - acc: 0.3199 - val_loss: 1.8613 - val_acc: 0.3760\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8633 - acc: 0.3622 - val_loss: 1.8329 - val_acc: 0.3806\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8432 - acc: 0.3687 - val_loss: 1.8322 - val_acc: 0.3824\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8311 - acc: 0.3728 - val_loss: 1.8163 - val_acc: 0.3891\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8234 - acc: 0.3747 - val_loss: 1.8109 - val_acc: 0.3868\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8169 - acc: 0.3772 - val_loss: 1.8185 - val_acc: 0.3790\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8125 - acc: 0.3775 - val_loss: 1.8077 - val_acc: 0.3880\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8081 - acc: 0.3795 - val_loss: 1.8144 - val_acc: 0.3837\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8051 - acc: 0.3809 - val_loss: 1.8142 - val_acc: 0.3870\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8019 - acc: 0.3818 - val_loss: 1.8064 - val_acc: 0.3874\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.7995 - acc: 0.3818 - val_loss: 1.8210 - val_acc: 0.3786\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.7973 - acc: 0.3829 - val_loss: 1.8032 - val_acc: 0.3869\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.7956 - acc: 0.3837 - val_loss: 1.8008 - val_acc: 0.3867\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.7935 - acc: 0.3842 - val_loss: 1.8008 - val_acc: 0.3879\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.7918 - acc: 0.3852 - val_loss: 1.7989 - val_acc: 0.3915\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.7901 - acc: 0.3861 - val_loss: 1.7961 - val_acc: 0.3928\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.7890 - acc: 0.3853 - val_loss: 1.7946 - val_acc: 0.3928\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.7872 - acc: 0.3863 - val_loss: 1.8112 - val_acc: 0.3864\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.7858 - acc: 0.3879 - val_loss: 1.8005 - val_acc: 0.3915\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.7846 - acc: 0.3870 - val_loss: 1.8004 - val_acc: 0.3895\n",
            "32890/32890 [==============================] - 0s 10us/sample - loss: 1.8944 - acc: 0.3369\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 2.0203 - acc: 0.3088 - val_loss: 1.8906 - val_acc: 0.3646\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8921 - acc: 0.3488 - val_loss: 1.8562 - val_acc: 0.3768\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8729 - acc: 0.3546 - val_loss: 1.8403 - val_acc: 0.3800\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8601 - acc: 0.3572 - val_loss: 1.8409 - val_acc: 0.3755\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8503 - acc: 0.3603 - val_loss: 1.8368 - val_acc: 0.3794\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8425 - acc: 0.3628 - val_loss: 1.8323 - val_acc: 0.3775\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8354 - acc: 0.3662 - val_loss: 1.8205 - val_acc: 0.3818\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8310 - acc: 0.3666 - val_loss: 1.8213 - val_acc: 0.3808\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8261 - acc: 0.3692 - val_loss: 1.8180 - val_acc: 0.3816\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8226 - acc: 0.3695 - val_loss: 1.8202 - val_acc: 0.3791\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8201 - acc: 0.3712 - val_loss: 1.8002 - val_acc: 0.3893\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8174 - acc: 0.3728 - val_loss: 1.8096 - val_acc: 0.3820\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8156 - acc: 0.3733 - val_loss: 1.8139 - val_acc: 0.3797\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8136 - acc: 0.3731 - val_loss: 1.8165 - val_acc: 0.3825\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8118 - acc: 0.3743 - val_loss: 1.8234 - val_acc: 0.3780\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8104 - acc: 0.3740 - val_loss: 1.8141 - val_acc: 0.3817\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8089 - acc: 0.3748 - val_loss: 1.8251 - val_acc: 0.3767\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8075 - acc: 0.3752 - val_loss: 1.8042 - val_acc: 0.3862\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8063 - acc: 0.3753 - val_loss: 1.8100 - val_acc: 0.3829\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8051 - acc: 0.3764 - val_loss: 1.8161 - val_acc: 0.3807\n",
            "32890/32890 [==============================] - 0s 12us/sample - loss: 1.7986 - acc: 0.3822\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 2s 24us/sample - loss: 2.0008 - acc: 0.3115 - val_loss: 1.9046 - val_acc: 0.3508\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8796 - acc: 0.3544 - val_loss: 1.8740 - val_acc: 0.3668\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8608 - acc: 0.3605 - val_loss: 1.8507 - val_acc: 0.3732\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8488 - acc: 0.3645 - val_loss: 1.8486 - val_acc: 0.3717\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8408 - acc: 0.3661 - val_loss: 1.8593 - val_acc: 0.3722\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8348 - acc: 0.3683 - val_loss: 1.8352 - val_acc: 0.3804\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8300 - acc: 0.3690 - val_loss: 1.8254 - val_acc: 0.3861\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8258 - acc: 0.3705 - val_loss: 1.8521 - val_acc: 0.3720\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8219 - acc: 0.3722 - val_loss: 1.8434 - val_acc: 0.3756\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8183 - acc: 0.3724 - val_loss: 1.8390 - val_acc: 0.3789\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8159 - acc: 0.3726 - val_loss: 1.8329 - val_acc: 0.3778\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8126 - acc: 0.3741 - val_loss: 1.8409 - val_acc: 0.3748\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8107 - acc: 0.3753 - val_loss: 1.8302 - val_acc: 0.3812\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8083 - acc: 0.3758 - val_loss: 1.8315 - val_acc: 0.3777\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8061 - acc: 0.3764 - val_loss: 1.8222 - val_acc: 0.3867\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8044 - acc: 0.3769 - val_loss: 1.8259 - val_acc: 0.3836\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8024 - acc: 0.3778 - val_loss: 1.8394 - val_acc: 0.3784\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8014 - acc: 0.3781 - val_loss: 1.8201 - val_acc: 0.3866\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.7999 - acc: 0.3781 - val_loss: 1.8261 - val_acc: 0.3852\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.7987 - acc: 0.3788 - val_loss: 1.8224 - val_acc: 0.3840\n",
            "32890/32890 [==============================] - 0s 11us/sample - loss: 1.8346 - acc: 0.3647\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 3s 25us/sample - loss: 2.0305 - acc: 0.2978 - val_loss: 1.9050 - val_acc: 0.3414\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8856 - acc: 0.3520 - val_loss: 1.8713 - val_acc: 0.3495\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 2s 16us/sample - loss: 1.8648 - acc: 0.3573 - val_loss: 1.8559 - val_acc: 0.3530\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 2s 16us/sample - loss: 1.8528 - acc: 0.3620 - val_loss: 1.8448 - val_acc: 0.3552\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8439 - acc: 0.3632 - val_loss: 1.8290 - val_acc: 0.3605\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8373 - acc: 0.3655 - val_loss: 1.8371 - val_acc: 0.3583\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8321 - acc: 0.3680 - val_loss: 1.8333 - val_acc: 0.3546\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 2s 16us/sample - loss: 1.8278 - acc: 0.3680 - val_loss: 1.8179 - val_acc: 0.3651\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8241 - acc: 0.3706 - val_loss: 1.8262 - val_acc: 0.3617\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8209 - acc: 0.3712 - val_loss: 1.8080 - val_acc: 0.3690\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 2s 18us/sample - loss: 1.8182 - acc: 0.3719 - val_loss: 1.8193 - val_acc: 0.3624\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 2s 16us/sample - loss: 1.8161 - acc: 0.3726 - val_loss: 1.8130 - val_acc: 0.3655\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 2s 16us/sample - loss: 1.8137 - acc: 0.3728 - val_loss: 1.8143 - val_acc: 0.3640\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8122 - acc: 0.3729 - val_loss: 1.8203 - val_acc: 0.3590\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 2s 16us/sample - loss: 1.8103 - acc: 0.3734 - val_loss: 1.8110 - val_acc: 0.3694\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8085 - acc: 0.3755 - val_loss: 1.8104 - val_acc: 0.3680\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 2s 16us/sample - loss: 1.8073 - acc: 0.3750 - val_loss: 1.8067 - val_acc: 0.3693\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8061 - acc: 0.3750 - val_loss: 1.8190 - val_acc: 0.3623\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 2s 16us/sample - loss: 1.8046 - acc: 0.3755 - val_loss: 1.8072 - val_acc: 0.3697\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 2s 16us/sample - loss: 1.8033 - acc: 0.3768 - val_loss: 1.8056 - val_acc: 0.3717\n",
            "32889/32889 [==============================] - 0s 11us/sample - loss: 1.8220 - acc: 0.3905\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 2.0330 - acc: 0.3093 - val_loss: 1.8555 - val_acc: 0.3728\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8814 - acc: 0.3530 - val_loss: 1.8367 - val_acc: 0.3826\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8624 - acc: 0.3583 - val_loss: 1.8184 - val_acc: 0.3901\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8504 - acc: 0.3618 - val_loss: 1.8281 - val_acc: 0.3829\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8420 - acc: 0.3646 - val_loss: 1.8064 - val_acc: 0.3919\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8345 - acc: 0.3665 - val_loss: 1.8145 - val_acc: 0.3853\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8283 - acc: 0.3684 - val_loss: 1.8033 - val_acc: 0.3936\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8238 - acc: 0.3704 - val_loss: 1.8092 - val_acc: 0.3873\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8194 - acc: 0.3717 - val_loss: 1.8064 - val_acc: 0.3877\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8157 - acc: 0.3735 - val_loss: 1.8000 - val_acc: 0.3902\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8127 - acc: 0.3746 - val_loss: 1.7954 - val_acc: 0.3928\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8098 - acc: 0.3755 - val_loss: 1.7976 - val_acc: 0.3926\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8078 - acc: 0.3756 - val_loss: 1.7912 - val_acc: 0.3947\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8055 - acc: 0.3780 - val_loss: 1.7958 - val_acc: 0.3928\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8038 - acc: 0.3777 - val_loss: 1.7953 - val_acc: 0.3931\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8022 - acc: 0.3780 - val_loss: 1.7905 - val_acc: 0.3954\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8008 - acc: 0.3779 - val_loss: 1.8027 - val_acc: 0.3903\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.7993 - acc: 0.3784 - val_loss: 1.7985 - val_acc: 0.3905\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.7980 - acc: 0.3795 - val_loss: 1.7965 - val_acc: 0.3937\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.7967 - acc: 0.3792 - val_loss: 1.7889 - val_acc: 0.3964\n",
            "32890/32890 [==============================] - 0s 11us/sample - loss: 1.8664 - acc: 0.3536\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 2.0134 - acc: 0.3135 - val_loss: 1.8797 - val_acc: 0.3647\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8675 - acc: 0.3629 - val_loss: 1.8583 - val_acc: 0.3728\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8435 - acc: 0.3684 - val_loss: 1.8242 - val_acc: 0.3855\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8302 - acc: 0.3719 - val_loss: 1.8232 - val_acc: 0.3834\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8212 - acc: 0.3742 - val_loss: 1.8317 - val_acc: 0.3816\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8150 - acc: 0.3770 - val_loss: 1.8201 - val_acc: 0.3851\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8101 - acc: 0.3783 - val_loss: 1.8169 - val_acc: 0.3895\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8063 - acc: 0.3800 - val_loss: 1.8176 - val_acc: 0.3852\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8027 - acc: 0.3821 - val_loss: 1.8056 - val_acc: 0.3871\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.7995 - acc: 0.3825 - val_loss: 1.7987 - val_acc: 0.3920\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.7970 - acc: 0.3836 - val_loss: 1.8021 - val_acc: 0.3881\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.7946 - acc: 0.3847 - val_loss: 1.8042 - val_acc: 0.3872\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.7922 - acc: 0.3853 - val_loss: 1.7968 - val_acc: 0.3912\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.7902 - acc: 0.3854 - val_loss: 1.7925 - val_acc: 0.3956\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.7883 - acc: 0.3853 - val_loss: 1.8000 - val_acc: 0.3884\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.7870 - acc: 0.3862 - val_loss: 1.7939 - val_acc: 0.3936\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.7853 - acc: 0.3871 - val_loss: 1.7972 - val_acc: 0.3941\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.7839 - acc: 0.3873 - val_loss: 1.8052 - val_acc: 0.3891\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.7826 - acc: 0.3879 - val_loss: 1.8025 - val_acc: 0.3851\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.7813 - acc: 0.3883 - val_loss: 1.7804 - val_acc: 0.3979\n",
            "32890/32890 [==============================] - 0s 11us/sample - loss: 1.8954 - acc: 0.3379\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 2.0290 - acc: 0.3047 - val_loss: 1.8831 - val_acc: 0.3614\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8874 - acc: 0.3502 - val_loss: 1.8507 - val_acc: 0.3753\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8675 - acc: 0.3551 - val_loss: 1.8508 - val_acc: 0.3720\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8563 - acc: 0.3586 - val_loss: 1.8449 - val_acc: 0.3747\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8482 - acc: 0.3617 - val_loss: 1.8416 - val_acc: 0.3750\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8419 - acc: 0.3641 - val_loss: 1.8310 - val_acc: 0.3784\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8373 - acc: 0.3647 - val_loss: 1.8265 - val_acc: 0.3850\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8329 - acc: 0.3662 - val_loss: 1.8346 - val_acc: 0.3757\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8295 - acc: 0.3685 - val_loss: 1.8217 - val_acc: 0.3818\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8267 - acc: 0.3685 - val_loss: 1.8195 - val_acc: 0.3828\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8239 - acc: 0.3698 - val_loss: 1.8145 - val_acc: 0.3859\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8215 - acc: 0.3697 - val_loss: 1.8221 - val_acc: 0.3812\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8193 - acc: 0.3705 - val_loss: 1.8347 - val_acc: 0.3755\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8176 - acc: 0.3719 - val_loss: 1.8175 - val_acc: 0.3837\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8160 - acc: 0.3720 - val_loss: 1.8057 - val_acc: 0.3896\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8140 - acc: 0.3720 - val_loss: 1.8224 - val_acc: 0.3801\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8130 - acc: 0.3731 - val_loss: 1.8195 - val_acc: 0.3834\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8114 - acc: 0.3744 - val_loss: 1.8182 - val_acc: 0.3808\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8104 - acc: 0.3735 - val_loss: 1.8171 - val_acc: 0.3814\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8092 - acc: 0.3739 - val_loss: 1.8160 - val_acc: 0.3795\n",
            "32890/32890 [==============================] - 0s 11us/sample - loss: 1.8057 - acc: 0.3772\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 2.0151 - acc: 0.3081 - val_loss: 1.9025 - val_acc: 0.3539\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8857 - acc: 0.3517 - val_loss: 1.8926 - val_acc: 0.3502\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8642 - acc: 0.3585 - val_loss: 1.8543 - val_acc: 0.3752\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8521 - acc: 0.3621 - val_loss: 1.8409 - val_acc: 0.3815\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8429 - acc: 0.3654 - val_loss: 1.8458 - val_acc: 0.3798\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8359 - acc: 0.3667 - val_loss: 1.8610 - val_acc: 0.3700\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8303 - acc: 0.3689 - val_loss: 1.8422 - val_acc: 0.3790\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8257 - acc: 0.3709 - val_loss: 1.8432 - val_acc: 0.3766\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8221 - acc: 0.3720 - val_loss: 1.8365 - val_acc: 0.3823\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8188 - acc: 0.3729 - val_loss: 1.8426 - val_acc: 0.3771\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8151 - acc: 0.3751 - val_loss: 1.8232 - val_acc: 0.3858\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8129 - acc: 0.3753 - val_loss: 1.8397 - val_acc: 0.3786\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8102 - acc: 0.3758 - val_loss: 1.8431 - val_acc: 0.3772\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8080 - acc: 0.3778 - val_loss: 1.8398 - val_acc: 0.3789\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8066 - acc: 0.3778 - val_loss: 1.8265 - val_acc: 0.3829\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8043 - acc: 0.3781 - val_loss: 1.8262 - val_acc: 0.3829\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8028 - acc: 0.3794 - val_loss: 1.8143 - val_acc: 0.3863\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8011 - acc: 0.3796 - val_loss: 1.8388 - val_acc: 0.3766\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.7996 - acc: 0.3792 - val_loss: 1.8234 - val_acc: 0.3834\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.7984 - acc: 0.3797 - val_loss: 1.8298 - val_acc: 0.3803\n",
            "32890/32890 [==============================] - 0s 11us/sample - loss: 1.8318 - acc: 0.3667\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 2.0222 - acc: 0.3083 - val_loss: 1.8900 - val_acc: 0.3448\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 2s 16us/sample - loss: 1.8845 - acc: 0.3495 - val_loss: 1.8633 - val_acc: 0.3520\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 2s 19us/sample - loss: 1.8636 - acc: 0.3564 - val_loss: 1.8457 - val_acc: 0.3538\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8521 - acc: 0.3591 - val_loss: 1.8444 - val_acc: 0.3509\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8437 - acc: 0.3619 - val_loss: 1.8361 - val_acc: 0.3581\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8371 - acc: 0.3643 - val_loss: 1.8369 - val_acc: 0.3558\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8325 - acc: 0.3660 - val_loss: 1.8202 - val_acc: 0.3612\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8286 - acc: 0.3678 - val_loss: 1.8196 - val_acc: 0.3625\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8253 - acc: 0.3693 - val_loss: 1.8239 - val_acc: 0.3593\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8226 - acc: 0.3701 - val_loss: 1.7996 - val_acc: 0.3724\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8201 - acc: 0.3704 - val_loss: 1.8128 - val_acc: 0.3679\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8178 - acc: 0.3716 - val_loss: 1.8168 - val_acc: 0.3642\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8158 - acc: 0.3715 - val_loss: 1.8148 - val_acc: 0.3656\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8139 - acc: 0.3723 - val_loss: 1.8277 - val_acc: 0.3557\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8121 - acc: 0.3730 - val_loss: 1.8070 - val_acc: 0.3690\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8109 - acc: 0.3739 - val_loss: 1.8082 - val_acc: 0.3658\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8092 - acc: 0.3740 - val_loss: 1.7926 - val_acc: 0.3754\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 2s 18us/sample - loss: 1.8076 - acc: 0.3748 - val_loss: 1.8040 - val_acc: 0.3693\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8061 - acc: 0.3754 - val_loss: 1.8168 - val_acc: 0.3626\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8051 - acc: 0.3750 - val_loss: 1.8025 - val_acc: 0.3695\n",
            "32889/32889 [==============================] - 0s 10us/sample - loss: 1.8284 - acc: 0.3870\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 2.0123 - acc: 0.3156 - val_loss: 1.8651 - val_acc: 0.3723\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8813 - acc: 0.3535 - val_loss: 1.8415 - val_acc: 0.3761\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.8636 - acc: 0.3568 - val_loss: 1.8194 - val_acc: 0.3858\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8517 - acc: 0.3612 - val_loss: 1.8140 - val_acc: 0.3876\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8430 - acc: 0.3648 - val_loss: 1.8049 - val_acc: 0.3899\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8351 - acc: 0.3671 - val_loss: 1.8202 - val_acc: 0.3817\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8290 - acc: 0.3687 - val_loss: 1.8115 - val_acc: 0.3892\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8244 - acc: 0.3711 - val_loss: 1.8008 - val_acc: 0.3909\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8200 - acc: 0.3719 - val_loss: 1.8103 - val_acc: 0.3894\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8163 - acc: 0.3720 - val_loss: 1.8090 - val_acc: 0.3879\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8132 - acc: 0.3745 - val_loss: 1.8059 - val_acc: 0.3908\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8102 - acc: 0.3753 - val_loss: 1.8069 - val_acc: 0.3902\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8074 - acc: 0.3763 - val_loss: 1.7880 - val_acc: 0.3961\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8050 - acc: 0.3772 - val_loss: 1.7978 - val_acc: 0.3919\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8033 - acc: 0.3770 - val_loss: 1.8012 - val_acc: 0.3896\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8012 - acc: 0.3785 - val_loss: 1.7963 - val_acc: 0.3945\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.7993 - acc: 0.3784 - val_loss: 1.7882 - val_acc: 0.3982\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 16us/sample - loss: 1.7984 - acc: 0.3790 - val_loss: 1.7900 - val_acc: 0.3954\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.7968 - acc: 0.3807 - val_loss: 1.7955 - val_acc: 0.3951\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.7953 - acc: 0.3809 - val_loss: 1.7963 - val_acc: 0.3948\n",
            "32890/32890 [==============================] - 0s 11us/sample - loss: 1.8496 - acc: 0.3599\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9991 - acc: 0.3176 - val_loss: 1.8844 - val_acc: 0.3652\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8647 - acc: 0.3618 - val_loss: 1.8338 - val_acc: 0.3863\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8457 - acc: 0.3671 - val_loss: 1.8321 - val_acc: 0.3848\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8337 - acc: 0.3710 - val_loss: 1.8207 - val_acc: 0.3890\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8247 - acc: 0.3748 - val_loss: 1.8085 - val_acc: 0.3924\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8184 - acc: 0.3761 - val_loss: 1.8125 - val_acc: 0.3907\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8129 - acc: 0.3788 - val_loss: 1.8258 - val_acc: 0.3809\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8087 - acc: 0.3798 - val_loss: 1.8250 - val_acc: 0.3828\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8044 - acc: 0.3819 - val_loss: 1.8084 - val_acc: 0.3895\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8012 - acc: 0.3830 - val_loss: 1.8092 - val_acc: 0.3908\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.7988 - acc: 0.3840 - val_loss: 1.8205 - val_acc: 0.3857\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.7963 - acc: 0.3840 - val_loss: 1.8074 - val_acc: 0.3875\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.7940 - acc: 0.3843 - val_loss: 1.7988 - val_acc: 0.3909\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.7921 - acc: 0.3845 - val_loss: 1.7861 - val_acc: 0.3992\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.7906 - acc: 0.3866 - val_loss: 1.7933 - val_acc: 0.3978\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.7884 - acc: 0.3861 - val_loss: 1.7941 - val_acc: 0.3930\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.7868 - acc: 0.3865 - val_loss: 1.7987 - val_acc: 0.3924\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.7853 - acc: 0.3883 - val_loss: 1.7881 - val_acc: 0.3972\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.7839 - acc: 0.3883 - val_loss: 1.8061 - val_acc: 0.3889\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.7824 - acc: 0.3887 - val_loss: 1.7896 - val_acc: 0.3971\n",
            "32890/32890 [==============================] - 0s 11us/sample - loss: 1.8905 - acc: 0.3402\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 2.0221 - acc: 0.3051 - val_loss: 1.8948 - val_acc: 0.3591\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8890 - acc: 0.3495 - val_loss: 1.8635 - val_acc: 0.3681\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8693 - acc: 0.3535 - val_loss: 1.8513 - val_acc: 0.3726\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8579 - acc: 0.3568 - val_loss: 1.8368 - val_acc: 0.3782\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8503 - acc: 0.3593 - val_loss: 1.8343 - val_acc: 0.3792\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8439 - acc: 0.3615 - val_loss: 1.8463 - val_acc: 0.3747\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8396 - acc: 0.3625 - val_loss: 1.8201 - val_acc: 0.3854\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8359 - acc: 0.3635 - val_loss: 1.8300 - val_acc: 0.3795\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8317 - acc: 0.3659 - val_loss: 1.8296 - val_acc: 0.3785\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8293 - acc: 0.3670 - val_loss: 1.8144 - val_acc: 0.3847\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8270 - acc: 0.3674 - val_loss: 1.8283 - val_acc: 0.3772\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8244 - acc: 0.3681 - val_loss: 1.8350 - val_acc: 0.3770\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8228 - acc: 0.3688 - val_loss: 1.8178 - val_acc: 0.3859\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8206 - acc: 0.3698 - val_loss: 1.8217 - val_acc: 0.3804\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8184 - acc: 0.3701 - val_loss: 1.8209 - val_acc: 0.3832\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8170 - acc: 0.3712 - val_loss: 1.8072 - val_acc: 0.3887\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8156 - acc: 0.3720 - val_loss: 1.8283 - val_acc: 0.3793\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8142 - acc: 0.3710 - val_loss: 1.8194 - val_acc: 0.3824\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8127 - acc: 0.3722 - val_loss: 1.8075 - val_acc: 0.3887\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8115 - acc: 0.3725 - val_loss: 1.8259 - val_acc: 0.3804\n",
            "32890/32890 [==============================] - 0s 11us/sample - loss: 1.8143 - acc: 0.3760\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 2.0141 - acc: 0.3118 - val_loss: 1.8883 - val_acc: 0.3661\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8813 - acc: 0.3549 - val_loss: 1.8675 - val_acc: 0.3695\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8620 - acc: 0.3600 - val_loss: 1.8766 - val_acc: 0.3683\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8499 - acc: 0.3632 - val_loss: 1.8595 - val_acc: 0.3726\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8408 - acc: 0.3666 - val_loss: 1.8508 - val_acc: 0.3690\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8340 - acc: 0.3692 - val_loss: 1.8487 - val_acc: 0.3756\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8286 - acc: 0.3699 - val_loss: 1.8270 - val_acc: 0.3852\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8242 - acc: 0.3721 - val_loss: 1.8531 - val_acc: 0.3718\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8202 - acc: 0.3734 - val_loss: 1.8251 - val_acc: 0.3886\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8173 - acc: 0.3741 - val_loss: 1.8491 - val_acc: 0.3736\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8140 - acc: 0.3751 - val_loss: 1.8271 - val_acc: 0.3833\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8115 - acc: 0.3753 - val_loss: 1.8339 - val_acc: 0.3822\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8090 - acc: 0.3757 - val_loss: 1.8316 - val_acc: 0.3817\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8067 - acc: 0.3774 - val_loss: 1.8428 - val_acc: 0.3745\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8043 - acc: 0.3785 - val_loss: 1.8406 - val_acc: 0.3788\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8023 - acc: 0.3783 - val_loss: 1.8285 - val_acc: 0.3804\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8010 - acc: 0.3794 - val_loss: 1.8240 - val_acc: 0.3851\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.7994 - acc: 0.3798 - val_loss: 1.8191 - val_acc: 0.3855\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.7982 - acc: 0.3798 - val_loss: 1.8300 - val_acc: 0.3810\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.7971 - acc: 0.3816 - val_loss: 1.8243 - val_acc: 0.3821\n",
            "32890/32890 [==============================] - 0s 12us/sample - loss: 1.8383 - acc: 0.3631\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 2.0304 - acc: 0.3037 - val_loss: 1.8896 - val_acc: 0.3445\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8889 - acc: 0.3490 - val_loss: 1.8779 - val_acc: 0.3474\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8689 - acc: 0.3544 - val_loss: 1.8533 - val_acc: 0.3518\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8569 - acc: 0.3577 - val_loss: 1.8570 - val_acc: 0.3465\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8477 - acc: 0.3618 - val_loss: 1.8452 - val_acc: 0.3540\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8409 - acc: 0.3634 - val_loss: 1.8403 - val_acc: 0.3533\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8354 - acc: 0.3655 - val_loss: 1.8284 - val_acc: 0.3601\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8300 - acc: 0.3669 - val_loss: 1.8286 - val_acc: 0.3609\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8255 - acc: 0.3687 - val_loss: 1.8209 - val_acc: 0.3622\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8226 - acc: 0.3699 - val_loss: 1.8219 - val_acc: 0.3633\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8196 - acc: 0.3704 - val_loss: 1.8113 - val_acc: 0.3668\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8168 - acc: 0.3717 - val_loss: 1.8155 - val_acc: 0.3638\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8150 - acc: 0.3718 - val_loss: 1.8079 - val_acc: 0.3634\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8127 - acc: 0.3712 - val_loss: 1.8053 - val_acc: 0.3687\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 2s 18us/sample - loss: 1.8110 - acc: 0.3739 - val_loss: 1.8211 - val_acc: 0.3583\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 2s 18us/sample - loss: 1.8098 - acc: 0.3732 - val_loss: 1.8061 - val_acc: 0.3665\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 2s 18us/sample - loss: 1.8086 - acc: 0.3743 - val_loss: 1.8028 - val_acc: 0.3724\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 2s 19us/sample - loss: 1.8069 - acc: 0.3746 - val_loss: 1.8010 - val_acc: 0.3726\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8056 - acc: 0.3745 - val_loss: 1.8117 - val_acc: 0.3639\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 2s 18us/sample - loss: 1.8046 - acc: 0.3752 - val_loss: 1.8180 - val_acc: 0.3599\n",
            "32889/32889 [==============================] - 0s 12us/sample - loss: 1.8395 - acc: 0.3829\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 2.0197 - acc: 0.3093 - val_loss: 1.8648 - val_acc: 0.3686\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8803 - acc: 0.3521 - val_loss: 1.8197 - val_acc: 0.3859\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8592 - acc: 0.3600 - val_loss: 1.8197 - val_acc: 0.3876\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8446 - acc: 0.3650 - val_loss: 1.8252 - val_acc: 0.3844\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8360 - acc: 0.3679 - val_loss: 1.8072 - val_acc: 0.3947\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8289 - acc: 0.3699 - val_loss: 1.8105 - val_acc: 0.3915\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8235 - acc: 0.3713 - val_loss: 1.7940 - val_acc: 0.3984\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8191 - acc: 0.3728 - val_loss: 1.7970 - val_acc: 0.3968\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8155 - acc: 0.3747 - val_loss: 1.7959 - val_acc: 0.3976\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8125 - acc: 0.3752 - val_loss: 1.7866 - val_acc: 0.3998\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8097 - acc: 0.3764 - val_loss: 1.7842 - val_acc: 0.4022\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8074 - acc: 0.3768 - val_loss: 1.7994 - val_acc: 0.3931\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8050 - acc: 0.3784 - val_loss: 1.7900 - val_acc: 0.3989\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8026 - acc: 0.3783 - val_loss: 1.7985 - val_acc: 0.3951\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8007 - acc: 0.3803 - val_loss: 1.7891 - val_acc: 0.3989\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.7996 - acc: 0.3793 - val_loss: 1.7806 - val_acc: 0.4041\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.7983 - acc: 0.3797 - val_loss: 1.7888 - val_acc: 0.3987\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.7965 - acc: 0.3804 - val_loss: 1.7833 - val_acc: 0.4004\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.7949 - acc: 0.3820 - val_loss: 1.7838 - val_acc: 0.4019\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.7938 - acc: 0.3811 - val_loss: 1.7809 - val_acc: 0.4008\n",
            "32890/32890 [==============================] - 0s 12us/sample - loss: 1.8481 - acc: 0.3612\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 2.0255 - acc: 0.3143 - val_loss: 1.8735 - val_acc: 0.3688\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8657 - acc: 0.3631 - val_loss: 1.8376 - val_acc: 0.3820\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8444 - acc: 0.3690 - val_loss: 1.8471 - val_acc: 0.3722\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8319 - acc: 0.3716 - val_loss: 1.8182 - val_acc: 0.3862\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8232 - acc: 0.3748 - val_loss: 1.8148 - val_acc: 0.3893\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8165 - acc: 0.3775 - val_loss: 1.8174 - val_acc: 0.3847\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8118 - acc: 0.3787 - val_loss: 1.8159 - val_acc: 0.3844\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8071 - acc: 0.3804 - val_loss: 1.8034 - val_acc: 0.3944\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8034 - acc: 0.3817 - val_loss: 1.8069 - val_acc: 0.3923\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8003 - acc: 0.3825 - val_loss: 1.8008 - val_acc: 0.3923\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.7981 - acc: 0.3833 - val_loss: 1.8161 - val_acc: 0.3865\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.7951 - acc: 0.3851 - val_loss: 1.8061 - val_acc: 0.3915\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.7931 - acc: 0.3849 - val_loss: 1.7995 - val_acc: 0.3961\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.7912 - acc: 0.3851 - val_loss: 1.7967 - val_acc: 0.3930\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.7894 - acc: 0.3858 - val_loss: 1.7956 - val_acc: 0.3935\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.7877 - acc: 0.3862 - val_loss: 1.8040 - val_acc: 0.3925\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.7860 - acc: 0.3874 - val_loss: 1.7985 - val_acc: 0.3917\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.7848 - acc: 0.3879 - val_loss: 1.7987 - val_acc: 0.3959\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.7834 - acc: 0.3874 - val_loss: 1.7928 - val_acc: 0.3986\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.7824 - acc: 0.3884 - val_loss: 1.7839 - val_acc: 0.4005\n",
            "32890/32890 [==============================] - 0s 11us/sample - loss: 1.8959 - acc: 0.3398\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 2.0203 - acc: 0.3069 - val_loss: 1.8905 - val_acc: 0.3612\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8921 - acc: 0.3476 - val_loss: 1.8601 - val_acc: 0.3729\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8705 - acc: 0.3553 - val_loss: 1.8498 - val_acc: 0.3725\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8572 - acc: 0.3596 - val_loss: 1.8344 - val_acc: 0.3782\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8483 - acc: 0.3622 - val_loss: 1.8294 - val_acc: 0.3857\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8408 - acc: 0.3652 - val_loss: 1.8317 - val_acc: 0.3798\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8347 - acc: 0.3661 - val_loss: 1.8218 - val_acc: 0.3847\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8304 - acc: 0.3686 - val_loss: 1.8179 - val_acc: 0.3837\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8262 - acc: 0.3692 - val_loss: 1.8176 - val_acc: 0.3873\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8230 - acc: 0.3700 - val_loss: 1.8224 - val_acc: 0.3797\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8202 - acc: 0.3710 - val_loss: 1.8242 - val_acc: 0.3785\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8182 - acc: 0.3709 - val_loss: 1.8207 - val_acc: 0.3828\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8160 - acc: 0.3711 - val_loss: 1.8139 - val_acc: 0.3860\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8139 - acc: 0.3720 - val_loss: 1.8289 - val_acc: 0.3769\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8123 - acc: 0.3732 - val_loss: 1.8148 - val_acc: 0.3839\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8110 - acc: 0.3741 - val_loss: 1.8198 - val_acc: 0.3814\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8092 - acc: 0.3733 - val_loss: 1.8128 - val_acc: 0.3840\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8081 - acc: 0.3737 - val_loss: 1.8112 - val_acc: 0.3875\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8065 - acc: 0.3740 - val_loss: 1.8245 - val_acc: 0.3821\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8054 - acc: 0.3746 - val_loss: 1.8143 - val_acc: 0.3845\n",
            "32890/32890 [==============================] - 0s 12us/sample - loss: 1.7942 - acc: 0.3856\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 2.0034 - acc: 0.3113 - val_loss: 1.9018 - val_acc: 0.3542\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8785 - acc: 0.3547 - val_loss: 1.8588 - val_acc: 0.3748\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8602 - acc: 0.3597 - val_loss: 1.8552 - val_acc: 0.3746\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8487 - acc: 0.3636 - val_loss: 1.8410 - val_acc: 0.3775\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8405 - acc: 0.3651 - val_loss: 1.8340 - val_acc: 0.3816\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8342 - acc: 0.3687 - val_loss: 1.8465 - val_acc: 0.3760\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8286 - acc: 0.3699 - val_loss: 1.8415 - val_acc: 0.3779\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8247 - acc: 0.3708 - val_loss: 1.8415 - val_acc: 0.3794\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8219 - acc: 0.3719 - val_loss: 1.8294 - val_acc: 0.3842\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8187 - acc: 0.3736 - val_loss: 1.8366 - val_acc: 0.3808\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8161 - acc: 0.3739 - val_loss: 1.8468 - val_acc: 0.3817\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8139 - acc: 0.3741 - val_loss: 1.8353 - val_acc: 0.3780\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8113 - acc: 0.3753 - val_loss: 1.8461 - val_acc: 0.3762\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8096 - acc: 0.3767 - val_loss: 1.8357 - val_acc: 0.3826\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8086 - acc: 0.3767 - val_loss: 1.8465 - val_acc: 0.3764\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8068 - acc: 0.3764 - val_loss: 1.8292 - val_acc: 0.3832\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8053 - acc: 0.3782 - val_loss: 1.8255 - val_acc: 0.3853\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8044 - acc: 0.3782 - val_loss: 1.8436 - val_acc: 0.3802\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8025 - acc: 0.3784 - val_loss: 1.8398 - val_acc: 0.3803\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 17us/sample - loss: 1.8017 - acc: 0.3790 - val_loss: 1.8265 - val_acc: 0.3869\n",
            "32890/32890 [==============================] - 0s 12us/sample - loss: 1.8325 - acc: 0.3661\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 3s 28us/sample - loss: 2.0126 - acc: 0.3140 - val_loss: 1.8988 - val_acc: 0.3375\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 2s 18us/sample - loss: 1.8875 - acc: 0.3492 - val_loss: 1.8729 - val_acc: 0.3483\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 2s 18us/sample - loss: 1.8664 - acc: 0.3563 - val_loss: 1.8472 - val_acc: 0.3582\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8545 - acc: 0.3592 - val_loss: 1.8541 - val_acc: 0.3571\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8459 - acc: 0.3631 - val_loss: 1.8370 - val_acc: 0.3570\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 2s 18us/sample - loss: 1.8387 - acc: 0.3645 - val_loss: 1.8292 - val_acc: 0.3613\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 2s 18us/sample - loss: 1.8336 - acc: 0.3661 - val_loss: 1.8293 - val_acc: 0.3600\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 2s 18us/sample - loss: 1.8294 - acc: 0.3675 - val_loss: 1.8163 - val_acc: 0.3646\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8263 - acc: 0.3687 - val_loss: 1.8377 - val_acc: 0.3539\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 2s 18us/sample - loss: 1.8229 - acc: 0.3702 - val_loss: 1.8250 - val_acc: 0.3618\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 2s 18us/sample - loss: 1.8204 - acc: 0.3710 - val_loss: 1.8267 - val_acc: 0.3591\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 2s 18us/sample - loss: 1.8183 - acc: 0.3709 - val_loss: 1.8185 - val_acc: 0.3647\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8159 - acc: 0.3716 - val_loss: 1.8162 - val_acc: 0.3602\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8139 - acc: 0.3728 - val_loss: 1.8103 - val_acc: 0.3640\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8127 - acc: 0.3726 - val_loss: 1.8178 - val_acc: 0.3628\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 2s 18us/sample - loss: 1.8107 - acc: 0.3736 - val_loss: 1.8175 - val_acc: 0.3598\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 2s 17us/sample - loss: 1.8092 - acc: 0.3745 - val_loss: 1.8149 - val_acc: 0.3638\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 2s 18us/sample - loss: 1.8077 - acc: 0.3742 - val_loss: 1.8157 - val_acc: 0.3655\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 2s 18us/sample - loss: 1.8065 - acc: 0.3753 - val_loss: 1.8089 - val_acc: 0.3699\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 2s 18us/sample - loss: 1.8052 - acc: 0.3756 - val_loss: 1.8075 - val_acc: 0.3699\n",
            "32889/32889 [==============================] - 0s 11us/sample - loss: 1.8308 - acc: 0.3889\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 2.0427 - acc: 0.3037 - val_loss: 1.8742 - val_acc: 0.3680\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8837 - acc: 0.3544 - val_loss: 1.8318 - val_acc: 0.3840\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8614 - acc: 0.3610 - val_loss: 1.8213 - val_acc: 0.3899\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8492 - acc: 0.3645 - val_loss: 1.8220 - val_acc: 0.3824\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8410 - acc: 0.3668 - val_loss: 1.8171 - val_acc: 0.3867\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8348 - acc: 0.3689 - val_loss: 1.7950 - val_acc: 0.3965\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8295 - acc: 0.3704 - val_loss: 1.8064 - val_acc: 0.3901\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8257 - acc: 0.3709 - val_loss: 1.8001 - val_acc: 0.3936\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8221 - acc: 0.3721 - val_loss: 1.7990 - val_acc: 0.3928\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8189 - acc: 0.3733 - val_loss: 1.7932 - val_acc: 0.3972\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8159 - acc: 0.3741 - val_loss: 1.7937 - val_acc: 0.3967\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8134 - acc: 0.3750 - val_loss: 1.7872 - val_acc: 0.3972\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8108 - acc: 0.3755 - val_loss: 1.7900 - val_acc: 0.3962\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8084 - acc: 0.3774 - val_loss: 1.8041 - val_acc: 0.3907\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8066 - acc: 0.3768 - val_loss: 1.7866 - val_acc: 0.3970\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8046 - acc: 0.3782 - val_loss: 1.8068 - val_acc: 0.3891\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8024 - acc: 0.3787 - val_loss: 1.7865 - val_acc: 0.3980\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8013 - acc: 0.3791 - val_loss: 1.7781 - val_acc: 0.4018\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.7994 - acc: 0.3793 - val_loss: 1.7922 - val_acc: 0.3948\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.7980 - acc: 0.3806 - val_loss: 1.7894 - val_acc: 0.3960\n",
            "32890/32890 [==============================] - 0s 13us/sample - loss: 1.8498 - acc: 0.3634\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 2.0160 - acc: 0.3117 - val_loss: 1.8697 - val_acc: 0.3696\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8683 - acc: 0.3610 - val_loss: 1.8368 - val_acc: 0.3826\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8467 - acc: 0.3675 - val_loss: 1.8283 - val_acc: 0.3859\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8344 - acc: 0.3700 - val_loss: 1.8310 - val_acc: 0.3840\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8261 - acc: 0.3737 - val_loss: 1.8222 - val_acc: 0.3850\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8191 - acc: 0.3765 - val_loss: 1.8382 - val_acc: 0.3828\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8145 - acc: 0.3774 - val_loss: 1.8107 - val_acc: 0.3867\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8103 - acc: 0.3791 - val_loss: 1.8106 - val_acc: 0.3884\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8065 - acc: 0.3802 - val_loss: 1.8082 - val_acc: 0.3851\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8034 - acc: 0.3808 - val_loss: 1.8116 - val_acc: 0.3880\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8010 - acc: 0.3818 - val_loss: 1.8015 - val_acc: 0.3908\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.7980 - acc: 0.3835 - val_loss: 1.8023 - val_acc: 0.3897\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.7958 - acc: 0.3831 - val_loss: 1.8090 - val_acc: 0.3863\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.7937 - acc: 0.3839 - val_loss: 1.8077 - val_acc: 0.3859\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.7919 - acc: 0.3847 - val_loss: 1.7989 - val_acc: 0.3886\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.7904 - acc: 0.3853 - val_loss: 1.7954 - val_acc: 0.3916\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.7884 - acc: 0.3852 - val_loss: 1.7958 - val_acc: 0.3897\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.7872 - acc: 0.3865 - val_loss: 1.7973 - val_acc: 0.3897\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.7859 - acc: 0.3866 - val_loss: 1.8017 - val_acc: 0.3909\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.7848 - acc: 0.3875 - val_loss: 1.7953 - val_acc: 0.3929\n",
            "32890/32890 [==============================] - 0s 12us/sample - loss: 1.9013 - acc: 0.3392\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 2.0257 - acc: 0.3034 - val_loss: 1.8745 - val_acc: 0.3650\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8860 - acc: 0.3504 - val_loss: 1.8716 - val_acc: 0.3658\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8668 - acc: 0.3550 - val_loss: 1.8485 - val_acc: 0.3722\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8553 - acc: 0.3588 - val_loss: 1.8551 - val_acc: 0.3704\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8469 - acc: 0.3613 - val_loss: 1.8535 - val_acc: 0.3695\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8409 - acc: 0.3626 - val_loss: 1.8466 - val_acc: 0.3706\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8357 - acc: 0.3640 - val_loss: 1.8295 - val_acc: 0.3796\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8318 - acc: 0.3657 - val_loss: 1.8326 - val_acc: 0.3774\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8284 - acc: 0.3673 - val_loss: 1.8296 - val_acc: 0.3796\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8251 - acc: 0.3684 - val_loss: 1.8283 - val_acc: 0.3801\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8224 - acc: 0.3689 - val_loss: 1.8270 - val_acc: 0.3794\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8203 - acc: 0.3693 - val_loss: 1.8216 - val_acc: 0.3850\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8182 - acc: 0.3713 - val_loss: 1.8266 - val_acc: 0.3794\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8163 - acc: 0.3708 - val_loss: 1.8188 - val_acc: 0.3817\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8140 - acc: 0.3732 - val_loss: 1.8325 - val_acc: 0.3756\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8122 - acc: 0.3731 - val_loss: 1.8232 - val_acc: 0.3791\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8104 - acc: 0.3743 - val_loss: 1.8090 - val_acc: 0.3856\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8090 - acc: 0.3744 - val_loss: 1.8186 - val_acc: 0.3831\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8076 - acc: 0.3753 - val_loss: 1.8342 - val_acc: 0.3761\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8063 - acc: 0.3751 - val_loss: 1.8172 - val_acc: 0.3822\n",
            "32890/32890 [==============================] - 0s 13us/sample - loss: 1.7971 - acc: 0.3836\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 2.0264 - acc: 0.3033 - val_loss: 1.8837 - val_acc: 0.3691\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8829 - acc: 0.3535 - val_loss: 1.8713 - val_acc: 0.3713\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8614 - acc: 0.3600 - val_loss: 1.8612 - val_acc: 0.3703\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8492 - acc: 0.3635 - val_loss: 1.8647 - val_acc: 0.3650\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8412 - acc: 0.3661 - val_loss: 1.8493 - val_acc: 0.3762\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8347 - acc: 0.3678 - val_loss: 1.8395 - val_acc: 0.3759\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8293 - acc: 0.3690 - val_loss: 1.8300 - val_acc: 0.3786\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8255 - acc: 0.3712 - val_loss: 1.8493 - val_acc: 0.3728\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8211 - acc: 0.3721 - val_loss: 1.8333 - val_acc: 0.3831\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8177 - acc: 0.3725 - val_loss: 1.8308 - val_acc: 0.3817\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8147 - acc: 0.3741 - val_loss: 1.8332 - val_acc: 0.3834\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8118 - acc: 0.3746 - val_loss: 1.8475 - val_acc: 0.3731\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8098 - acc: 0.3760 - val_loss: 1.8201 - val_acc: 0.3874\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8075 - acc: 0.3759 - val_loss: 1.8274 - val_acc: 0.3828\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8057 - acc: 0.3762 - val_loss: 1.8234 - val_acc: 0.3866\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8040 - acc: 0.3776 - val_loss: 1.8482 - val_acc: 0.3778\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8023 - acc: 0.3787 - val_loss: 1.8385 - val_acc: 0.3815\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8006 - acc: 0.3790 - val_loss: 1.8368 - val_acc: 0.3812\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.7994 - acc: 0.3790 - val_loss: 1.8338 - val_acc: 0.3841\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.7978 - acc: 0.3797 - val_loss: 1.8199 - val_acc: 0.3842\n",
            "32890/32890 [==============================] - 0s 14us/sample - loss: 1.8270 - acc: 0.3693\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 3s 30us/sample - loss: 2.0258 - acc: 0.3058 - val_loss: 1.9075 - val_acc: 0.3378\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 2s 19us/sample - loss: 1.8890 - acc: 0.3498 - val_loss: 1.8624 - val_acc: 0.3538\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 2s 19us/sample - loss: 1.8684 - acc: 0.3550 - val_loss: 1.8378 - val_acc: 0.3619\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 2s 19us/sample - loss: 1.8559 - acc: 0.3587 - val_loss: 1.8383 - val_acc: 0.3576\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 2s 19us/sample - loss: 1.8471 - acc: 0.3620 - val_loss: 1.8261 - val_acc: 0.3643\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 2s 19us/sample - loss: 1.8401 - acc: 0.3637 - val_loss: 1.8433 - val_acc: 0.3538\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 2s 19us/sample - loss: 1.8343 - acc: 0.3654 - val_loss: 1.8272 - val_acc: 0.3601\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 2s 19us/sample - loss: 1.8294 - acc: 0.3670 - val_loss: 1.8258 - val_acc: 0.3556\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 2s 19us/sample - loss: 1.8257 - acc: 0.3682 - val_loss: 1.8156 - val_acc: 0.3675\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 2s 19us/sample - loss: 1.8221 - acc: 0.3694 - val_loss: 1.8199 - val_acc: 0.3630\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 2s 19us/sample - loss: 1.8195 - acc: 0.3707 - val_loss: 1.8143 - val_acc: 0.3660\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 2s 18us/sample - loss: 1.8171 - acc: 0.3716 - val_loss: 1.8193 - val_acc: 0.3653\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 2s 18us/sample - loss: 1.8147 - acc: 0.3721 - val_loss: 1.8120 - val_acc: 0.3639\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 2s 19us/sample - loss: 1.8124 - acc: 0.3730 - val_loss: 1.8080 - val_acc: 0.3675\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 2s 19us/sample - loss: 1.8107 - acc: 0.3741 - val_loss: 1.8008 - val_acc: 0.3735\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 2s 19us/sample - loss: 1.8084 - acc: 0.3739 - val_loss: 1.7965 - val_acc: 0.3761\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 2s 18us/sample - loss: 1.8072 - acc: 0.3750 - val_loss: 1.8130 - val_acc: 0.3667\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8055 - acc: 0.3751 - val_loss: 1.8059 - val_acc: 0.3677\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 2s 19us/sample - loss: 1.8042 - acc: 0.3766 - val_loss: 1.7854 - val_acc: 0.3799\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 2s 19us/sample - loss: 1.8029 - acc: 0.3775 - val_loss: 1.7972 - val_acc: 0.3729\n",
            "32889/32889 [==============================] - 0s 13us/sample - loss: 1.8332 - acc: 0.3847\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 2.0199 - acc: 0.3141 - val_loss: 1.8704 - val_acc: 0.3698\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8849 - acc: 0.3528 - val_loss: 1.8468 - val_acc: 0.3774\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8627 - acc: 0.3595 - val_loss: 1.8370 - val_acc: 0.3800\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8499 - acc: 0.3635 - val_loss: 1.8220 - val_acc: 0.3823\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8412 - acc: 0.3657 - val_loss: 1.8150 - val_acc: 0.3864\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8344 - acc: 0.3687 - val_loss: 1.8158 - val_acc: 0.3869\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8289 - acc: 0.3696 - val_loss: 1.7987 - val_acc: 0.3933\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8244 - acc: 0.3724 - val_loss: 1.8037 - val_acc: 0.3918\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8209 - acc: 0.3729 - val_loss: 1.8054 - val_acc: 0.3880\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8178 - acc: 0.3736 - val_loss: 1.8072 - val_acc: 0.3885\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8150 - acc: 0.3756 - val_loss: 1.7992 - val_acc: 0.3948\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8125 - acc: 0.3756 - val_loss: 1.8174 - val_acc: 0.3839\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8104 - acc: 0.3762 - val_loss: 1.7989 - val_acc: 0.3933\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8081 - acc: 0.3777 - val_loss: 1.7961 - val_acc: 0.3962\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8058 - acc: 0.3784 - val_loss: 1.8019 - val_acc: 0.3924\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8039 - acc: 0.3785 - val_loss: 1.7981 - val_acc: 0.3951\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8021 - acc: 0.3786 - val_loss: 1.7938 - val_acc: 0.3965\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8004 - acc: 0.3793 - val_loss: 1.7998 - val_acc: 0.3931\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.7988 - acc: 0.3803 - val_loss: 1.7917 - val_acc: 0.3940\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.7977 - acc: 0.3799 - val_loss: 1.8021 - val_acc: 0.3907\n",
            "32890/32890 [==============================] - 0s 12us/sample - loss: 1.8448 - acc: 0.3601\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.9815 - acc: 0.3259 - val_loss: 1.8678 - val_acc: 0.3723\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8637 - acc: 0.3625 - val_loss: 1.8492 - val_acc: 0.3785\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8447 - acc: 0.3686 - val_loss: 1.8314 - val_acc: 0.3850\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8331 - acc: 0.3717 - val_loss: 1.8270 - val_acc: 0.3845\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8240 - acc: 0.3749 - val_loss: 1.7993 - val_acc: 0.3923\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8170 - acc: 0.3776 - val_loss: 1.8015 - val_acc: 0.3915\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8110 - acc: 0.3793 - val_loss: 1.8028 - val_acc: 0.3936\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8059 - acc: 0.3801 - val_loss: 1.8052 - val_acc: 0.3877\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8020 - acc: 0.3823 - val_loss: 1.8010 - val_acc: 0.3918\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.7985 - acc: 0.3836 - val_loss: 1.8029 - val_acc: 0.3898\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.7955 - acc: 0.3843 - val_loss: 1.7931 - val_acc: 0.3948\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.7924 - acc: 0.3857 - val_loss: 1.8011 - val_acc: 0.3931\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.7900 - acc: 0.3857 - val_loss: 1.7941 - val_acc: 0.3938\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.7880 - acc: 0.3875 - val_loss: 1.7962 - val_acc: 0.3921\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.7861 - acc: 0.3877 - val_loss: 1.7957 - val_acc: 0.3939\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.7844 - acc: 0.3884 - val_loss: 1.8011 - val_acc: 0.3910\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.7825 - acc: 0.3891 - val_loss: 1.8070 - val_acc: 0.3905\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.7809 - acc: 0.3890 - val_loss: 1.7877 - val_acc: 0.3973\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.7796 - acc: 0.3899 - val_loss: 1.7941 - val_acc: 0.3909\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.7786 - acc: 0.3905 - val_loss: 1.8009 - val_acc: 0.3925\n",
            "32890/32890 [==============================] - 0s 13us/sample - loss: 1.8972 - acc: 0.3378\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 2.0120 - acc: 0.3059 - val_loss: 1.8812 - val_acc: 0.3635\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8871 - acc: 0.3491 - val_loss: 1.8729 - val_acc: 0.3675\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8691 - acc: 0.3537 - val_loss: 1.8340 - val_acc: 0.3855\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8581 - acc: 0.3574 - val_loss: 1.8403 - val_acc: 0.3758\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8507 - acc: 0.3595 - val_loss: 1.8339 - val_acc: 0.3820\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8453 - acc: 0.3609 - val_loss: 1.8329 - val_acc: 0.3806\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8402 - acc: 0.3624 - val_loss: 1.8332 - val_acc: 0.3812\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8359 - acc: 0.3630 - val_loss: 1.8263 - val_acc: 0.3795\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8324 - acc: 0.3664 - val_loss: 1.8317 - val_acc: 0.3777\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8292 - acc: 0.3657 - val_loss: 1.8250 - val_acc: 0.3765\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8266 - acc: 0.3670 - val_loss: 1.8217 - val_acc: 0.3813\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8238 - acc: 0.3673 - val_loss: 1.8137 - val_acc: 0.3844\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8214 - acc: 0.3692 - val_loss: 1.8278 - val_acc: 0.3776\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8193 - acc: 0.3693 - val_loss: 1.8413 - val_acc: 0.3745\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8172 - acc: 0.3707 - val_loss: 1.8142 - val_acc: 0.3818\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8155 - acc: 0.3707 - val_loss: 1.8230 - val_acc: 0.3799\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8136 - acc: 0.3710 - val_loss: 1.8110 - val_acc: 0.3872\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8119 - acc: 0.3716 - val_loss: 1.7992 - val_acc: 0.3908\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8103 - acc: 0.3728 - val_loss: 1.8071 - val_acc: 0.3870\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8092 - acc: 0.3726 - val_loss: 1.8140 - val_acc: 0.3843\n",
            "32890/32890 [==============================] - 0s 14us/sample - loss: 1.7978 - acc: 0.3844\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 2.0159 - acc: 0.3150 - val_loss: 1.9013 - val_acc: 0.3600\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8813 - acc: 0.3537 - val_loss: 1.8732 - val_acc: 0.3641\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8615 - acc: 0.3589 - val_loss: 1.8517 - val_acc: 0.3754\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8488 - acc: 0.3632 - val_loss: 1.8500 - val_acc: 0.3735\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8388 - acc: 0.3652 - val_loss: 1.8451 - val_acc: 0.3726\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8317 - acc: 0.3673 - val_loss: 1.8326 - val_acc: 0.3805\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8258 - acc: 0.3706 - val_loss: 1.8369 - val_acc: 0.3757\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8213 - acc: 0.3713 - val_loss: 1.8325 - val_acc: 0.3808\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8177 - acc: 0.3728 - val_loss: 1.8351 - val_acc: 0.3730\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8151 - acc: 0.3736 - val_loss: 1.8267 - val_acc: 0.3816\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 18us/sample - loss: 1.8127 - acc: 0.3744 - val_loss: 1.8480 - val_acc: 0.3724\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8102 - acc: 0.3744 - val_loss: 1.8206 - val_acc: 0.3848\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8080 - acc: 0.3761 - val_loss: 1.8287 - val_acc: 0.3764\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8060 - acc: 0.3770 - val_loss: 1.8250 - val_acc: 0.3830\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8040 - acc: 0.3769 - val_loss: 1.8135 - val_acc: 0.3902\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8026 - acc: 0.3774 - val_loss: 1.8237 - val_acc: 0.3842\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8013 - acc: 0.3781 - val_loss: 1.8322 - val_acc: 0.3770\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.7995 - acc: 0.3783 - val_loss: 1.8333 - val_acc: 0.3759\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.7982 - acc: 0.3787 - val_loss: 1.8163 - val_acc: 0.3855\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.7970 - acc: 0.3798 - val_loss: 1.8277 - val_acc: 0.3820\n",
            "32890/32890 [==============================] - 0s 13us/sample - loss: 1.8388 - acc: 0.3633\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 3s 32us/sample - loss: 2.0166 - acc: 0.3049 - val_loss: 1.9038 - val_acc: 0.3395\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 2s 19us/sample - loss: 1.8885 - acc: 0.3488 - val_loss: 1.8756 - val_acc: 0.3485\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 2s 19us/sample - loss: 1.8680 - acc: 0.3549 - val_loss: 1.8686 - val_acc: 0.3465\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 2s 19us/sample - loss: 1.8558 - acc: 0.3594 - val_loss: 1.8535 - val_acc: 0.3506\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 2s 19us/sample - loss: 1.8475 - acc: 0.3634 - val_loss: 1.8370 - val_acc: 0.3596\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 2s 19us/sample - loss: 1.8412 - acc: 0.3650 - val_loss: 1.8343 - val_acc: 0.3584\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 2s 21us/sample - loss: 1.8364 - acc: 0.3659 - val_loss: 1.8375 - val_acc: 0.3551\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 2s 19us/sample - loss: 1.8323 - acc: 0.3679 - val_loss: 1.8434 - val_acc: 0.3509\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 2s 21us/sample - loss: 1.8286 - acc: 0.3682 - val_loss: 1.8428 - val_acc: 0.3542\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8259 - acc: 0.3690 - val_loss: 1.8299 - val_acc: 0.3606\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8232 - acc: 0.3703 - val_loss: 1.8254 - val_acc: 0.3603\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 2s 19us/sample - loss: 1.8207 - acc: 0.3709 - val_loss: 1.8216 - val_acc: 0.3625\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 2s 19us/sample - loss: 1.8188 - acc: 0.3711 - val_loss: 1.8058 - val_acc: 0.3691\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 2s 19us/sample - loss: 1.8163 - acc: 0.3736 - val_loss: 1.8026 - val_acc: 0.3701\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 2s 19us/sample - loss: 1.8142 - acc: 0.3740 - val_loss: 1.8242 - val_acc: 0.3587\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 2s 19us/sample - loss: 1.8127 - acc: 0.3739 - val_loss: 1.8138 - val_acc: 0.3659\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 2s 19us/sample - loss: 1.8108 - acc: 0.3742 - val_loss: 1.8087 - val_acc: 0.3660\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 2s 19us/sample - loss: 1.8089 - acc: 0.3744 - val_loss: 1.8035 - val_acc: 0.3730\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 2s 19us/sample - loss: 1.8077 - acc: 0.3753 - val_loss: 1.8094 - val_acc: 0.3674\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 2s 19us/sample - loss: 1.8060 - acc: 0.3760 - val_loss: 1.8055 - val_acc: 0.3680\n",
            "32889/32889 [==============================] - 0s 15us/sample - loss: 1.8318 - acc: 0.3890\n",
            "Train on 131559 samples, validate on 32890 samples\n",
            "Epoch 1/20\n",
            "131559/131559 [==============================] - 4s 29us/sample - loss: 1.9866 - acc: 0.3134 - val_loss: 1.8808 - val_acc: 0.3740\n",
            "Epoch 2/20\n",
            "131559/131559 [==============================] - 3s 19us/sample - loss: 1.8729 - acc: 0.3527 - val_loss: 1.8601 - val_acc: 0.3800\n",
            "Epoch 3/20\n",
            "131559/131559 [==============================] - 2s 19us/sample - loss: 1.8526 - acc: 0.3594 - val_loss: 1.8637 - val_acc: 0.3747\n",
            "Epoch 4/20\n",
            "131559/131559 [==============================] - 3s 19us/sample - loss: 1.8409 - acc: 0.3638 - val_loss: 1.8636 - val_acc: 0.3774\n",
            "Epoch 5/20\n",
            "131559/131559 [==============================] - 3s 20us/sample - loss: 1.8335 - acc: 0.3655 - val_loss: 1.8519 - val_acc: 0.3833\n",
            "Epoch 6/20\n",
            "131559/131559 [==============================] - 3s 19us/sample - loss: 1.8272 - acc: 0.3672 - val_loss: 1.8423 - val_acc: 0.3853\n",
            "Epoch 7/20\n",
            "131559/131559 [==============================] - 3s 19us/sample - loss: 1.8230 - acc: 0.3691 - val_loss: 1.8331 - val_acc: 0.3886\n",
            "Epoch 8/20\n",
            "131559/131559 [==============================] - 3s 19us/sample - loss: 1.8194 - acc: 0.3698 - val_loss: 1.8345 - val_acc: 0.3883\n",
            "Epoch 9/20\n",
            "131559/131559 [==============================] - 3s 19us/sample - loss: 1.8159 - acc: 0.3713 - val_loss: 1.8246 - val_acc: 0.3903\n",
            "Epoch 10/20\n",
            "131559/131559 [==============================] - 3s 19us/sample - loss: 1.8130 - acc: 0.3717 - val_loss: 1.8300 - val_acc: 0.3895\n",
            "Epoch 11/20\n",
            "131559/131559 [==============================] - 3s 20us/sample - loss: 1.8111 - acc: 0.3742 - val_loss: 1.8391 - val_acc: 0.3851\n",
            "Epoch 12/20\n",
            "131559/131559 [==============================] - 3s 19us/sample - loss: 1.8086 - acc: 0.3731 - val_loss: 1.8250 - val_acc: 0.3921\n",
            "Epoch 13/20\n",
            "131559/131559 [==============================] - 3s 19us/sample - loss: 1.8063 - acc: 0.3749 - val_loss: 1.8130 - val_acc: 0.3942\n",
            "Epoch 14/20\n",
            "131559/131559 [==============================] - 3s 19us/sample - loss: 1.8044 - acc: 0.3749 - val_loss: 1.8315 - val_acc: 0.3896\n",
            "Epoch 15/20\n",
            "131559/131559 [==============================] - 3s 19us/sample - loss: 1.8032 - acc: 0.3755 - val_loss: 1.8372 - val_acc: 0.3847\n",
            "Epoch 16/20\n",
            "131559/131559 [==============================] - 3s 19us/sample - loss: 1.8018 - acc: 0.3753 - val_loss: 1.8214 - val_acc: 0.3898\n",
            "Epoch 17/20\n",
            "131559/131559 [==============================] - 2s 19us/sample - loss: 1.7999 - acc: 0.3760 - val_loss: 1.8349 - val_acc: 0.3842\n",
            "Epoch 18/20\n",
            "131559/131559 [==============================] - 3s 19us/sample - loss: 1.7990 - acc: 0.3760 - val_loss: 1.8241 - val_acc: 0.3895\n",
            "Epoch 19/20\n",
            "131559/131559 [==============================] - 2s 19us/sample - loss: 1.7982 - acc: 0.3765 - val_loss: 1.8197 - val_acc: 0.3912\n",
            "Epoch 20/20\n",
            "131559/131559 [==============================] - 2s 19us/sample - loss: 1.7969 - acc: 0.3773 - val_loss: 1.8168 - val_acc: 0.3926\n",
            "Best: 0.36832100749015806 using {'optimizer': 'Adam'}\n",
            "Means: 0.3679500579833984, Stdev: 0.016786607351509133 with: {'optimizer': 'SGD'}\n",
            "Means: 0.36746969223022463, Stdev: 0.018483988165786305 with: {'optimizer': 'RMSprop'}\n",
            "Means: 0.3644596338272095, Stdev: 0.017298559964720555 with: {'optimizer': 'Adagrad'}\n",
            "Means: 0.3644291996955872, Stdev: 0.014714839412015226 with: {'optimizer': 'Adadelta'}\n",
            "Means: 0.36832100749015806, Stdev: 0.017837141320071373 with: {'optimizer': 'Adam'}\n",
            "Means: 0.36800476908683777, Stdev: 0.01658343332838925 with: {'optimizer': 'Adamax'}\n",
            "Means: 0.3669467270374298, Stdev: 0.018438780436616186 with: {'optimizer': 'Nadam'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXCtFmJbRG1W",
        "colab_type": "text"
      },
      "source": [
        "### learning Rate.*(0.001)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPrib6ktRIz4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function for NN model.\n",
        "def create_model(learn_rate=0.01):\n",
        "    # set the model.\n",
        "    model = Sequential()\n",
        "    # hidden layers.\n",
        "    model.add(Dense(48, input_dim=13, activation='relu'))\n",
        "    #model.add(Dropout(0.2))\n",
        "    model.add(Dense(24, activation='relu'))\n",
        "    #model.add(Dropout(0.2))\n",
        "    model.add(Dense(12,activation='softmax'))\n",
        "    # function for top 3.\n",
        "    def top_3_accuracy(y_true, y_pred):\n",
        "        return top_k_categorical_accuracy(y_true, y_pred, k=3)\n",
        "\n",
        "    # set the optimizer.\n",
        "    optimizer = Adam(lr=learn_rate)\n",
        "    # compile the model.\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                        optimizer=optimizer,\n",
        "                        metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_zRbzR7RI2D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set the model with KerasClassifier.\n",
        "model = KerasClassifier(build_fn=create_model, epochs=20, batch_size=200, validation_split=0.2, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2Rz4wBlRI6K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set the parameters for learn rate.\n",
        "learn_rate = [0, 0.001, 0.01, 0.05]\n",
        "param_grid = dict(learn_rate=learn_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOGaWkCNRI4P",
        "colab_type": "code",
        "outputId": "792a19a9-4278-476f-9bf9-741e4a76b305",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# set the gridsearch.\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
        "grid_result = grid.fit(X_scaled, y_train)\n",
        "# show the results.\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 4s 33us/sample - loss: 2.6479 - acc: 0.0516 - val_loss: 2.6658 - val_acc: 0.0615\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.6479 - acc: 0.0516 - val_loss: 2.6658 - val_acc: 0.0615\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.6479 - acc: 0.0516 - val_loss: 2.6658 - val_acc: 0.0615\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.6479 - acc: 0.0516 - val_loss: 2.6658 - val_acc: 0.0615\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.6479 - acc: 0.0516 - val_loss: 2.6658 - val_acc: 0.0615\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 2.6479 - acc: 0.0516 - val_loss: 2.6658 - val_acc: 0.0615\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.6479 - acc: 0.0516 - val_loss: 2.6658 - val_acc: 0.0615\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.6479 - acc: 0.0516 - val_loss: 2.6658 - val_acc: 0.0615\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.6479 - acc: 0.0516 - val_loss: 2.6658 - val_acc: 0.0615\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.6479 - acc: 0.0516 - val_loss: 2.6658 - val_acc: 0.0615\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.6479 - acc: 0.0516 - val_loss: 2.6658 - val_acc: 0.0615\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 2.6479 - acc: 0.0516 - val_loss: 2.6658 - val_acc: 0.0615\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.6479 - acc: 0.0516 - val_loss: 2.6658 - val_acc: 0.0615\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 2.6479 - acc: 0.0516 - val_loss: 2.6658 - val_acc: 0.0615\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 2.6479 - acc: 0.0516 - val_loss: 2.6658 - val_acc: 0.0615\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 2.6479 - acc: 0.0516 - val_loss: 2.6658 - val_acc: 0.0615\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.6479 - acc: 0.0516 - val_loss: 2.6658 - val_acc: 0.0615\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 2.6479 - acc: 0.0516 - val_loss: 2.6658 - val_acc: 0.0615\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 2.6479 - acc: 0.0516 - val_loss: 2.6658 - val_acc: 0.0615\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.6479 - acc: 0.0516 - val_loss: 2.6658 - val_acc: 0.0615\n",
            "32890/32890 [==============================] - 0s 14us/sample - loss: 2.6479 - acc: 0.0415\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 2.4883 - acc: 0.1087 - val_loss: 2.4717 - val_acc: 0.1222\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.4883 - acc: 0.1087 - val_loss: 2.4717 - val_acc: 0.1222\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 2.4883 - acc: 0.1087 - val_loss: 2.4717 - val_acc: 0.1222\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.4883 - acc: 0.1087 - val_loss: 2.4717 - val_acc: 0.1222\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.4883 - acc: 0.1087 - val_loss: 2.4717 - val_acc: 0.1222\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 2.4883 - acc: 0.1087 - val_loss: 2.4717 - val_acc: 0.1222\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 2.4883 - acc: 0.1087 - val_loss: 2.4717 - val_acc: 0.1222\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.4883 - acc: 0.1087 - val_loss: 2.4717 - val_acc: 0.1222\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.4883 - acc: 0.1087 - val_loss: 2.4717 - val_acc: 0.1222\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.4883 - acc: 0.1087 - val_loss: 2.4717 - val_acc: 0.1222\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 2.4883 - acc: 0.1087 - val_loss: 2.4717 - val_acc: 0.1222\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.4883 - acc: 0.1087 - val_loss: 2.4717 - val_acc: 0.1222\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 2.4883 - acc: 0.1087 - val_loss: 2.4717 - val_acc: 0.1222\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 2.4883 - acc: 0.1087 - val_loss: 2.4717 - val_acc: 0.1222\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 2.4883 - acc: 0.1087 - val_loss: 2.4717 - val_acc: 0.1222\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.4883 - acc: 0.1087 - val_loss: 2.4717 - val_acc: 0.1222\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 2.4883 - acc: 0.1087 - val_loss: 2.4717 - val_acc: 0.1222\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.4883 - acc: 0.1087 - val_loss: 2.4717 - val_acc: 0.1222\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 2.4883 - acc: 0.1087 - val_loss: 2.4717 - val_acc: 0.1222\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 2.4883 - acc: 0.1087 - val_loss: 2.4717 - val_acc: 0.1222\n",
            "32890/32890 [==============================] - 0s 13us/sample - loss: 2.4847 - acc: 0.1071\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 2.5832 - acc: 0.1069 - val_loss: 2.6349 - val_acc: 0.0903\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.5832 - acc: 0.1069 - val_loss: 2.6349 - val_acc: 0.0903\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 2.5832 - acc: 0.1069 - val_loss: 2.6349 - val_acc: 0.0903\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.5832 - acc: 0.1069 - val_loss: 2.6349 - val_acc: 0.0903\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.5832 - acc: 0.1069 - val_loss: 2.6349 - val_acc: 0.0903\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.5832 - acc: 0.1069 - val_loss: 2.6349 - val_acc: 0.0903\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 2.5832 - acc: 0.1069 - val_loss: 2.6349 - val_acc: 0.0903\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 2.5832 - acc: 0.1069 - val_loss: 2.6349 - val_acc: 0.0903\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.5832 - acc: 0.1069 - val_loss: 2.6349 - val_acc: 0.0903\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.5832 - acc: 0.1069 - val_loss: 2.6349 - val_acc: 0.0903\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.5832 - acc: 0.1069 - val_loss: 2.6349 - val_acc: 0.0903\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.5832 - acc: 0.1069 - val_loss: 2.6349 - val_acc: 0.0903\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.5832 - acc: 0.1069 - val_loss: 2.6349 - val_acc: 0.0903\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 2.5832 - acc: 0.1069 - val_loss: 2.6349 - val_acc: 0.0903\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.5832 - acc: 0.1069 - val_loss: 2.6349 - val_acc: 0.0903\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.5832 - acc: 0.1069 - val_loss: 2.6349 - val_acc: 0.0903\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.5832 - acc: 0.1069 - val_loss: 2.6349 - val_acc: 0.0903\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.5832 - acc: 0.1069 - val_loss: 2.6349 - val_acc: 0.0903\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.5832 - acc: 0.1069 - val_loss: 2.6349 - val_acc: 0.0903\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.5832 - acc: 0.1069 - val_loss: 2.6349 - val_acc: 0.0903\n",
            "32890/32890 [==============================] - 0s 13us/sample - loss: 2.6416 - acc: 0.0997\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 2.6333 - acc: 0.0580 - val_loss: 2.6630 - val_acc: 0.0675\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.6333 - acc: 0.0580 - val_loss: 2.6630 - val_acc: 0.0675\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.6333 - acc: 0.0580 - val_loss: 2.6630 - val_acc: 0.0675\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.6333 - acc: 0.0580 - val_loss: 2.6630 - val_acc: 0.0675\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.6333 - acc: 0.0580 - val_loss: 2.6630 - val_acc: 0.0675\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.6333 - acc: 0.0580 - val_loss: 2.6630 - val_acc: 0.0675\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.6333 - acc: 0.0580 - val_loss: 2.6630 - val_acc: 0.0675\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.6333 - acc: 0.0580 - val_loss: 2.6630 - val_acc: 0.0675\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.6333 - acc: 0.0580 - val_loss: 2.6630 - val_acc: 0.0675\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.6333 - acc: 0.0580 - val_loss: 2.6630 - val_acc: 0.0675\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 2.6333 - acc: 0.0580 - val_loss: 2.6630 - val_acc: 0.0675\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.6333 - acc: 0.0580 - val_loss: 2.6630 - val_acc: 0.0675\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.6333 - acc: 0.0580 - val_loss: 2.6630 - val_acc: 0.0675\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 2.6333 - acc: 0.0580 - val_loss: 2.6630 - val_acc: 0.0675\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.6333 - acc: 0.0580 - val_loss: 2.6630 - val_acc: 0.0675\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.6333 - acc: 0.0580 - val_loss: 2.6630 - val_acc: 0.0675\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.6333 - acc: 0.0580 - val_loss: 2.6630 - val_acc: 0.0675\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.6333 - acc: 0.0580 - val_loss: 2.6630 - val_acc: 0.0675\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.6333 - acc: 0.0580 - val_loss: 2.6630 - val_acc: 0.0675\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 2.6333 - acc: 0.0580 - val_loss: 2.6630 - val_acc: 0.0675\n",
            "32890/32890 [==============================] - 0s 14us/sample - loss: 2.6367 - acc: 0.0576\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 4s 35us/sample - loss: 2.5110 - acc: 0.1100 - val_loss: 2.5700 - val_acc: 0.0595\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 2.5110 - acc: 0.1100 - val_loss: 2.5700 - val_acc: 0.0595\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 2.5110 - acc: 0.1100 - val_loss: 2.5700 - val_acc: 0.0595\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 2.5110 - acc: 0.1100 - val_loss: 2.5700 - val_acc: 0.0595\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 2.5110 - acc: 0.1100 - val_loss: 2.5700 - val_acc: 0.0595\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 2s 21us/sample - loss: 2.5110 - acc: 0.1100 - val_loss: 2.5700 - val_acc: 0.0595\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 2.5110 - acc: 0.1100 - val_loss: 2.5700 - val_acc: 0.0595\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 2.5110 - acc: 0.1100 - val_loss: 2.5700 - val_acc: 0.0595\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 2.5110 - acc: 0.1100 - val_loss: 2.5700 - val_acc: 0.0595\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 2.5110 - acc: 0.1100 - val_loss: 2.5700 - val_acc: 0.0595\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 2.5110 - acc: 0.1100 - val_loss: 2.5700 - val_acc: 0.0595\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 2.5110 - acc: 0.1100 - val_loss: 2.5700 - val_acc: 0.0595\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 2s 19us/sample - loss: 2.5110 - acc: 0.1100 - val_loss: 2.5700 - val_acc: 0.0595\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 2.5110 - acc: 0.1100 - val_loss: 2.5700 - val_acc: 0.0595\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 2.5110 - acc: 0.1100 - val_loss: 2.5700 - val_acc: 0.0595\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 2.5110 - acc: 0.1100 - val_loss: 2.5700 - val_acc: 0.0595\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 2.5110 - acc: 0.1100 - val_loss: 2.5700 - val_acc: 0.0595\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 2.5110 - acc: 0.1100 - val_loss: 2.5700 - val_acc: 0.0595\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 2.5110 - acc: 0.1100 - val_loss: 2.5700 - val_acc: 0.0595\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 2s 21us/sample - loss: 2.5110 - acc: 0.1100 - val_loss: 2.5700 - val_acc: 0.0595\n",
            "32889/32889 [==============================] - 0s 15us/sample - loss: 2.5215 - acc: 0.0898\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 2.0169 - acc: 0.3096 - val_loss: 1.8588 - val_acc: 0.3747\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8856 - acc: 0.3527 - val_loss: 1.8319 - val_acc: 0.3886\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8645 - acc: 0.3586 - val_loss: 1.8276 - val_acc: 0.3853\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8512 - acc: 0.3621 - val_loss: 1.8244 - val_acc: 0.3823\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8417 - acc: 0.3644 - val_loss: 1.8160 - val_acc: 0.3870\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8350 - acc: 0.3664 - val_loss: 1.8120 - val_acc: 0.3874\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8294 - acc: 0.3687 - val_loss: 1.8017 - val_acc: 0.3962\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8245 - acc: 0.3694 - val_loss: 1.8031 - val_acc: 0.3927\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8209 - acc: 0.3702 - val_loss: 1.7935 - val_acc: 0.3988\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8173 - acc: 0.3737 - val_loss: 1.8021 - val_acc: 0.3914\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8144 - acc: 0.3734 - val_loss: 1.7976 - val_acc: 0.3967\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8114 - acc: 0.3749 - val_loss: 1.8005 - val_acc: 0.3929\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8088 - acc: 0.3753 - val_loss: 1.8042 - val_acc: 0.3908\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8068 - acc: 0.3758 - val_loss: 1.7976 - val_acc: 0.3959\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8047 - acc: 0.3769 - val_loss: 1.7868 - val_acc: 0.3976\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8030 - acc: 0.3777 - val_loss: 1.7881 - val_acc: 0.3982\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8014 - acc: 0.3782 - val_loss: 1.7904 - val_acc: 0.3975\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8001 - acc: 0.3784 - val_loss: 1.7988 - val_acc: 0.3946\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.7989 - acc: 0.3786 - val_loss: 1.7871 - val_acc: 0.4016\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.7980 - acc: 0.3789 - val_loss: 1.7988 - val_acc: 0.3957\n",
            "32890/32890 [==============================] - 0s 14us/sample - loss: 1.8386 - acc: 0.3676\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 2.0066 - acc: 0.3113 - val_loss: 1.8698 - val_acc: 0.3687\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8627 - acc: 0.3619 - val_loss: 1.8431 - val_acc: 0.3783\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8434 - acc: 0.3692 - val_loss: 1.8337 - val_acc: 0.3832\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8316 - acc: 0.3726 - val_loss: 1.8172 - val_acc: 0.3878\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8231 - acc: 0.3757 - val_loss: 1.8245 - val_acc: 0.3802\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8163 - acc: 0.3781 - val_loss: 1.8277 - val_acc: 0.3817\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8111 - acc: 0.3796 - val_loss: 1.8180 - val_acc: 0.3840\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8068 - acc: 0.3817 - val_loss: 1.8104 - val_acc: 0.3878\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8035 - acc: 0.3818 - val_loss: 1.8152 - val_acc: 0.3840\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8003 - acc: 0.3825 - val_loss: 1.8134 - val_acc: 0.3907\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.7972 - acc: 0.3847 - val_loss: 1.8144 - val_acc: 0.3811\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.7946 - acc: 0.3851 - val_loss: 1.8078 - val_acc: 0.3892\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.7923 - acc: 0.3856 - val_loss: 1.8115 - val_acc: 0.3882\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.7903 - acc: 0.3858 - val_loss: 1.8145 - val_acc: 0.3852\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.7887 - acc: 0.3863 - val_loss: 1.7958 - val_acc: 0.3933\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.7868 - acc: 0.3876 - val_loss: 1.8075 - val_acc: 0.3893\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.7852 - acc: 0.3872 - val_loss: 1.8030 - val_acc: 0.3889\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.7841 - acc: 0.3883 - val_loss: 1.8007 - val_acc: 0.3916\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.7826 - acc: 0.3884 - val_loss: 1.7928 - val_acc: 0.3953\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.7814 - acc: 0.3889 - val_loss: 1.8048 - val_acc: 0.3876\n",
            "32890/32890 [==============================] - 0s 13us/sample - loss: 1.8900 - acc: 0.3395\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 2.0240 - acc: 0.3031 - val_loss: 1.8828 - val_acc: 0.3662\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8944 - acc: 0.3468 - val_loss: 1.8764 - val_acc: 0.3688\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8735 - acc: 0.3526 - val_loss: 1.8526 - val_acc: 0.3722\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8609 - acc: 0.3559 - val_loss: 1.8446 - val_acc: 0.3758\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8520 - acc: 0.3599 - val_loss: 1.8411 - val_acc: 0.3795\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8455 - acc: 0.3598 - val_loss: 1.8301 - val_acc: 0.3802\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8404 - acc: 0.3627 - val_loss: 1.8342 - val_acc: 0.3772\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8363 - acc: 0.3641 - val_loss: 1.8319 - val_acc: 0.3783\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8323 - acc: 0.3649 - val_loss: 1.8384 - val_acc: 0.3736\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8292 - acc: 0.3664 - val_loss: 1.8150 - val_acc: 0.3864\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8263 - acc: 0.3666 - val_loss: 1.8301 - val_acc: 0.3810\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8241 - acc: 0.3682 - val_loss: 1.8164 - val_acc: 0.3861\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8222 - acc: 0.3691 - val_loss: 1.8268 - val_acc: 0.3831\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8196 - acc: 0.3696 - val_loss: 1.8087 - val_acc: 0.3899\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8183 - acc: 0.3701 - val_loss: 1.8209 - val_acc: 0.3830\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8163 - acc: 0.3706 - val_loss: 1.8149 - val_acc: 0.3852\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8151 - acc: 0.3719 - val_loss: 1.8107 - val_acc: 0.3897\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8132 - acc: 0.3715 - val_loss: 1.8184 - val_acc: 0.3823\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8117 - acc: 0.3720 - val_loss: 1.8160 - val_acc: 0.3834\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8107 - acc: 0.3729 - val_loss: 1.8121 - val_acc: 0.3861\n",
            "32890/32890 [==============================] - 0s 14us/sample - loss: 1.8031 - acc: 0.3846\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 2.0324 - acc: 0.3040 - val_loss: 1.9021 - val_acc: 0.3575\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8810 - acc: 0.3542 - val_loss: 1.8811 - val_acc: 0.3604\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8620 - acc: 0.3602 - val_loss: 1.8627 - val_acc: 0.3629\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8503 - acc: 0.3627 - val_loss: 1.8474 - val_acc: 0.3748\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8424 - acc: 0.3653 - val_loss: 1.8373 - val_acc: 0.3812\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8360 - acc: 0.3677 - val_loss: 1.8400 - val_acc: 0.3750\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8309 - acc: 0.3698 - val_loss: 1.8401 - val_acc: 0.3768\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8268 - acc: 0.3707 - val_loss: 1.8408 - val_acc: 0.3753\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8234 - acc: 0.3718 - val_loss: 1.8432 - val_acc: 0.3778\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8205 - acc: 0.3731 - val_loss: 1.8317 - val_acc: 0.3814\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8173 - acc: 0.3733 - val_loss: 1.8302 - val_acc: 0.3846\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8151 - acc: 0.3738 - val_loss: 1.8230 - val_acc: 0.3839\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8126 - acc: 0.3747 - val_loss: 1.8283 - val_acc: 0.3804\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8102 - acc: 0.3765 - val_loss: 1.8187 - val_acc: 0.3859\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8086 - acc: 0.3770 - val_loss: 1.8256 - val_acc: 0.3805\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8072 - acc: 0.3764 - val_loss: 1.8259 - val_acc: 0.3837\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8059 - acc: 0.3778 - val_loss: 1.8302 - val_acc: 0.3805\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8046 - acc: 0.3782 - val_loss: 1.8236 - val_acc: 0.3854\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8032 - acc: 0.3778 - val_loss: 1.8288 - val_acc: 0.3815\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8020 - acc: 0.3789 - val_loss: 1.8205 - val_acc: 0.3804\n",
            "32890/32890 [==============================] - 0s 14us/sample - loss: 1.8311 - acc: 0.3662\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 4s 35us/sample - loss: 2.0044 - acc: 0.3127 - val_loss: 1.8898 - val_acc: 0.3480\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8803 - acc: 0.3515 - val_loss: 1.8740 - val_acc: 0.3471\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8598 - acc: 0.3580 - val_loss: 1.8564 - val_acc: 0.3505\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8488 - acc: 0.3607 - val_loss: 1.8364 - val_acc: 0.3560\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8403 - acc: 0.3641 - val_loss: 1.8214 - val_acc: 0.3655\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8345 - acc: 0.3658 - val_loss: 1.8385 - val_acc: 0.3514\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8298 - acc: 0.3671 - val_loss: 1.8225 - val_acc: 0.3591\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8259 - acc: 0.3692 - val_loss: 1.8124 - val_acc: 0.3642\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 2s 21us/sample - loss: 1.8229 - acc: 0.3695 - val_loss: 1.8157 - val_acc: 0.3613\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8197 - acc: 0.3712 - val_loss: 1.8210 - val_acc: 0.3612\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8176 - acc: 0.3716 - val_loss: 1.8208 - val_acc: 0.3620\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8158 - acc: 0.3725 - val_loss: 1.8090 - val_acc: 0.3679\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8139 - acc: 0.3725 - val_loss: 1.8066 - val_acc: 0.3647\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8120 - acc: 0.3736 - val_loss: 1.8145 - val_acc: 0.3626\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8104 - acc: 0.3739 - val_loss: 1.8031 - val_acc: 0.3685\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8089 - acc: 0.3751 - val_loss: 1.8163 - val_acc: 0.3650\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8078 - acc: 0.3752 - val_loss: 1.8049 - val_acc: 0.3695\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8067 - acc: 0.3757 - val_loss: 1.8247 - val_acc: 0.3567\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8050 - acc: 0.3769 - val_loss: 1.8205 - val_acc: 0.3630\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8041 - acc: 0.3762 - val_loss: 1.8230 - val_acc: 0.3591\n",
            "32889/32889 [==============================] - 0s 13us/sample - loss: 1.8245 - acc: 0.3870\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.9011 - acc: 0.3446 - val_loss: 1.8309 - val_acc: 0.3800\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8473 - acc: 0.3624 - val_loss: 1.8130 - val_acc: 0.3858\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8359 - acc: 0.3674 - val_loss: 1.8066 - val_acc: 0.3910\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8256 - acc: 0.3703 - val_loss: 1.8053 - val_acc: 0.3891\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8203 - acc: 0.3726 - val_loss: 1.8061 - val_acc: 0.3980\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8156 - acc: 0.3733 - val_loss: 1.8298 - val_acc: 0.3879\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8145 - acc: 0.3737 - val_loss: 1.8027 - val_acc: 0.3975\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8113 - acc: 0.3751 - val_loss: 1.8000 - val_acc: 0.3963\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8100 - acc: 0.3765 - val_loss: 1.7970 - val_acc: 0.3969\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8092 - acc: 0.3760 - val_loss: 1.8167 - val_acc: 0.3936\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8064 - acc: 0.3772 - val_loss: 1.7819 - val_acc: 0.4046\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8049 - acc: 0.3786 - val_loss: 1.7808 - val_acc: 0.4045\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8043 - acc: 0.3767 - val_loss: 1.8292 - val_acc: 0.3858\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8042 - acc: 0.3780 - val_loss: 1.8109 - val_acc: 0.3877\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8034 - acc: 0.3771 - val_loss: 1.7872 - val_acc: 0.4023\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8022 - acc: 0.3781 - val_loss: 1.7873 - val_acc: 0.4072\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8014 - acc: 0.3779 - val_loss: 1.7924 - val_acc: 0.3968\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8019 - acc: 0.3781 - val_loss: 1.8053 - val_acc: 0.3964\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8010 - acc: 0.3799 - val_loss: 1.7870 - val_acc: 0.4074\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.7998 - acc: 0.3800 - val_loss: 1.7802 - val_acc: 0.4047\n",
            "32890/32890 [==============================] - 0s 14us/sample - loss: 1.8918 - acc: 0.3455\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.8854 - acc: 0.3522 - val_loss: 1.8369 - val_acc: 0.3798\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8300 - acc: 0.3716 - val_loss: 1.8148 - val_acc: 0.3916\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8154 - acc: 0.3771 - val_loss: 1.8189 - val_acc: 0.3830\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8086 - acc: 0.3789 - val_loss: 1.8376 - val_acc: 0.3816\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8016 - acc: 0.3825 - val_loss: 1.8403 - val_acc: 0.3691\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.7989 - acc: 0.3833 - val_loss: 1.8144 - val_acc: 0.3894\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.7970 - acc: 0.3847 - val_loss: 1.8221 - val_acc: 0.3845\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.7944 - acc: 0.3846 - val_loss: 1.8086 - val_acc: 0.3910\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.7919 - acc: 0.3855 - val_loss: 1.8240 - val_acc: 0.3810\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.7910 - acc: 0.3862 - val_loss: 1.8135 - val_acc: 0.3902\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.7898 - acc: 0.3859 - val_loss: 1.8009 - val_acc: 0.3968\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.7882 - acc: 0.3870 - val_loss: 1.8213 - val_acc: 0.3863\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.7863 - acc: 0.3872 - val_loss: 1.8065 - val_acc: 0.3855\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.7857 - acc: 0.3876 - val_loss: 1.8278 - val_acc: 0.3896\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.7852 - acc: 0.3873 - val_loss: 1.7879 - val_acc: 0.3966\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.7835 - acc: 0.3875 - val_loss: 1.7978 - val_acc: 0.3984\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.7830 - acc: 0.3890 - val_loss: 1.8033 - val_acc: 0.3842\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.7820 - acc: 0.3888 - val_loss: 1.8121 - val_acc: 0.3912\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.7820 - acc: 0.3883 - val_loss: 1.8207 - val_acc: 0.3834\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.7807 - acc: 0.3904 - val_loss: 1.8041 - val_acc: 0.3884\n",
            "32890/32890 [==============================] - 0s 14us/sample - loss: 1.8983 - acc: 0.3348\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.9104 - acc: 0.3427 - val_loss: 1.8262 - val_acc: 0.3852\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8572 - acc: 0.3585 - val_loss: 1.8463 - val_acc: 0.3719\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8438 - acc: 0.3629 - val_loss: 1.8435 - val_acc: 0.3716\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8361 - acc: 0.3641 - val_loss: 1.8490 - val_acc: 0.3704\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8323 - acc: 0.3656 - val_loss: 1.8556 - val_acc: 0.3661\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8277 - acc: 0.3678 - val_loss: 1.8170 - val_acc: 0.3784\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8258 - acc: 0.3679 - val_loss: 1.8272 - val_acc: 0.3803\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8242 - acc: 0.3693 - val_loss: 1.8223 - val_acc: 0.3798\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8215 - acc: 0.3693 - val_loss: 1.8216 - val_acc: 0.3803\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8208 - acc: 0.3696 - val_loss: 1.8114 - val_acc: 0.3881\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8190 - acc: 0.3712 - val_loss: 1.8063 - val_acc: 0.3881\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8179 - acc: 0.3712 - val_loss: 1.8223 - val_acc: 0.3842\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8165 - acc: 0.3710 - val_loss: 1.8431 - val_acc: 0.3645\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8151 - acc: 0.3713 - val_loss: 1.8260 - val_acc: 0.3763\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8173 - acc: 0.3706 - val_loss: 1.8251 - val_acc: 0.3777\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8154 - acc: 0.3705 - val_loss: 1.8436 - val_acc: 0.3708\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8148 - acc: 0.3716 - val_loss: 1.8186 - val_acc: 0.3807\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8138 - acc: 0.3727 - val_loss: 1.8400 - val_acc: 0.3712\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8129 - acc: 0.3720 - val_loss: 1.8179 - val_acc: 0.3860\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8130 - acc: 0.3730 - val_loss: 1.8279 - val_acc: 0.3778\n",
            "32890/32890 [==============================] - 0s 14us/sample - loss: 1.8069 - acc: 0.3808\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.9046 - acc: 0.3446 - val_loss: 1.8734 - val_acc: 0.3628\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8479 - acc: 0.3636 - val_loss: 1.8396 - val_acc: 0.3812\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8364 - acc: 0.3663 - val_loss: 1.8233 - val_acc: 0.3927\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8310 - acc: 0.3692 - val_loss: 1.8604 - val_acc: 0.3742\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8270 - acc: 0.3696 - val_loss: 1.8516 - val_acc: 0.3753\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8226 - acc: 0.3715 - val_loss: 1.8655 - val_acc: 0.3687\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8201 - acc: 0.3720 - val_loss: 1.8442 - val_acc: 0.3763\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8185 - acc: 0.3738 - val_loss: 1.8293 - val_acc: 0.3823\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8183 - acc: 0.3727 - val_loss: 1.8234 - val_acc: 0.3809\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8169 - acc: 0.3749 - val_loss: 1.8333 - val_acc: 0.3797\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8140 - acc: 0.3743 - val_loss: 1.8233 - val_acc: 0.3836\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8112 - acc: 0.3752 - val_loss: 1.8580 - val_acc: 0.3641\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8108 - acc: 0.3753 - val_loss: 1.8267 - val_acc: 0.3881\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8103 - acc: 0.3746 - val_loss: 1.8569 - val_acc: 0.3680\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8095 - acc: 0.3761 - val_loss: 1.8457 - val_acc: 0.3752\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8075 - acc: 0.3773 - val_loss: 1.8413 - val_acc: 0.3762\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8068 - acc: 0.3769 - val_loss: 1.8347 - val_acc: 0.3792\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8067 - acc: 0.3765 - val_loss: 1.8584 - val_acc: 0.3677\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8063 - acc: 0.3770 - val_loss: 1.8386 - val_acc: 0.3785\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8051 - acc: 0.3774 - val_loss: 1.8138 - val_acc: 0.3913\n",
            "32890/32890 [==============================] - 0s 14us/sample - loss: 1.8412 - acc: 0.3651\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 4s 37us/sample - loss: 1.9042 - acc: 0.3403 - val_loss: 1.8397 - val_acc: 0.3573\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 2s 21us/sample - loss: 1.8555 - acc: 0.3579 - val_loss: 1.8292 - val_acc: 0.3620\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 2s 21us/sample - loss: 1.8413 - acc: 0.3644 - val_loss: 1.8265 - val_acc: 0.3671\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 2s 21us/sample - loss: 1.8351 - acc: 0.3653 - val_loss: 1.8424 - val_acc: 0.3556\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 2s 21us/sample - loss: 1.8301 - acc: 0.3665 - val_loss: 1.8483 - val_acc: 0.3475\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 2s 21us/sample - loss: 1.8266 - acc: 0.3696 - val_loss: 1.8187 - val_acc: 0.3644\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8247 - acc: 0.3692 - val_loss: 1.8396 - val_acc: 0.3603\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8219 - acc: 0.3701 - val_loss: 1.8344 - val_acc: 0.3607\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 2s 21us/sample - loss: 1.8214 - acc: 0.3723 - val_loss: 1.8179 - val_acc: 0.3635\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 2s 21us/sample - loss: 1.8183 - acc: 0.3720 - val_loss: 1.8148 - val_acc: 0.3669\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8171 - acc: 0.3725 - val_loss: 1.8122 - val_acc: 0.3628\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8155 - acc: 0.3719 - val_loss: 1.8374 - val_acc: 0.3574\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8155 - acc: 0.3745 - val_loss: 1.8093 - val_acc: 0.3689\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8138 - acc: 0.3738 - val_loss: 1.8312 - val_acc: 0.3560\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8136 - acc: 0.3743 - val_loss: 1.8243 - val_acc: 0.3655\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8127 - acc: 0.3730 - val_loss: 1.8327 - val_acc: 0.3579\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 2s 21us/sample - loss: 1.8094 - acc: 0.3744 - val_loss: 1.8196 - val_acc: 0.3720\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8115 - acc: 0.3754 - val_loss: 1.8107 - val_acc: 0.3733\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8107 - acc: 0.3741 - val_loss: 1.8278 - val_acc: 0.3703\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8088 - acc: 0.3740 - val_loss: 1.8190 - val_acc: 0.3690\n",
            "32889/32889 [==============================] - 0s 14us/sample - loss: 1.8416 - acc: 0.3794\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.9549 - acc: 0.3318 - val_loss: 1.8816 - val_acc: 0.3746\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.9220 - acc: 0.3440 - val_loss: 1.8604 - val_acc: 0.3848\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.9171 - acc: 0.3445 - val_loss: 1.9272 - val_acc: 0.3515\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.9167 - acc: 0.3459 - val_loss: 1.8668 - val_acc: 0.3793\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.9153 - acc: 0.3474 - val_loss: 1.8693 - val_acc: 0.3777\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.9139 - acc: 0.3472 - val_loss: 1.8689 - val_acc: 0.3834\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.9131 - acc: 0.3476 - val_loss: 1.8724 - val_acc: 0.3836\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.9140 - acc: 0.3471 - val_loss: 1.9299 - val_acc: 0.3507\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.9108 - acc: 0.3489 - val_loss: 1.8548 - val_acc: 0.3856\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.9119 - acc: 0.3491 - val_loss: 1.8879 - val_acc: 0.3747\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.9105 - acc: 0.3474 - val_loss: 1.9121 - val_acc: 0.3476\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.9118 - acc: 0.3486 - val_loss: 1.8964 - val_acc: 0.3723\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.9083 - acc: 0.3492 - val_loss: 1.8927 - val_acc: 0.3722\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.9097 - acc: 0.3481 - val_loss: 1.8594 - val_acc: 0.3798\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.9134 - acc: 0.3475 - val_loss: 1.8649 - val_acc: 0.3789\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.9130 - acc: 0.3486 - val_loss: 1.8613 - val_acc: 0.3763\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.9113 - acc: 0.3480 - val_loss: 1.8847 - val_acc: 0.3632\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.9115 - acc: 0.3489 - val_loss: 1.9031 - val_acc: 0.3679\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.9120 - acc: 0.3483 - val_loss: 1.9226 - val_acc: 0.3557\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.9122 - acc: 0.3479 - val_loss: 1.8886 - val_acc: 0.3721\n",
            "32890/32890 [==============================] - 0s 14us/sample - loss: 1.9315 - acc: 0.3445\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.9408 - acc: 0.3358 - val_loss: 1.9628 - val_acc: 0.3458\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.9110 - acc: 0.3499 - val_loss: 1.9058 - val_acc: 0.3623\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.9021 - acc: 0.3544 - val_loss: 1.9498 - val_acc: 0.3400\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.9030 - acc: 0.3544 - val_loss: 1.8770 - val_acc: 0.3816\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.9011 - acc: 0.3550 - val_loss: 1.9031 - val_acc: 0.3634\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8976 - acc: 0.3557 - val_loss: 1.8900 - val_acc: 0.3725\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8969 - acc: 0.3556 - val_loss: 1.8989 - val_acc: 0.3690\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8956 - acc: 0.3564 - val_loss: 1.8954 - val_acc: 0.3745\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8953 - acc: 0.3572 - val_loss: 1.8723 - val_acc: 0.3721\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8927 - acc: 0.3568 - val_loss: 1.9162 - val_acc: 0.3507\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8955 - acc: 0.3585 - val_loss: 1.8828 - val_acc: 0.3621\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8911 - acc: 0.3571 - val_loss: 1.8601 - val_acc: 0.3732\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8976 - acc: 0.3572 - val_loss: 1.9133 - val_acc: 0.3692\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8916 - acc: 0.3577 - val_loss: 1.8785 - val_acc: 0.3849\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8955 - acc: 0.3565 - val_loss: 1.9061 - val_acc: 0.3691\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8924 - acc: 0.3573 - val_loss: 1.8850 - val_acc: 0.3650\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8948 - acc: 0.3596 - val_loss: 1.9243 - val_acc: 0.3582\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8941 - acc: 0.3577 - val_loss: 1.9071 - val_acc: 0.3697\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8956 - acc: 0.3586 - val_loss: 1.8880 - val_acc: 0.3845\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8972 - acc: 0.3566 - val_loss: 1.8601 - val_acc: 0.3790\n",
            "32890/32890 [==============================] - 0s 15us/sample - loss: 2.0135 - acc: 0.3147\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.9662 - acc: 0.3204 - val_loss: 1.9144 - val_acc: 0.3554\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.9340 - acc: 0.3341 - val_loss: 1.8874 - val_acc: 0.3655\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.9310 - acc: 0.3360 - val_loss: 1.9184 - val_acc: 0.3602\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.9282 - acc: 0.3365 - val_loss: 1.8981 - val_acc: 0.3625\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.9279 - acc: 0.3346 - val_loss: 1.9577 - val_acc: 0.3264\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.9315 - acc: 0.3344 - val_loss: 1.9421 - val_acc: 0.3400\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.9291 - acc: 0.3359 - val_loss: 1.8943 - val_acc: 0.3555\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.9276 - acc: 0.3364 - val_loss: 1.9036 - val_acc: 0.3658\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.9273 - acc: 0.3372 - val_loss: 1.8713 - val_acc: 0.3748\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.9281 - acc: 0.3367 - val_loss: 1.8848 - val_acc: 0.3631\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.9246 - acc: 0.3365 - val_loss: 1.9279 - val_acc: 0.3436\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.9272 - acc: 0.3363 - val_loss: 1.9366 - val_acc: 0.3515\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.9245 - acc: 0.3399 - val_loss: 1.9114 - val_acc: 0.3737\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.9291 - acc: 0.3375 - val_loss: 1.9245 - val_acc: 0.3309\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.9290 - acc: 0.3367 - val_loss: 1.8900 - val_acc: 0.3700\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.9288 - acc: 0.3336 - val_loss: 1.9078 - val_acc: 0.3482\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.9261 - acc: 0.3375 - val_loss: 1.9383 - val_acc: 0.3387\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.9291 - acc: 0.3358 - val_loss: 1.9396 - val_acc: 0.3468\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.9271 - acc: 0.3362 - val_loss: 1.9104 - val_acc: 0.3623\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.9277 - acc: 0.3383 - val_loss: 1.9330 - val_acc: 0.3571\n",
            "32890/32890 [==============================] - 1s 16us/sample - loss: 1.9238 - acc: 0.3447\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.9495 - acc: 0.3312 - val_loss: 1.9323 - val_acc: 0.3638\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.9228 - acc: 0.3404 - val_loss: 1.9810 - val_acc: 0.3264\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.9206 - acc: 0.3407 - val_loss: 1.9045 - val_acc: 0.3615\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.9171 - acc: 0.3430 - val_loss: 1.9504 - val_acc: 0.3407\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.9195 - acc: 0.3427 - val_loss: 1.9163 - val_acc: 0.3694\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.9163 - acc: 0.3428 - val_loss: 1.9611 - val_acc: 0.3197\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.9187 - acc: 0.3432 - val_loss: 1.9175 - val_acc: 0.3541\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.9161 - acc: 0.3427 - val_loss: 1.9243 - val_acc: 0.3542\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.9156 - acc: 0.3425 - val_loss: 1.9213 - val_acc: 0.3500\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.9151 - acc: 0.3432 - val_loss: 1.9581 - val_acc: 0.3440\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.9151 - acc: 0.3435 - val_loss: 1.9188 - val_acc: 0.3701\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.9144 - acc: 0.3446 - val_loss: 1.9021 - val_acc: 0.3678\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.9165 - acc: 0.3434 - val_loss: 1.9074 - val_acc: 0.3521\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.9156 - acc: 0.3419 - val_loss: 1.9131 - val_acc: 0.3549\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.9155 - acc: 0.3441 - val_loss: 1.9086 - val_acc: 0.3566\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.9166 - acc: 0.3429 - val_loss: 1.9566 - val_acc: 0.3387\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.9147 - acc: 0.3435 - val_loss: 1.9203 - val_acc: 0.3643\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.9156 - acc: 0.3433 - val_loss: 1.9615 - val_acc: 0.3572\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.9161 - acc: 0.3430 - val_loss: 1.9272 - val_acc: 0.3538\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.9123 - acc: 0.3455 - val_loss: 1.8982 - val_acc: 0.3670\n",
            "32890/32890 [==============================] - 0s 15us/sample - loss: 1.9280 - acc: 0.3445\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 4s 39us/sample - loss: 1.9564 - acc: 0.3258 - val_loss: 1.9093 - val_acc: 0.3216\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 2s 22us/sample - loss: 1.9276 - acc: 0.3364 - val_loss: 1.9980 - val_acc: 0.2810\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 2s 21us/sample - loss: 1.9252 - acc: 0.3372 - val_loss: 1.8973 - val_acc: 0.3562\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 2s 21us/sample - loss: 1.9214 - acc: 0.3375 - val_loss: 1.8794 - val_acc: 0.3483\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 2s 23us/sample - loss: 1.9215 - acc: 0.3399 - val_loss: 1.8946 - val_acc: 0.3420\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 2s 21us/sample - loss: 1.9193 - acc: 0.3400 - val_loss: 1.9259 - val_acc: 0.3257\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 2s 21us/sample - loss: 1.9205 - acc: 0.3392 - val_loss: 1.9305 - val_acc: 0.3156\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 2s 21us/sample - loss: 1.9207 - acc: 0.3394 - val_loss: 1.9080 - val_acc: 0.3492\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 2s 21us/sample - loss: 1.9210 - acc: 0.3395 - val_loss: 1.8951 - val_acc: 0.3330\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 2s 21us/sample - loss: 1.9177 - acc: 0.3413 - val_loss: 1.9258 - val_acc: 0.3262\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 2s 21us/sample - loss: 1.9189 - acc: 0.3395 - val_loss: 1.9407 - val_acc: 0.3189\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 2s 22us/sample - loss: 1.9180 - acc: 0.3398 - val_loss: 1.8856 - val_acc: 0.3482\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 2s 21us/sample - loss: 1.9175 - acc: 0.3414 - val_loss: 1.9061 - val_acc: 0.3497\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 2s 21us/sample - loss: 1.9169 - acc: 0.3416 - val_loss: 1.9215 - val_acc: 0.3348\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 2s 21us/sample - loss: 1.9196 - acc: 0.3397 - val_loss: 1.9422 - val_acc: 0.3349\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 2s 22us/sample - loss: 1.9201 - acc: 0.3399 - val_loss: 1.8955 - val_acc: 0.3357\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 2s 21us/sample - loss: 1.9202 - acc: 0.3384 - val_loss: 1.8771 - val_acc: 0.3556\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 2s 23us/sample - loss: 1.9177 - acc: 0.3428 - val_loss: 1.9164 - val_acc: 0.3513\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 2s 21us/sample - loss: 1.9195 - acc: 0.3401 - val_loss: 1.8905 - val_acc: 0.3380\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 2s 21us/sample - loss: 1.9173 - acc: 0.3412 - val_loss: 1.9472 - val_acc: 0.3150\n",
            "32889/32889 [==============================] - 0s 15us/sample - loss: 1.9361 - acc: 0.3514\n",
            "Train on 131559 samples, validate on 32890 samples\n",
            "Epoch 1/20\n",
            "131559/131559 [==============================] - 5s 36us/sample - loss: 1.9933 - acc: 0.3162 - val_loss: 1.8855 - val_acc: 0.3761\n",
            "Epoch 2/20\n",
            "131559/131559 [==============================] - 3s 21us/sample - loss: 1.8781 - acc: 0.3526 - val_loss: 1.8705 - val_acc: 0.3758\n",
            "Epoch 3/20\n",
            "131559/131559 [==============================] - 3s 22us/sample - loss: 1.8576 - acc: 0.3582 - val_loss: 1.8570 - val_acc: 0.3824\n",
            "Epoch 4/20\n",
            "131559/131559 [==============================] - 3s 21us/sample - loss: 1.8447 - acc: 0.3616 - val_loss: 1.8457 - val_acc: 0.3842\n",
            "Epoch 5/20\n",
            "131559/131559 [==============================] - 3s 22us/sample - loss: 1.8357 - acc: 0.3643 - val_loss: 1.8457 - val_acc: 0.3843\n",
            "Epoch 6/20\n",
            "131559/131559 [==============================] - 3s 21us/sample - loss: 1.8286 - acc: 0.3666 - val_loss: 1.8435 - val_acc: 0.3853\n",
            "Epoch 7/20\n",
            "131559/131559 [==============================] - 3s 23us/sample - loss: 1.8233 - acc: 0.3686 - val_loss: 1.8426 - val_acc: 0.3807\n",
            "Epoch 8/20\n",
            "131559/131559 [==============================] - 3s 22us/sample - loss: 1.8191 - acc: 0.3708 - val_loss: 1.8367 - val_acc: 0.3860\n",
            "Epoch 9/20\n",
            "131559/131559 [==============================] - 3s 22us/sample - loss: 1.8146 - acc: 0.3711 - val_loss: 1.8352 - val_acc: 0.3852\n",
            "Epoch 10/20\n",
            "131559/131559 [==============================] - 3s 22us/sample - loss: 1.8114 - acc: 0.3728 - val_loss: 1.8242 - val_acc: 0.3880\n",
            "Epoch 11/20\n",
            "131559/131559 [==============================] - 3s 21us/sample - loss: 1.8086 - acc: 0.3734 - val_loss: 1.8244 - val_acc: 0.3882\n",
            "Epoch 12/20\n",
            "131559/131559 [==============================] - 3s 22us/sample - loss: 1.8061 - acc: 0.3751 - val_loss: 1.8341 - val_acc: 0.3838\n",
            "Epoch 13/20\n",
            "131559/131559 [==============================] - 3s 22us/sample - loss: 1.8043 - acc: 0.3750 - val_loss: 1.8240 - val_acc: 0.3915\n",
            "Epoch 14/20\n",
            "131559/131559 [==============================] - 3s 21us/sample - loss: 1.8020 - acc: 0.3756 - val_loss: 1.8190 - val_acc: 0.3879\n",
            "Epoch 15/20\n",
            "131559/131559 [==============================] - 3s 21us/sample - loss: 1.8005 - acc: 0.3757 - val_loss: 1.8283 - val_acc: 0.3878\n",
            "Epoch 16/20\n",
            "131559/131559 [==============================] - 3s 22us/sample - loss: 1.7986 - acc: 0.3763 - val_loss: 1.8272 - val_acc: 0.3848\n",
            "Epoch 17/20\n",
            "131559/131559 [==============================] - 3s 22us/sample - loss: 1.7969 - acc: 0.3769 - val_loss: 1.8215 - val_acc: 0.3883\n",
            "Epoch 18/20\n",
            "131559/131559 [==============================] - 3s 22us/sample - loss: 1.7959 - acc: 0.3778 - val_loss: 1.8206 - val_acc: 0.3892\n",
            "Epoch 19/20\n",
            "131559/131559 [==============================] - 3s 22us/sample - loss: 1.7944 - acc: 0.3781 - val_loss: 1.8296 - val_acc: 0.3864\n",
            "Epoch 20/20\n",
            "131559/131559 [==============================] - 3s 21us/sample - loss: 1.7931 - acc: 0.3787 - val_loss: 1.8192 - val_acc: 0.3894\n",
            "Best: 0.3689777314662933 using {'learn_rate': 0.001}\n",
            "Means: 0.07912489995360375, Stdev: 0.02527369485796117 with: {'learn_rate': 0}\n",
            "Means: 0.3689777314662933, Stdev: 0.017008803185438633 with: {'learn_rate': 0.001}\n",
            "Means: 0.3611151099205017, Stdev: 0.018318309947003317 with: {'learn_rate': 0.01}\n",
            "Means: 0.33995349407196046, Stdev: 0.012919174881221369 with: {'learn_rate': 0.05}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyFaWOZlRjCJ",
        "colab_type": "text"
      },
      "source": [
        "### Activation Function.*(relu)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l287_A5aRk5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function for NN model.\n",
        "def create_model(activation='relu'):\n",
        "    # set the model.\n",
        "    model = Sequential()\n",
        "    # hidden layers.\n",
        "    model.add(Dense(48, input_dim=13, activation=activation))\n",
        "    model.add(Dense(24, activation=activation))\n",
        "    model.add(Dense(12,activation='softmax'))\n",
        "    # function for top 3.\n",
        "    def top_3_accuracy(y_true, y_pred):\n",
        "        return top_k_categorical_accuracy(y_true, y_pred, k=3)\n",
        "\n",
        "    # set the optimizer.\n",
        "    optimizer = Adam(lr=0.001)\n",
        "    # compile the model.\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                        optimizer=optimizer,\n",
        "                        metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbOlMtxERk70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set the model with KerasClassifier.\n",
        "model = KerasClassifier(build_fn=create_model, epochs=20, batch_size=200, validation_split=0.2, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFV9dVgBRk-i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set the parameters for activation.\n",
        "activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
        "param_grid = dict(activation=activation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wo1gqELZSEVu",
        "colab_type": "code",
        "outputId": "799bb2f3-1653-4720-aee1-fd123c4ada2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# set the gridsearch.\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
        "grid_result = grid.fit(X_scaled, y_train)\n",
        "# show the results.\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 4s 41us/sample - loss: 2.4237 - acc: 0.1596 - val_loss: 2.4025 - val_acc: 0.1794\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 2.2519 - acc: 0.2417 - val_loss: 2.1393 - val_acc: 0.3275\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 2.0916 - acc: 0.3023 - val_loss: 2.0286 - val_acc: 0.3390\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 2.0202 - acc: 0.3163 - val_loss: 1.9714 - val_acc: 0.3502\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.9834 - acc: 0.3261 - val_loss: 1.9437 - val_acc: 0.3587\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.9595 - acc: 0.3331 - val_loss: 1.9239 - val_acc: 0.3669\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.9427 - acc: 0.3383 - val_loss: 1.9031 - val_acc: 0.3735\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.9300 - acc: 0.3431 - val_loss: 1.8932 - val_acc: 0.3756\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.9195 - acc: 0.3462 - val_loss: 1.8848 - val_acc: 0.3769\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.9110 - acc: 0.3481 - val_loss: 1.8808 - val_acc: 0.3790\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.9040 - acc: 0.3500 - val_loss: 1.8687 - val_acc: 0.3841\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8981 - acc: 0.3518 - val_loss: 1.8638 - val_acc: 0.3858\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8933 - acc: 0.3530 - val_loss: 1.8615 - val_acc: 0.3845\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8894 - acc: 0.3541 - val_loss: 1.8561 - val_acc: 0.3872\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8858 - acc: 0.3545 - val_loss: 1.8522 - val_acc: 0.3883\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8827 - acc: 0.3559 - val_loss: 1.8519 - val_acc: 0.3872\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8799 - acc: 0.3567 - val_loss: 1.8502 - val_acc: 0.3878\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8770 - acc: 0.3575 - val_loss: 1.8455 - val_acc: 0.3894\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8744 - acc: 0.3583 - val_loss: 1.8435 - val_acc: 0.3889\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8717 - acc: 0.3589 - val_loss: 1.8409 - val_acc: 0.3892\n",
            "32890/32890 [==============================] - 0s 14us/sample - loss: 1.9010 - acc: 0.3455\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 2.4039 - acc: 0.1591 - val_loss: 2.4017 - val_acc: 0.1481\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 2.2296 - acc: 0.2070 - val_loss: 2.1549 - val_acc: 0.2070\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 2.0948 - acc: 0.2784 - val_loss: 2.0697 - val_acc: 0.3200\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 2.0420 - acc: 0.3101 - val_loss: 2.0200 - val_acc: 0.3254\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.9856 - acc: 0.3270 - val_loss: 1.9685 - val_acc: 0.3463\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.9488 - acc: 0.3384 - val_loss: 1.9389 - val_acc: 0.3553\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.9288 - acc: 0.3423 - val_loss: 1.9253 - val_acc: 0.3580\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.9160 - acc: 0.3450 - val_loss: 1.9130 - val_acc: 0.3614\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.9067 - acc: 0.3476 - val_loss: 1.9009 - val_acc: 0.3643\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8991 - acc: 0.3509 - val_loss: 1.8932 - val_acc: 0.3672\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8928 - acc: 0.3530 - val_loss: 1.8888 - val_acc: 0.3686\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8872 - acc: 0.3549 - val_loss: 1.8820 - val_acc: 0.3720\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8816 - acc: 0.3576 - val_loss: 1.8774 - val_acc: 0.3730\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8765 - acc: 0.3606 - val_loss: 1.8731 - val_acc: 0.3728\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8717 - acc: 0.3617 - val_loss: 1.8655 - val_acc: 0.3774\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8669 - acc: 0.3626 - val_loss: 1.8654 - val_acc: 0.3788\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8625 - acc: 0.3650 - val_loss: 1.8581 - val_acc: 0.3812\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8582 - acc: 0.3668 - val_loss: 1.8578 - val_acc: 0.3814\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8542 - acc: 0.3677 - val_loss: 1.8482 - val_acc: 0.3844\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8504 - acc: 0.3687 - val_loss: 1.8479 - val_acc: 0.3840\n",
            "32890/32890 [==============================] - 1s 15us/sample - loss: 1.9415 - acc: 0.3201\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 4s 42us/sample - loss: 2.4104 - acc: 0.1525 - val_loss: 2.4326 - val_acc: 0.1486\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 2.2528 - acc: 0.2017 - val_loss: 2.1955 - val_acc: 0.1856\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 2.1070 - acc: 0.2547 - val_loss: 2.0819 - val_acc: 0.2430\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 2.0381 - acc: 0.2944 - val_loss: 2.0245 - val_acc: 0.3112\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.9980 - acc: 0.3102 - val_loss: 1.9960 - val_acc: 0.3176\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.9771 - acc: 0.3149 - val_loss: 1.9736 - val_acc: 0.3279\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.9617 - acc: 0.3221 - val_loss: 1.9583 - val_acc: 0.3445\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.9489 - acc: 0.3269 - val_loss: 1.9455 - val_acc: 0.3471\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.9390 - acc: 0.3311 - val_loss: 1.9325 - val_acc: 0.3525\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.9309 - acc: 0.3332 - val_loss: 1.9269 - val_acc: 0.3542\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.9238 - acc: 0.3358 - val_loss: 1.9160 - val_acc: 0.3586\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.9175 - acc: 0.3379 - val_loss: 1.9118 - val_acc: 0.3586\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.9116 - acc: 0.3404 - val_loss: 1.9052 - val_acc: 0.3626\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.9059 - acc: 0.3427 - val_loss: 1.8937 - val_acc: 0.3677\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.9006 - acc: 0.3462 - val_loss: 1.8908 - val_acc: 0.3687\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8952 - acc: 0.3480 - val_loss: 1.8823 - val_acc: 0.3719\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8904 - acc: 0.3493 - val_loss: 1.8785 - val_acc: 0.3733\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8864 - acc: 0.3509 - val_loss: 1.8725 - val_acc: 0.3756\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8828 - acc: 0.3520 - val_loss: 1.8715 - val_acc: 0.3744\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8794 - acc: 0.3527 - val_loss: 1.8703 - val_acc: 0.3742\n",
            "32890/32890 [==============================] - 0s 15us/sample - loss: 1.8590 - acc: 0.3691\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 4s 41us/sample - loss: 2.4055 - acc: 0.1497 - val_loss: 2.4295 - val_acc: 0.1468\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 2.2516 - acc: 0.2068 - val_loss: 2.2079 - val_acc: 0.2037\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 2.0979 - acc: 0.2780 - val_loss: 2.0720 - val_acc: 0.2977\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 2.0176 - acc: 0.3125 - val_loss: 2.0116 - val_acc: 0.3229\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.9787 - acc: 0.3265 - val_loss: 1.9815 - val_acc: 0.3380\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.9570 - acc: 0.3339 - val_loss: 1.9634 - val_acc: 0.3471\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.9426 - acc: 0.3381 - val_loss: 1.9533 - val_acc: 0.3508\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.9321 - acc: 0.3396 - val_loss: 1.9380 - val_acc: 0.3568\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.9237 - acc: 0.3422 - val_loss: 1.9305 - val_acc: 0.3577\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.9161 - acc: 0.3446 - val_loss: 1.9227 - val_acc: 0.3622\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.9088 - acc: 0.3482 - val_loss: 1.9134 - val_acc: 0.3664\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.9017 - acc: 0.3509 - val_loss: 1.9077 - val_acc: 0.3705\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8952 - acc: 0.3543 - val_loss: 1.9042 - val_acc: 0.3730\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8892 - acc: 0.3560 - val_loss: 1.8947 - val_acc: 0.3746\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8841 - acc: 0.3574 - val_loss: 1.8882 - val_acc: 0.3769\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8793 - acc: 0.3583 - val_loss: 1.8863 - val_acc: 0.3759\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8751 - acc: 0.3598 - val_loss: 1.8811 - val_acc: 0.3785\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8714 - acc: 0.3600 - val_loss: 1.8818 - val_acc: 0.3761\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8678 - acc: 0.3607 - val_loss: 1.8761 - val_acc: 0.3777\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8649 - acc: 0.3621 - val_loss: 1.8747 - val_acc: 0.3791\n",
            "32890/32890 [==============================] - 0s 15us/sample - loss: 1.8920 - acc: 0.3505\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 4s 40us/sample - loss: 2.4056 - acc: 0.1458 - val_loss: 2.3771 - val_acc: 0.1531\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 2s 23us/sample - loss: 2.2329 - acc: 0.2099 - val_loss: 2.1476 - val_acc: 0.2145\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 2s 22us/sample - loss: 2.0803 - acc: 0.2820 - val_loss: 2.0314 - val_acc: 0.2961\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 2s 23us/sample - loss: 2.0112 - acc: 0.3184 - val_loss: 1.9869 - val_acc: 0.3138\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 2s 22us/sample - loss: 1.9771 - acc: 0.3300 - val_loss: 1.9562 - val_acc: 0.3286\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 2s 22us/sample - loss: 1.9541 - acc: 0.3358 - val_loss: 1.9377 - val_acc: 0.3331\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 2s 22us/sample - loss: 1.9384 - acc: 0.3398 - val_loss: 1.9244 - val_acc: 0.3388\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 2s 22us/sample - loss: 1.9270 - acc: 0.3426 - val_loss: 1.9136 - val_acc: 0.3438\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 3s 24us/sample - loss: 1.9181 - acc: 0.3450 - val_loss: 1.9077 - val_acc: 0.3448\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 2s 23us/sample - loss: 1.9111 - acc: 0.3471 - val_loss: 1.9013 - val_acc: 0.3461\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 2s 23us/sample - loss: 1.9054 - acc: 0.3484 - val_loss: 1.8928 - val_acc: 0.3496\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 2s 23us/sample - loss: 1.9004 - acc: 0.3498 - val_loss: 1.8910 - val_acc: 0.3471\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 2s 23us/sample - loss: 1.8958 - acc: 0.3512 - val_loss: 1.8900 - val_acc: 0.3475\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 2s 23us/sample - loss: 1.8912 - acc: 0.3533 - val_loss: 1.8829 - val_acc: 0.3511\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 2s 22us/sample - loss: 1.8869 - acc: 0.3543 - val_loss: 1.8810 - val_acc: 0.3500\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 2s 23us/sample - loss: 1.8829 - acc: 0.3551 - val_loss: 1.8754 - val_acc: 0.3529\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 2s 22us/sample - loss: 1.8793 - acc: 0.3563 - val_loss: 1.8698 - val_acc: 0.3536\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 2s 23us/sample - loss: 1.8760 - acc: 0.3573 - val_loss: 1.8685 - val_acc: 0.3542\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 2s 22us/sample - loss: 1.8730 - acc: 0.3586 - val_loss: 1.8653 - val_acc: 0.3542\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 2s 22us/sample - loss: 1.8699 - acc: 0.3597 - val_loss: 1.8628 - val_acc: 0.3552\n",
            "32889/32889 [==============================] - 1s 16us/sample - loss: 1.8771 - acc: 0.3826\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 2.0772 - acc: 0.2904 - val_loss: 1.8771 - val_acc: 0.3649\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.9128 - acc: 0.3432 - val_loss: 1.8728 - val_acc: 0.3628\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8944 - acc: 0.3493 - val_loss: 1.8292 - val_acc: 0.3875\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8823 - acc: 0.3526 - val_loss: 1.8398 - val_acc: 0.3740\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8740 - acc: 0.3541 - val_loss: 1.8453 - val_acc: 0.3716\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8677 - acc: 0.3561 - val_loss: 1.8384 - val_acc: 0.3780\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8610 - acc: 0.3583 - val_loss: 1.8227 - val_acc: 0.3841\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8564 - acc: 0.3593 - val_loss: 1.8092 - val_acc: 0.3908\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8515 - acc: 0.3613 - val_loss: 1.8186 - val_acc: 0.3807\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8478 - acc: 0.3624 - val_loss: 1.8003 - val_acc: 0.3944\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8440 - acc: 0.3633 - val_loss: 1.8028 - val_acc: 0.3900\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8412 - acc: 0.3651 - val_loss: 1.8036 - val_acc: 0.3907\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8384 - acc: 0.3650 - val_loss: 1.8130 - val_acc: 0.3869\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8357 - acc: 0.3664 - val_loss: 1.8095 - val_acc: 0.3888\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8332 - acc: 0.3677 - val_loss: 1.8160 - val_acc: 0.3825\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8315 - acc: 0.3678 - val_loss: 1.8016 - val_acc: 0.3891\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8293 - acc: 0.3679 - val_loss: 1.8069 - val_acc: 0.3860\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8275 - acc: 0.3686 - val_loss: 1.7937 - val_acc: 0.3908\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8258 - acc: 0.3690 - val_loss: 1.8013 - val_acc: 0.3913\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8242 - acc: 0.3689 - val_loss: 1.8019 - val_acc: 0.3890\n",
            "32890/32890 [==============================] - 1s 16us/sample - loss: 1.8824 - acc: 0.3466\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 4s 41us/sample - loss: 2.0553 - acc: 0.2997 - val_loss: 1.8942 - val_acc: 0.3604\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8983 - acc: 0.3509 - val_loss: 1.8595 - val_acc: 0.3696\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8784 - acc: 0.3570 - val_loss: 1.8680 - val_acc: 0.3671\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8663 - acc: 0.3607 - val_loss: 1.8456 - val_acc: 0.3760\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8566 - acc: 0.3645 - val_loss: 1.8351 - val_acc: 0.3772\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8486 - acc: 0.3663 - val_loss: 1.8488 - val_acc: 0.3689\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8422 - acc: 0.3677 - val_loss: 1.8235 - val_acc: 0.3847\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8365 - acc: 0.3706 - val_loss: 1.8240 - val_acc: 0.3792\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8317 - acc: 0.3711 - val_loss: 1.8109 - val_acc: 0.3916\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8280 - acc: 0.3720 - val_loss: 1.8183 - val_acc: 0.3812\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8245 - acc: 0.3736 - val_loss: 1.8299 - val_acc: 0.3740\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8212 - acc: 0.3747 - val_loss: 1.8012 - val_acc: 0.3954\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8191 - acc: 0.3756 - val_loss: 1.8210 - val_acc: 0.3840\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8166 - acc: 0.3767 - val_loss: 1.8357 - val_acc: 0.3736\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8139 - acc: 0.3768 - val_loss: 1.8012 - val_acc: 0.3913\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8121 - acc: 0.3773 - val_loss: 1.8041 - val_acc: 0.3907\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8097 - acc: 0.3781 - val_loss: 1.8116 - val_acc: 0.3870\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8081 - acc: 0.3785 - val_loss: 1.8045 - val_acc: 0.3876\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8064 - acc: 0.3788 - val_loss: 1.8171 - val_acc: 0.3831\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8046 - acc: 0.3800 - val_loss: 1.7959 - val_acc: 0.3950\n",
            "32890/32890 [==============================] - 1s 16us/sample - loss: 1.9031 - acc: 0.3400\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 2.0692 - acc: 0.2826 - val_loss: 1.9009 - val_acc: 0.3555\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.9246 - acc: 0.3379 - val_loss: 1.8938 - val_acc: 0.3583\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.9051 - acc: 0.3440 - val_loss: 1.8729 - val_acc: 0.3662\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8935 - acc: 0.3480 - val_loss: 1.8536 - val_acc: 0.3745\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8841 - acc: 0.3505 - val_loss: 1.8598 - val_acc: 0.3701\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8770 - acc: 0.3513 - val_loss: 1.8450 - val_acc: 0.3777\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8705 - acc: 0.3544 - val_loss: 1.8440 - val_acc: 0.3746\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8649 - acc: 0.3546 - val_loss: 1.8554 - val_acc: 0.3688\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8607 - acc: 0.3567 - val_loss: 1.8226 - val_acc: 0.3817\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 24us/sample - loss: 1.8566 - acc: 0.3574 - val_loss: 1.8437 - val_acc: 0.3716\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8529 - acc: 0.3592 - val_loss: 1.8340 - val_acc: 0.3741\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8499 - acc: 0.3594 - val_loss: 1.8307 - val_acc: 0.3789\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8475 - acc: 0.3594 - val_loss: 1.8411 - val_acc: 0.3722\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8453 - acc: 0.3608 - val_loss: 1.8345 - val_acc: 0.3739\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8429 - acc: 0.3615 - val_loss: 1.8459 - val_acc: 0.3725\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8403 - acc: 0.3623 - val_loss: 1.8363 - val_acc: 0.3736\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 24us/sample - loss: 1.8394 - acc: 0.3640 - val_loss: 1.8523 - val_acc: 0.3689\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8369 - acc: 0.3635 - val_loss: 1.8347 - val_acc: 0.3770\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 24us/sample - loss: 1.8354 - acc: 0.3645 - val_loss: 1.8276 - val_acc: 0.3824\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8340 - acc: 0.3656 - val_loss: 1.8245 - val_acc: 0.3769\n",
            "32890/32890 [==============================] - 1s 17us/sample - loss: 1.8125 - acc: 0.3771\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 2.0803 - acc: 0.2914 - val_loss: 1.9166 - val_acc: 0.3557\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.9144 - acc: 0.3438 - val_loss: 1.9051 - val_acc: 0.3440\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8930 - acc: 0.3492 - val_loss: 1.8789 - val_acc: 0.3669\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8806 - acc: 0.3540 - val_loss: 1.8739 - val_acc: 0.3619\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8721 - acc: 0.3553 - val_loss: 1.8800 - val_acc: 0.3588\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8661 - acc: 0.3576 - val_loss: 1.8719 - val_acc: 0.3704\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8599 - acc: 0.3593 - val_loss: 1.8727 - val_acc: 0.3677\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8554 - acc: 0.3611 - val_loss: 1.8483 - val_acc: 0.3755\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8502 - acc: 0.3619 - val_loss: 1.8507 - val_acc: 0.3724\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8464 - acc: 0.3639 - val_loss: 1.8481 - val_acc: 0.3783\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8432 - acc: 0.3650 - val_loss: 1.8727 - val_acc: 0.3632\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8398 - acc: 0.3657 - val_loss: 1.8471 - val_acc: 0.3727\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8371 - acc: 0.3665 - val_loss: 1.8480 - val_acc: 0.3731\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8346 - acc: 0.3674 - val_loss: 1.8383 - val_acc: 0.3764\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8325 - acc: 0.3683 - val_loss: 1.8160 - val_acc: 0.3913\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8306 - acc: 0.3690 - val_loss: 1.8423 - val_acc: 0.3716\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8283 - acc: 0.3693 - val_loss: 1.8391 - val_acc: 0.3738\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8265 - acc: 0.3700 - val_loss: 1.8424 - val_acc: 0.3758\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 24us/sample - loss: 1.8256 - acc: 0.3709 - val_loss: 1.8321 - val_acc: 0.3770\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8237 - acc: 0.3709 - val_loss: 1.8479 - val_acc: 0.3701\n",
            "32890/32890 [==============================] - 1s 19us/sample - loss: 1.8560 - acc: 0.3549\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 5s 44us/sample - loss: 2.0708 - acc: 0.2919 - val_loss: 1.9272 - val_acc: 0.3231\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 2s 23us/sample - loss: 1.9196 - acc: 0.3412 - val_loss: 1.8891 - val_acc: 0.3457\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 2s 23us/sample - loss: 1.9015 - acc: 0.3468 - val_loss: 1.8839 - val_acc: 0.3436\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 2s 23us/sample - loss: 1.8903 - acc: 0.3492 - val_loss: 1.8585 - val_acc: 0.3574\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 2s 23us/sample - loss: 1.8813 - acc: 0.3528 - val_loss: 1.8585 - val_acc: 0.3541\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 2s 23us/sample - loss: 1.8747 - acc: 0.3551 - val_loss: 1.8695 - val_acc: 0.3491\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 2s 23us/sample - loss: 1.8685 - acc: 0.3567 - val_loss: 1.8640 - val_acc: 0.3483\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 2s 23us/sample - loss: 1.8640 - acc: 0.3579 - val_loss: 1.8489 - val_acc: 0.3519\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 3s 24us/sample - loss: 1.8598 - acc: 0.3584 - val_loss: 1.8527 - val_acc: 0.3523\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 2s 23us/sample - loss: 1.8563 - acc: 0.3595 - val_loss: 1.8716 - val_acc: 0.3380\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 2s 23us/sample - loss: 1.8524 - acc: 0.3611 - val_loss: 1.8392 - val_acc: 0.3552\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 2s 23us/sample - loss: 1.8496 - acc: 0.3622 - val_loss: 1.8560 - val_acc: 0.3472\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 2s 23us/sample - loss: 1.8471 - acc: 0.3621 - val_loss: 1.8461 - val_acc: 0.3514\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 3s 24us/sample - loss: 1.8449 - acc: 0.3633 - val_loss: 1.8473 - val_acc: 0.3516\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 2s 23us/sample - loss: 1.8422 - acc: 0.3631 - val_loss: 1.8515 - val_acc: 0.3439\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 2s 23us/sample - loss: 1.8404 - acc: 0.3643 - val_loss: 1.8349 - val_acc: 0.3571\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 2s 23us/sample - loss: 1.8378 - acc: 0.3651 - val_loss: 1.8196 - val_acc: 0.3638\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 2s 23us/sample - loss: 1.8364 - acc: 0.3656 - val_loss: 1.8186 - val_acc: 0.3631\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 2s 23us/sample - loss: 1.8349 - acc: 0.3665 - val_loss: 1.8120 - val_acc: 0.3672\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 2s 23us/sample - loss: 1.8330 - acc: 0.3664 - val_loss: 1.8373 - val_acc: 0.3463\n",
            "32889/32889 [==============================] - 1s 16us/sample - loss: 1.8591 - acc: 0.3746\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 5s 45us/sample - loss: 2.0223 - acc: 0.3185 - val_loss: 1.8814 - val_acc: 0.3781\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.9037 - acc: 0.3502 - val_loss: 1.8514 - val_acc: 0.3851\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8848 - acc: 0.3545 - val_loss: 1.8398 - val_acc: 0.3871\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8728 - acc: 0.3579 - val_loss: 1.8368 - val_acc: 0.3834\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8642 - acc: 0.3599 - val_loss: 1.8244 - val_acc: 0.3904\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8572 - acc: 0.3618 - val_loss: 1.8205 - val_acc: 0.3916\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8516 - acc: 0.3629 - val_loss: 1.8205 - val_acc: 0.3896\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8462 - acc: 0.3646 - val_loss: 1.8156 - val_acc: 0.3875\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8417 - acc: 0.3671 - val_loss: 1.8089 - val_acc: 0.3908\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8374 - acc: 0.3687 - val_loss: 1.8061 - val_acc: 0.3934\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8334 - acc: 0.3691 - val_loss: 1.8028 - val_acc: 0.3952\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8296 - acc: 0.3710 - val_loss: 1.8046 - val_acc: 0.3923\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8265 - acc: 0.3722 - val_loss: 1.7913 - val_acc: 0.3997\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8234 - acc: 0.3728 - val_loss: 1.7929 - val_acc: 0.3991\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8203 - acc: 0.3741 - val_loss: 1.8025 - val_acc: 0.3937\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8182 - acc: 0.3750 - val_loss: 1.7887 - val_acc: 0.3977\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8156 - acc: 0.3761 - val_loss: 1.7966 - val_acc: 0.3957\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8133 - acc: 0.3768 - val_loss: 1.8007 - val_acc: 0.3912\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8115 - acc: 0.3782 - val_loss: 1.7991 - val_acc: 0.3944\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8097 - acc: 0.3774 - val_loss: 1.7923 - val_acc: 0.3962\n",
            "32890/32890 [==============================] - 1s 16us/sample - loss: 1.8555 - acc: 0.3601\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 5s 45us/sample - loss: 2.0060 - acc: 0.3250 - val_loss: 1.8833 - val_acc: 0.3707\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8890 - acc: 0.3565 - val_loss: 1.8696 - val_acc: 0.3758\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8699 - acc: 0.3629 - val_loss: 1.8480 - val_acc: 0.3820\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8577 - acc: 0.3662 - val_loss: 1.8443 - val_acc: 0.3776\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8482 - acc: 0.3687 - val_loss: 1.8392 - val_acc: 0.3797\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8401 - acc: 0.3715 - val_loss: 1.8384 - val_acc: 0.3820\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8339 - acc: 0.3723 - val_loss: 1.8341 - val_acc: 0.3807\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8279 - acc: 0.3754 - val_loss: 1.8223 - val_acc: 0.3869\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8229 - acc: 0.3755 - val_loss: 1.8231 - val_acc: 0.3845\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8188 - acc: 0.3777 - val_loss: 1.8199 - val_acc: 0.3851\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8145 - acc: 0.3787 - val_loss: 1.8105 - val_acc: 0.3923\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8107 - acc: 0.3795 - val_loss: 1.8089 - val_acc: 0.3904\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8072 - acc: 0.3808 - val_loss: 1.8064 - val_acc: 0.3907\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8042 - acc: 0.3814 - val_loss: 1.8063 - val_acc: 0.3867\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8016 - acc: 0.3828 - val_loss: 1.8128 - val_acc: 0.3874\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.7994 - acc: 0.3834 - val_loss: 1.8025 - val_acc: 0.3938\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.7969 - acc: 0.3841 - val_loss: 1.8016 - val_acc: 0.3913\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.7952 - acc: 0.3848 - val_loss: 1.8145 - val_acc: 0.3868\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.7930 - acc: 0.3855 - val_loss: 1.8005 - val_acc: 0.3927\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.7916 - acc: 0.3861 - val_loss: 1.8065 - val_acc: 0.3888\n",
            "32890/32890 [==============================] - 1s 16us/sample - loss: 1.9011 - acc: 0.3387\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 5s 45us/sample - loss: 2.0413 - acc: 0.3078 - val_loss: 1.9115 - val_acc: 0.3541\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.9159 - acc: 0.3438 - val_loss: 1.8878 - val_acc: 0.3648\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8980 - acc: 0.3485 - val_loss: 1.8776 - val_acc: 0.3697\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8859 - acc: 0.3523 - val_loss: 1.8722 - val_acc: 0.3701\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8774 - acc: 0.3549 - val_loss: 1.8732 - val_acc: 0.3676\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8698 - acc: 0.3560 - val_loss: 1.8540 - val_acc: 0.3745\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8631 - acc: 0.3581 - val_loss: 1.8573 - val_acc: 0.3723\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8574 - acc: 0.3604 - val_loss: 1.8364 - val_acc: 0.3809\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8520 - acc: 0.3612 - val_loss: 1.8367 - val_acc: 0.3803\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8478 - acc: 0.3618 - val_loss: 1.8388 - val_acc: 0.3786\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8436 - acc: 0.3636 - val_loss: 1.8447 - val_acc: 0.3736\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8399 - acc: 0.3654 - val_loss: 1.8440 - val_acc: 0.3752\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8367 - acc: 0.3666 - val_loss: 1.8338 - val_acc: 0.3799\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8336 - acc: 0.3668 - val_loss: 1.8440 - val_acc: 0.3734\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8306 - acc: 0.3677 - val_loss: 1.8313 - val_acc: 0.3770\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8282 - acc: 0.3691 - val_loss: 1.8247 - val_acc: 0.3814\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 22us/sample - loss: 1.8259 - acc: 0.3690 - val_loss: 1.8193 - val_acc: 0.3826\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8238 - acc: 0.3703 - val_loss: 1.8197 - val_acc: 0.3839\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8218 - acc: 0.3712 - val_loss: 1.8200 - val_acc: 0.3816\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 24us/sample - loss: 1.8199 - acc: 0.3718 - val_loss: 1.8227 - val_acc: 0.3804\n",
            "32890/32890 [==============================] - 1s 16us/sample - loss: 1.8045 - acc: 0.3852\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 5s 45us/sample - loss: 2.0299 - acc: 0.3132 - val_loss: 1.9221 - val_acc: 0.3551\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 24us/sample - loss: 1.9067 - acc: 0.3479 - val_loss: 1.8893 - val_acc: 0.3733\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8883 - acc: 0.3525 - val_loss: 1.8791 - val_acc: 0.3703\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8758 - acc: 0.3573 - val_loss: 1.8736 - val_acc: 0.3730\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8665 - acc: 0.3601 - val_loss: 1.8629 - val_acc: 0.3789\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8587 - acc: 0.3613 - val_loss: 1.8622 - val_acc: 0.3778\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8525 - acc: 0.3633 - val_loss: 1.8599 - val_acc: 0.3772\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8471 - acc: 0.3660 - val_loss: 1.8540 - val_acc: 0.3774\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8425 - acc: 0.3671 - val_loss: 1.8470 - val_acc: 0.3782\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 24us/sample - loss: 1.8384 - acc: 0.3688 - val_loss: 1.8463 - val_acc: 0.3827\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8349 - acc: 0.3693 - val_loss: 1.8428 - val_acc: 0.3792\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8316 - acc: 0.3702 - val_loss: 1.8371 - val_acc: 0.3794\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8288 - acc: 0.3710 - val_loss: 1.8407 - val_acc: 0.3828\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8259 - acc: 0.3717 - val_loss: 1.8433 - val_acc: 0.3790\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8234 - acc: 0.3726 - val_loss: 1.8400 - val_acc: 0.3809\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8208 - acc: 0.3736 - val_loss: 1.8410 - val_acc: 0.3816\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 24us/sample - loss: 1.8184 - acc: 0.3755 - val_loss: 1.8396 - val_acc: 0.3757\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8165 - acc: 0.3753 - val_loss: 1.8298 - val_acc: 0.3853\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 24us/sample - loss: 1.8144 - acc: 0.3753 - val_loss: 1.8301 - val_acc: 0.3826\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 24us/sample - loss: 1.8129 - acc: 0.3757 - val_loss: 1.8292 - val_acc: 0.3850\n",
            "32890/32890 [==============================] - 1s 17us/sample - loss: 1.8398 - acc: 0.3656\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 5s 46us/sample - loss: 2.0283 - acc: 0.3160 - val_loss: 1.9161 - val_acc: 0.3449\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 3s 24us/sample - loss: 1.9093 - acc: 0.3463 - val_loss: 1.8909 - val_acc: 0.3508\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 3s 24us/sample - loss: 1.8931 - acc: 0.3510 - val_loss: 1.8849 - val_acc: 0.3459\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 2s 23us/sample - loss: 1.8825 - acc: 0.3544 - val_loss: 1.8653 - val_acc: 0.3566\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 3s 25us/sample - loss: 1.8735 - acc: 0.3565 - val_loss: 1.8587 - val_acc: 0.3603\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 3s 25us/sample - loss: 1.8661 - acc: 0.3588 - val_loss: 1.8532 - val_acc: 0.3596\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 2s 24us/sample - loss: 1.8599 - acc: 0.3599 - val_loss: 1.8453 - val_acc: 0.3626\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 2s 24us/sample - loss: 1.8546 - acc: 0.3613 - val_loss: 1.8404 - val_acc: 0.3634\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 2s 23us/sample - loss: 1.8496 - acc: 0.3634 - val_loss: 1.8360 - val_acc: 0.3640\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 3s 24us/sample - loss: 1.8456 - acc: 0.3638 - val_loss: 1.8386 - val_acc: 0.3627\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 2s 23us/sample - loss: 1.8418 - acc: 0.3653 - val_loss: 1.8339 - val_acc: 0.3629\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 2s 23us/sample - loss: 1.8390 - acc: 0.3659 - val_loss: 1.8417 - val_acc: 0.3562\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 3s 24us/sample - loss: 1.8362 - acc: 0.3672 - val_loss: 1.8282 - val_acc: 0.3632\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 2s 24us/sample - loss: 1.8336 - acc: 0.3675 - val_loss: 1.8295 - val_acc: 0.3633\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 2s 24us/sample - loss: 1.8312 - acc: 0.3693 - val_loss: 1.8385 - val_acc: 0.3554\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 2s 23us/sample - loss: 1.8290 - acc: 0.3694 - val_loss: 1.8313 - val_acc: 0.3590\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 3s 25us/sample - loss: 1.8270 - acc: 0.3697 - val_loss: 1.8186 - val_acc: 0.3657\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 2s 24us/sample - loss: 1.8253 - acc: 0.3716 - val_loss: 1.8121 - val_acc: 0.3702\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 2s 23us/sample - loss: 1.8234 - acc: 0.3716 - val_loss: 1.8221 - val_acc: 0.3626\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 3s 24us/sample - loss: 1.8218 - acc: 0.3726 - val_loss: 1.8197 - val_acc: 0.3664\n",
            "32889/32889 [==============================] - 1s 16us/sample - loss: 1.8402 - acc: 0.3843\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 5s 47us/sample - loss: 2.0082 - acc: 0.3167 - val_loss: 1.8537 - val_acc: 0.3810\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8790 - acc: 0.3557 - val_loss: 1.8374 - val_acc: 0.3790\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 24us/sample - loss: 1.8579 - acc: 0.3600 - val_loss: 1.8318 - val_acc: 0.3789\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 24us/sample - loss: 1.8444 - acc: 0.3637 - val_loss: 1.8255 - val_acc: 0.3798\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8355 - acc: 0.3671 - val_loss: 1.8166 - val_acc: 0.3850\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8290 - acc: 0.3683 - val_loss: 1.8156 - val_acc: 0.3858\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8242 - acc: 0.3705 - val_loss: 1.8011 - val_acc: 0.3912\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8202 - acc: 0.3729 - val_loss: 1.8081 - val_acc: 0.3853\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 24us/sample - loss: 1.8159 - acc: 0.3745 - val_loss: 1.7926 - val_acc: 0.3958\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8126 - acc: 0.3742 - val_loss: 1.7935 - val_acc: 0.3930\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 24us/sample - loss: 1.8102 - acc: 0.3744 - val_loss: 1.7973 - val_acc: 0.3934\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8075 - acc: 0.3765 - val_loss: 1.7990 - val_acc: 0.3889\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8054 - acc: 0.3779 - val_loss: 1.7916 - val_acc: 0.3938\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8036 - acc: 0.3785 - val_loss: 1.7860 - val_acc: 0.3943\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8018 - acc: 0.3784 - val_loss: 1.8017 - val_acc: 0.3869\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8001 - acc: 0.3789 - val_loss: 1.7921 - val_acc: 0.3915\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.7989 - acc: 0.3791 - val_loss: 1.7886 - val_acc: 0.3958\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.7973 - acc: 0.3806 - val_loss: 1.7873 - val_acc: 0.3951\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 24us/sample - loss: 1.7959 - acc: 0.3811 - val_loss: 1.7933 - val_acc: 0.3907\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 24us/sample - loss: 1.7946 - acc: 0.3811 - val_loss: 1.7877 - val_acc: 0.3917\n",
            "32890/32890 [==============================] - 1s 17us/sample - loss: 1.8430 - acc: 0.3696\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 5s 47us/sample - loss: 2.0056 - acc: 0.3179 - val_loss: 1.8568 - val_acc: 0.3752\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8668 - acc: 0.3624 - val_loss: 1.8418 - val_acc: 0.3760\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8470 - acc: 0.3663 - val_loss: 1.8428 - val_acc: 0.3735\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 24us/sample - loss: 1.8353 - acc: 0.3700 - val_loss: 1.8152 - val_acc: 0.3883\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8264 - acc: 0.3732 - val_loss: 1.8223 - val_acc: 0.3863\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8190 - acc: 0.3755 - val_loss: 1.8191 - val_acc: 0.3877\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8133 - acc: 0.3776 - val_loss: 1.8153 - val_acc: 0.3845\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8075 - acc: 0.3804 - val_loss: 1.8075 - val_acc: 0.3885\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8034 - acc: 0.3814 - val_loss: 1.8095 - val_acc: 0.3889\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.7995 - acc: 0.3834 - val_loss: 1.8067 - val_acc: 0.3896\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.7966 - acc: 0.3840 - val_loss: 1.7913 - val_acc: 0.3931\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.7934 - acc: 0.3844 - val_loss: 1.8020 - val_acc: 0.3884\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.7912 - acc: 0.3860 - val_loss: 1.8077 - val_acc: 0.3877\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 24us/sample - loss: 1.7882 - acc: 0.3868 - val_loss: 1.8111 - val_acc: 0.3828\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.7863 - acc: 0.3878 - val_loss: 1.8118 - val_acc: 0.3845\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.7842 - acc: 0.3875 - val_loss: 1.7920 - val_acc: 0.3926\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.7823 - acc: 0.3887 - val_loss: 1.7862 - val_acc: 0.3951\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.7803 - acc: 0.3883 - val_loss: 1.7901 - val_acc: 0.3939\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.7785 - acc: 0.3896 - val_loss: 1.7971 - val_acc: 0.3909\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 24us/sample - loss: 1.7770 - acc: 0.3895 - val_loss: 1.7913 - val_acc: 0.3919\n",
            "32890/32890 [==============================] - 1s 16us/sample - loss: 1.8835 - acc: 0.3398\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 5s 45us/sample - loss: 2.0114 - acc: 0.3067 - val_loss: 1.8940 - val_acc: 0.3565\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8856 - acc: 0.3492 - val_loss: 1.8641 - val_acc: 0.3673\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8662 - acc: 0.3549 - val_loss: 1.8474 - val_acc: 0.3744\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8544 - acc: 0.3589 - val_loss: 1.8359 - val_acc: 0.3771\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 24us/sample - loss: 1.8465 - acc: 0.3605 - val_loss: 1.8319 - val_acc: 0.3782\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8405 - acc: 0.3634 - val_loss: 1.8306 - val_acc: 0.3780\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8351 - acc: 0.3656 - val_loss: 1.8109 - val_acc: 0.3887\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8307 - acc: 0.3659 - val_loss: 1.8383 - val_acc: 0.3731\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8274 - acc: 0.3671 - val_loss: 1.8225 - val_acc: 0.3777\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8238 - acc: 0.3689 - val_loss: 1.8236 - val_acc: 0.3781\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8212 - acc: 0.3704 - val_loss: 1.8221 - val_acc: 0.3787\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8187 - acc: 0.3710 - val_loss: 1.8231 - val_acc: 0.3754\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 24us/sample - loss: 1.8166 - acc: 0.3714 - val_loss: 1.8185 - val_acc: 0.3822\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8145 - acc: 0.3715 - val_loss: 1.8164 - val_acc: 0.3809\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 24us/sample - loss: 1.8131 - acc: 0.3732 - val_loss: 1.8158 - val_acc: 0.3808\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 24us/sample - loss: 1.8116 - acc: 0.3732 - val_loss: 1.8189 - val_acc: 0.3816\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8101 - acc: 0.3740 - val_loss: 1.8253 - val_acc: 0.3763\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8089 - acc: 0.3740 - val_loss: 1.8155 - val_acc: 0.3825\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8079 - acc: 0.3751 - val_loss: 1.8179 - val_acc: 0.3796\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8062 - acc: 0.3755 - val_loss: 1.8146 - val_acc: 0.3807\n",
            "32890/32890 [==============================] - 1s 20us/sample - loss: 1.7998 - acc: 0.3822\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 2.0034 - acc: 0.3127 - val_loss: 1.8980 - val_acc: 0.3610\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8822 - acc: 0.3530 - val_loss: 1.8608 - val_acc: 0.3747\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8611 - acc: 0.3595 - val_loss: 1.8718 - val_acc: 0.3673\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8489 - acc: 0.3639 - val_loss: 1.8661 - val_acc: 0.3711\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8399 - acc: 0.3656 - val_loss: 1.8477 - val_acc: 0.3790\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8325 - acc: 0.3686 - val_loss: 1.8306 - val_acc: 0.3846\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8271 - acc: 0.3695 - val_loss: 1.8409 - val_acc: 0.3756\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8223 - acc: 0.3719 - val_loss: 1.8341 - val_acc: 0.3786\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8183 - acc: 0.3729 - val_loss: 1.8244 - val_acc: 0.3845\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8149 - acc: 0.3748 - val_loss: 1.8244 - val_acc: 0.3824\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8123 - acc: 0.3755 - val_loss: 1.8195 - val_acc: 0.3883\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 24us/sample - loss: 1.8099 - acc: 0.3764 - val_loss: 1.8198 - val_acc: 0.3857\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8073 - acc: 0.3768 - val_loss: 1.8331 - val_acc: 0.3799\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8053 - acc: 0.3774 - val_loss: 1.8255 - val_acc: 0.3801\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8031 - acc: 0.3779 - val_loss: 1.8260 - val_acc: 0.3851\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8014 - acc: 0.3785 - val_loss: 1.8311 - val_acc: 0.3804\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.7995 - acc: 0.3797 - val_loss: 1.8125 - val_acc: 0.3877\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.7984 - acc: 0.3796 - val_loss: 1.8457 - val_acc: 0.3726\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.7970 - acc: 0.3803 - val_loss: 1.8177 - val_acc: 0.3843\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.7955 - acc: 0.3804 - val_loss: 1.8408 - val_acc: 0.3783\n",
            "32890/32890 [==============================] - 1s 17us/sample - loss: 1.8403 - acc: 0.3656\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 5s 45us/sample - loss: 2.0331 - acc: 0.3026 - val_loss: 1.8992 - val_acc: 0.3432\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 2s 24us/sample - loss: 1.8898 - acc: 0.3490 - val_loss: 1.8619 - val_acc: 0.3551\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 3s 24us/sample - loss: 1.8677 - acc: 0.3561 - val_loss: 1.8436 - val_acc: 0.3587\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 3s 25us/sample - loss: 1.8549 - acc: 0.3610 - val_loss: 1.8417 - val_acc: 0.3573\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 2s 23us/sample - loss: 1.8462 - acc: 0.3630 - val_loss: 1.8367 - val_acc: 0.3571\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 2s 24us/sample - loss: 1.8402 - acc: 0.3645 - val_loss: 1.8408 - val_acc: 0.3562\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 3s 24us/sample - loss: 1.8348 - acc: 0.3666 - val_loss: 1.8389 - val_acc: 0.3580\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 3s 24us/sample - loss: 1.8304 - acc: 0.3674 - val_loss: 1.8346 - val_acc: 0.3571\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 3s 24us/sample - loss: 1.8267 - acc: 0.3687 - val_loss: 1.8270 - val_acc: 0.3622\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 2s 23us/sample - loss: 1.8237 - acc: 0.3698 - val_loss: 1.8169 - val_acc: 0.3632\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 3s 25us/sample - loss: 1.8206 - acc: 0.3695 - val_loss: 1.8184 - val_acc: 0.3662\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 2s 23us/sample - loss: 1.8180 - acc: 0.3711 - val_loss: 1.8126 - val_acc: 0.3679\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 3s 25us/sample - loss: 1.8159 - acc: 0.3735 - val_loss: 1.8031 - val_acc: 0.3700\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 2s 23us/sample - loss: 1.8139 - acc: 0.3727 - val_loss: 1.8085 - val_acc: 0.3687\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 2s 23us/sample - loss: 1.8119 - acc: 0.3740 - val_loss: 1.8099 - val_acc: 0.3645\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 2s 23us/sample - loss: 1.8107 - acc: 0.3743 - val_loss: 1.8063 - val_acc: 0.3673\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 2s 23us/sample - loss: 1.8088 - acc: 0.3754 - val_loss: 1.8010 - val_acc: 0.3726\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 2s 24us/sample - loss: 1.8076 - acc: 0.3750 - val_loss: 1.8244 - val_acc: 0.3636\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 2s 23us/sample - loss: 1.8059 - acc: 0.3760 - val_loss: 1.8242 - val_acc: 0.3581\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 2s 23us/sample - loss: 1.8045 - acc: 0.3762 - val_loss: 1.8113 - val_acc: 0.3671\n",
            "32889/32889 [==============================] - 1s 16us/sample - loss: 1.8418 - acc: 0.3825\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 5s 46us/sample - loss: 2.0032 - acc: 0.3216 - val_loss: 1.8761 - val_acc: 0.3702\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.9029 - acc: 0.3489 - val_loss: 1.8609 - val_acc: 0.3743\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 24us/sample - loss: 1.8860 - acc: 0.3534 - val_loss: 1.8397 - val_acc: 0.3821\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8722 - acc: 0.3569 - val_loss: 1.8345 - val_acc: 0.3794\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8618 - acc: 0.3604 - val_loss: 1.8242 - val_acc: 0.3836\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8539 - acc: 0.3625 - val_loss: 1.8176 - val_acc: 0.3872\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8471 - acc: 0.3646 - val_loss: 1.8062 - val_acc: 0.3924\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 24us/sample - loss: 1.8415 - acc: 0.3655 - val_loss: 1.8138 - val_acc: 0.3848\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8368 - acc: 0.3676 - val_loss: 1.8066 - val_acc: 0.3912\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8330 - acc: 0.3686 - val_loss: 1.7977 - val_acc: 0.3945\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 24us/sample - loss: 1.8293 - acc: 0.3699 - val_loss: 1.7942 - val_acc: 0.3947\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8265 - acc: 0.3709 - val_loss: 1.8037 - val_acc: 0.3902\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8238 - acc: 0.3713 - val_loss: 1.7941 - val_acc: 0.3949\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8218 - acc: 0.3721 - val_loss: 1.7903 - val_acc: 0.3958\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8196 - acc: 0.3732 - val_loss: 1.7912 - val_acc: 0.3978\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8176 - acc: 0.3738 - val_loss: 1.8074 - val_acc: 0.3880\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8160 - acc: 0.3735 - val_loss: 1.7944 - val_acc: 0.3934\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8143 - acc: 0.3728 - val_loss: 1.7986 - val_acc: 0.3901\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8128 - acc: 0.3748 - val_loss: 1.7922 - val_acc: 0.3950\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8108 - acc: 0.3750 - val_loss: 1.7865 - val_acc: 0.3977\n",
            "32890/32890 [==============================] - 1s 17us/sample - loss: 1.8540 - acc: 0.3607\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 5s 45us/sample - loss: 1.9968 - acc: 0.3221 - val_loss: 1.8845 - val_acc: 0.3701\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8828 - acc: 0.3581 - val_loss: 1.8670 - val_acc: 0.3767\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8669 - acc: 0.3629 - val_loss: 1.8571 - val_acc: 0.3776\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8564 - acc: 0.3656 - val_loss: 1.8464 - val_acc: 0.3802\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8479 - acc: 0.3694 - val_loss: 1.8404 - val_acc: 0.3830\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8406 - acc: 0.3717 - val_loss: 1.8330 - val_acc: 0.3854\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8342 - acc: 0.3737 - val_loss: 1.8194 - val_acc: 0.3907\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 24us/sample - loss: 1.8286 - acc: 0.3753 - val_loss: 1.8259 - val_acc: 0.3844\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8242 - acc: 0.3766 - val_loss: 1.8287 - val_acc: 0.3850\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8197 - acc: 0.3772 - val_loss: 1.8134 - val_acc: 0.3909\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8165 - acc: 0.3787 - val_loss: 1.8187 - val_acc: 0.3891\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8133 - acc: 0.3797 - val_loss: 1.8121 - val_acc: 0.3888\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 24us/sample - loss: 1.8108 - acc: 0.3803 - val_loss: 1.8075 - val_acc: 0.3930\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8082 - acc: 0.3805 - val_loss: 1.8123 - val_acc: 0.3877\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 24us/sample - loss: 1.8065 - acc: 0.3808 - val_loss: 1.8106 - val_acc: 0.3893\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8044 - acc: 0.3813 - val_loss: 1.8082 - val_acc: 0.3934\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 24us/sample - loss: 1.8022 - acc: 0.3819 - val_loss: 1.8077 - val_acc: 0.3921\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8007 - acc: 0.3827 - val_loss: 1.8184 - val_acc: 0.3852\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.7989 - acc: 0.3835 - val_loss: 1.8022 - val_acc: 0.3924\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.7971 - acc: 0.3838 - val_loss: 1.8060 - val_acc: 0.3932\n",
            "32890/32890 [==============================] - 1s 17us/sample - loss: 1.8979 - acc: 0.3369\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 5s 45us/sample - loss: 2.0026 - acc: 0.3166 - val_loss: 1.9079 - val_acc: 0.3609\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 24us/sample - loss: 1.9084 - acc: 0.3456 - val_loss: 1.8940 - val_acc: 0.3666\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8930 - acc: 0.3511 - val_loss: 1.8725 - val_acc: 0.3712\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 23us/sample - loss: 1.8819 - acc: 0.3538 - val_loss: 1.8606 - val_acc: 0.3739\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8727 - acc: 0.3565 - val_loss: 1.8575 - val_acc: 0.3757\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8648 - acc: 0.3577 - val_loss: 1.8419 - val_acc: 0.3779\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 24us/sample - loss: 1.8586 - acc: 0.3578 - val_loss: 1.8482 - val_acc: 0.3730\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8527 - acc: 0.3596 - val_loss: 1.8479 - val_acc: 0.3727\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8477 - acc: 0.3612 - val_loss: 1.8255 - val_acc: 0.3838\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8434 - acc: 0.3628 - val_loss: 1.8268 - val_acc: 0.3824\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8393 - acc: 0.3646 - val_loss: 1.8272 - val_acc: 0.3846\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 24us/sample - loss: 1.8361 - acc: 0.3647 - val_loss: 1.8229 - val_acc: 0.3851\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 24us/sample - loss: 1.8326 - acc: 0.3667 - val_loss: 1.8298 - val_acc: 0.3791\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8294 - acc: 0.3673 - val_loss: 1.8226 - val_acc: 0.3796\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8269 - acc: 0.3680 - val_loss: 1.8229 - val_acc: 0.3817\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8245 - acc: 0.3698 - val_loss: 1.8272 - val_acc: 0.3815\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8221 - acc: 0.3703 - val_loss: 1.8178 - val_acc: 0.3834\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8200 - acc: 0.3704 - val_loss: 1.7920 - val_acc: 0.3945\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8181 - acc: 0.3717 - val_loss: 1.8243 - val_acc: 0.3818\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8160 - acc: 0.3724 - val_loss: 1.8213 - val_acc: 0.3784\n",
            "32890/32890 [==============================] - 1s 17us/sample - loss: 1.8094 - acc: 0.3792\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 5s 48us/sample - loss: 1.9830 - acc: 0.3271 - val_loss: 1.8941 - val_acc: 0.3727\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8992 - acc: 0.3500 - val_loss: 1.9015 - val_acc: 0.3612\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8828 - acc: 0.3538 - val_loss: 1.8804 - val_acc: 0.3640\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8708 - acc: 0.3586 - val_loss: 1.8691 - val_acc: 0.3770\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8614 - acc: 0.3615 - val_loss: 1.8606 - val_acc: 0.3778\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8540 - acc: 0.3623 - val_loss: 1.8601 - val_acc: 0.3744\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8475 - acc: 0.3644 - val_loss: 1.8450 - val_acc: 0.3801\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8422 - acc: 0.3667 - val_loss: 1.8465 - val_acc: 0.3786\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8376 - acc: 0.3683 - val_loss: 1.8589 - val_acc: 0.3724\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8332 - acc: 0.3692 - val_loss: 1.8507 - val_acc: 0.3736\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8300 - acc: 0.3706 - val_loss: 1.8496 - val_acc: 0.3771\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8264 - acc: 0.3713 - val_loss: 1.8447 - val_acc: 0.3764\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 24us/sample - loss: 1.8233 - acc: 0.3728 - val_loss: 1.8257 - val_acc: 0.3866\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8207 - acc: 0.3736 - val_loss: 1.8338 - val_acc: 0.3843\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8183 - acc: 0.3734 - val_loss: 1.8368 - val_acc: 0.3797\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8155 - acc: 0.3747 - val_loss: 1.8176 - val_acc: 0.3923\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8137 - acc: 0.3755 - val_loss: 1.8287 - val_acc: 0.3896\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8113 - acc: 0.3765 - val_loss: 1.8248 - val_acc: 0.3864\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8096 - acc: 0.3765 - val_loss: 1.8175 - val_acc: 0.3894\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8079 - acc: 0.3761 - val_loss: 1.8359 - val_acc: 0.3804\n",
            "32890/32890 [==============================] - 1s 18us/sample - loss: 1.8337 - acc: 0.3650\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 5s 49us/sample - loss: 1.9972 - acc: 0.3213 - val_loss: 1.9085 - val_acc: 0.3421\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 3s 25us/sample - loss: 1.9111 - acc: 0.3432 - val_loss: 1.8816 - val_acc: 0.3561\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 3s 24us/sample - loss: 1.8959 - acc: 0.3494 - val_loss: 1.8761 - val_acc: 0.3518\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 3s 25us/sample - loss: 1.8847 - acc: 0.3522 - val_loss: 1.8590 - val_acc: 0.3581\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 3s 25us/sample - loss: 1.8748 - acc: 0.3556 - val_loss: 1.8573 - val_acc: 0.3564\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 2s 24us/sample - loss: 1.8668 - acc: 0.3592 - val_loss: 1.8477 - val_acc: 0.3587\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 3s 25us/sample - loss: 1.8603 - acc: 0.3605 - val_loss: 1.8484 - val_acc: 0.3558\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 3s 25us/sample - loss: 1.8547 - acc: 0.3614 - val_loss: 1.8354 - val_acc: 0.3654\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.8498 - acc: 0.3629 - val_loss: 1.8406 - val_acc: 0.3617\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 3s 25us/sample - loss: 1.8458 - acc: 0.3642 - val_loss: 1.8400 - val_acc: 0.3588\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 3s 25us/sample - loss: 1.8416 - acc: 0.3657 - val_loss: 1.8257 - val_acc: 0.3645\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 3s 24us/sample - loss: 1.8383 - acc: 0.3665 - val_loss: 1.8332 - val_acc: 0.3614\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 3s 25us/sample - loss: 1.8349 - acc: 0.3668 - val_loss: 1.8273 - val_acc: 0.3634\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 3s 24us/sample - loss: 1.8321 - acc: 0.3684 - val_loss: 1.8165 - val_acc: 0.3669\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 3s 24us/sample - loss: 1.8292 - acc: 0.3686 - val_loss: 1.8312 - val_acc: 0.3601\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 3s 25us/sample - loss: 1.8267 - acc: 0.3697 - val_loss: 1.8122 - val_acc: 0.3695\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 3s 25us/sample - loss: 1.8244 - acc: 0.3703 - val_loss: 1.8106 - val_acc: 0.3687\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 3s 24us/sample - loss: 1.8220 - acc: 0.3716 - val_loss: 1.8100 - val_acc: 0.3697\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 3s 24us/sample - loss: 1.8196 - acc: 0.3729 - val_loss: 1.8365 - val_acc: 0.3572\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 3s 24us/sample - loss: 1.8176 - acc: 0.3727 - val_loss: 1.8238 - val_acc: 0.3593\n",
            "32889/32889 [==============================] - 1s 17us/sample - loss: 1.8348 - acc: 0.3892\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 5s 49us/sample - loss: 2.2150 - acc: 0.2529 - val_loss: 1.9786 - val_acc: 0.3459\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9623 - acc: 0.3298 - val_loss: 1.9069 - val_acc: 0.3630\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.9298 - acc: 0.3401 - val_loss: 1.8793 - val_acc: 0.3735\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9155 - acc: 0.3445 - val_loss: 1.8766 - val_acc: 0.3728\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.9046 - acc: 0.3481 - val_loss: 1.8590 - val_acc: 0.3807\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8960 - acc: 0.3506 - val_loss: 1.8552 - val_acc: 0.3812\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8892 - acc: 0.3525 - val_loss: 1.8461 - val_acc: 0.3831\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8833 - acc: 0.3541 - val_loss: 1.8447 - val_acc: 0.3817\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8780 - acc: 0.3550 - val_loss: 1.8437 - val_acc: 0.3794\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8734 - acc: 0.3569 - val_loss: 1.8402 - val_acc: 0.3844\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8689 - acc: 0.3572 - val_loss: 1.8442 - val_acc: 0.3811\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8649 - acc: 0.3591 - val_loss: 1.8247 - val_acc: 0.3886\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8613 - acc: 0.3594 - val_loss: 1.8235 - val_acc: 0.3888\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8580 - acc: 0.3600 - val_loss: 1.8203 - val_acc: 0.3889\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8550 - acc: 0.3619 - val_loss: 1.8237 - val_acc: 0.3850\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8522 - acc: 0.3627 - val_loss: 1.8234 - val_acc: 0.3865\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8497 - acc: 0.3634 - val_loss: 1.8252 - val_acc: 0.3867\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8472 - acc: 0.3641 - val_loss: 1.8185 - val_acc: 0.3886\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8451 - acc: 0.3646 - val_loss: 1.8169 - val_acc: 0.3883\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8429 - acc: 0.3650 - val_loss: 1.8110 - val_acc: 0.3909\n",
            "32890/32890 [==============================] - 1s 18us/sample - loss: 1.8817 - acc: 0.3498\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 5s 50us/sample - loss: 2.1782 - acc: 0.2589 - val_loss: 1.9731 - val_acc: 0.3290\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.9439 - acc: 0.3378 - val_loss: 1.9122 - val_acc: 0.3568\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9126 - acc: 0.3485 - val_loss: 1.8902 - val_acc: 0.3683\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8988 - acc: 0.3531 - val_loss: 1.8914 - val_acc: 0.3642\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8884 - acc: 0.3548 - val_loss: 1.8766 - val_acc: 0.3686\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8801 - acc: 0.3567 - val_loss: 1.8594 - val_acc: 0.3778\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8728 - acc: 0.3596 - val_loss: 1.8620 - val_acc: 0.3748\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8666 - acc: 0.3612 - val_loss: 1.8524 - val_acc: 0.3777\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8609 - acc: 0.3634 - val_loss: 1.8605 - val_acc: 0.3744\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8557 - acc: 0.3650 - val_loss: 1.8456 - val_acc: 0.3793\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8517 - acc: 0.3665 - val_loss: 1.8512 - val_acc: 0.3766\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8477 - acc: 0.3681 - val_loss: 1.8355 - val_acc: 0.3858\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8438 - acc: 0.3690 - val_loss: 1.8339 - val_acc: 0.3851\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8408 - acc: 0.3703 - val_loss: 1.8292 - val_acc: 0.3851\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8380 - acc: 0.3710 - val_loss: 1.8295 - val_acc: 0.3846\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8352 - acc: 0.3717 - val_loss: 1.8364 - val_acc: 0.3824\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8326 - acc: 0.3733 - val_loss: 1.8354 - val_acc: 0.3801\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8305 - acc: 0.3735 - val_loss: 1.8380 - val_acc: 0.3812\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8284 - acc: 0.3743 - val_loss: 1.8312 - val_acc: 0.3821\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8262 - acc: 0.3751 - val_loss: 1.8219 - val_acc: 0.3878\n",
            "32890/32890 [==============================] - 1s 18us/sample - loss: 1.9223 - acc: 0.3309\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 5s 49us/sample - loss: 2.2188 - acc: 0.2369 - val_loss: 2.0451 - val_acc: 0.2997\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9798 - acc: 0.3213 - val_loss: 1.9470 - val_acc: 0.3440\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.9397 - acc: 0.3343 - val_loss: 1.9263 - val_acc: 0.3546\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9246 - acc: 0.3402 - val_loss: 1.9104 - val_acc: 0.3610\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.9143 - acc: 0.3433 - val_loss: 1.9084 - val_acc: 0.3616\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.9057 - acc: 0.3452 - val_loss: 1.8947 - val_acc: 0.3669\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8988 - acc: 0.3467 - val_loss: 1.8815 - val_acc: 0.3717\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8928 - acc: 0.3484 - val_loss: 1.8737 - val_acc: 0.3742\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8877 - acc: 0.3501 - val_loss: 1.8649 - val_acc: 0.3775\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8836 - acc: 0.3515 - val_loss: 1.8730 - val_acc: 0.3758\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8791 - acc: 0.3526 - val_loss: 1.8563 - val_acc: 0.3790\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8757 - acc: 0.3536 - val_loss: 1.8684 - val_acc: 0.3726\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8724 - acc: 0.3533 - val_loss: 1.8607 - val_acc: 0.3759\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8693 - acc: 0.3553 - val_loss: 1.8546 - val_acc: 0.3779\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8665 - acc: 0.3560 - val_loss: 1.8484 - val_acc: 0.3805\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8637 - acc: 0.3566 - val_loss: 1.8461 - val_acc: 0.3807\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8613 - acc: 0.3573 - val_loss: 1.8491 - val_acc: 0.3777\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8591 - acc: 0.3583 - val_loss: 1.8479 - val_acc: 0.3761\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8570 - acc: 0.3587 - val_loss: 1.8397 - val_acc: 0.3817\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8548 - acc: 0.3598 - val_loss: 1.8429 - val_acc: 0.3791\n",
            "32890/32890 [==============================] - 1s 18us/sample - loss: 1.8347 - acc: 0.3739\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 5s 49us/sample - loss: 2.2088 - acc: 0.2374 - val_loss: 2.0573 - val_acc: 0.2999\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.9675 - acc: 0.3279 - val_loss: 1.9457 - val_acc: 0.3471\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.9294 - acc: 0.3396 - val_loss: 1.9259 - val_acc: 0.3543\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9141 - acc: 0.3448 - val_loss: 1.9092 - val_acc: 0.3636\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.9031 - acc: 0.3480 - val_loss: 1.9101 - val_acc: 0.3597\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8937 - acc: 0.3503 - val_loss: 1.8940 - val_acc: 0.3648\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8865 - acc: 0.3517 - val_loss: 1.8868 - val_acc: 0.3666\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8804 - acc: 0.3539 - val_loss: 1.8883 - val_acc: 0.3667\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8758 - acc: 0.3549 - val_loss: 1.8819 - val_acc: 0.3697\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8711 - acc: 0.3566 - val_loss: 1.8729 - val_acc: 0.3740\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8673 - acc: 0.3569 - val_loss: 1.8704 - val_acc: 0.3721\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8638 - acc: 0.3588 - val_loss: 1.8635 - val_acc: 0.3762\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8606 - acc: 0.3592 - val_loss: 1.8625 - val_acc: 0.3757\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8577 - acc: 0.3604 - val_loss: 1.8660 - val_acc: 0.3742\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8550 - acc: 0.3611 - val_loss: 1.8751 - val_acc: 0.3658\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8522 - acc: 0.3624 - val_loss: 1.8622 - val_acc: 0.3728\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8500 - acc: 0.3636 - val_loss: 1.8514 - val_acc: 0.3809\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8480 - acc: 0.3635 - val_loss: 1.8635 - val_acc: 0.3695\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8456 - acc: 0.3649 - val_loss: 1.8584 - val_acc: 0.3738\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 24us/sample - loss: 1.8438 - acc: 0.3653 - val_loss: 1.8498 - val_acc: 0.3772\n",
            "32890/32890 [==============================] - 1s 18us/sample - loss: 1.8736 - acc: 0.3533\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 5s 49us/sample - loss: 2.1935 - acc: 0.2333 - val_loss: 2.0086 - val_acc: 0.2902\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 3s 25us/sample - loss: 1.9651 - acc: 0.3255 - val_loss: 1.9332 - val_acc: 0.3320\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 3s 25us/sample - loss: 1.9321 - acc: 0.3376 - val_loss: 1.9171 - val_acc: 0.3368\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.9187 - acc: 0.3416 - val_loss: 1.9083 - val_acc: 0.3378\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 3s 24us/sample - loss: 1.9080 - acc: 0.3438 - val_loss: 1.8914 - val_acc: 0.3464\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 3s 25us/sample - loss: 1.8997 - acc: 0.3463 - val_loss: 1.8898 - val_acc: 0.3470\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 3s 24us/sample - loss: 1.8929 - acc: 0.3481 - val_loss: 1.8877 - val_acc: 0.3474\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 3s 24us/sample - loss: 1.8879 - acc: 0.3492 - val_loss: 1.8703 - val_acc: 0.3527\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 3s 25us/sample - loss: 1.8832 - acc: 0.3509 - val_loss: 1.8746 - val_acc: 0.3475\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 3s 25us/sample - loss: 1.8793 - acc: 0.3517 - val_loss: 1.8726 - val_acc: 0.3476\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8760 - acc: 0.3530 - val_loss: 1.8753 - val_acc: 0.3454\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8727 - acc: 0.3546 - val_loss: 1.8645 - val_acc: 0.3522\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8702 - acc: 0.3548 - val_loss: 1.8645 - val_acc: 0.3521\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8674 - acc: 0.3559 - val_loss: 1.8618 - val_acc: 0.3530\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 3s 25us/sample - loss: 1.8651 - acc: 0.3565 - val_loss: 1.8615 - val_acc: 0.3522\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8629 - acc: 0.3570 - val_loss: 1.8552 - val_acc: 0.3533\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8607 - acc: 0.3577 - val_loss: 1.8590 - val_acc: 0.3522\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 3s 25us/sample - loss: 1.8588 - acc: 0.3588 - val_loss: 1.8587 - val_acc: 0.3508\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 3s 25us/sample - loss: 1.8569 - acc: 0.3589 - val_loss: 1.8499 - val_acc: 0.3557\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 3s 24us/sample - loss: 1.8547 - acc: 0.3603 - val_loss: 1.8538 - val_acc: 0.3550\n",
            "32889/32889 [==============================] - 1s 18us/sample - loss: 1.8555 - acc: 0.3829\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 5s 52us/sample - loss: 2.2032 - acc: 0.2403 - val_loss: 1.9806 - val_acc: 0.3198\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9625 - acc: 0.3275 - val_loss: 1.8974 - val_acc: 0.3644\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9291 - acc: 0.3398 - val_loss: 1.8826 - val_acc: 0.3686\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.9153 - acc: 0.3430 - val_loss: 1.8615 - val_acc: 0.3793\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9050 - acc: 0.3467 - val_loss: 1.8628 - val_acc: 0.3777\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8965 - acc: 0.3489 - val_loss: 1.8545 - val_acc: 0.3790\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8896 - acc: 0.3520 - val_loss: 1.8431 - val_acc: 0.3870\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8843 - acc: 0.3529 - val_loss: 1.8483 - val_acc: 0.3812\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8792 - acc: 0.3539 - val_loss: 1.8408 - val_acc: 0.3834\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8753 - acc: 0.3552 - val_loss: 1.8371 - val_acc: 0.3844\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8716 - acc: 0.3559 - val_loss: 1.8327 - val_acc: 0.3863\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8684 - acc: 0.3569 - val_loss: 1.8331 - val_acc: 0.3851\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8655 - acc: 0.3580 - val_loss: 1.8336 - val_acc: 0.3835\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8627 - acc: 0.3585 - val_loss: 1.8312 - val_acc: 0.3869\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8602 - acc: 0.3595 - val_loss: 1.8314 - val_acc: 0.3853\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8579 - acc: 0.3601 - val_loss: 1.8299 - val_acc: 0.3875\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8556 - acc: 0.3615 - val_loss: 1.8309 - val_acc: 0.3844\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8537 - acc: 0.3613 - val_loss: 1.8266 - val_acc: 0.3879\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8515 - acc: 0.3616 - val_loss: 1.8345 - val_acc: 0.3829\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8499 - acc: 0.3624 - val_loss: 1.8246 - val_acc: 0.3875\n",
            "32890/32890 [==============================] - 1s 18us/sample - loss: 1.8872 - acc: 0.3475\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 5s 52us/sample - loss: 2.2285 - acc: 0.2298 - val_loss: 2.0333 - val_acc: 0.2851\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9552 - acc: 0.3305 - val_loss: 1.9118 - val_acc: 0.3530\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.9137 - acc: 0.3458 - val_loss: 1.9050 - val_acc: 0.3546\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8970 - acc: 0.3505 - val_loss: 1.8832 - val_acc: 0.3700\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8850 - acc: 0.3547 - val_loss: 1.8688 - val_acc: 0.3693\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8761 - acc: 0.3578 - val_loss: 1.8613 - val_acc: 0.3745\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8693 - acc: 0.3595 - val_loss: 1.8453 - val_acc: 0.3810\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8642 - acc: 0.3611 - val_loss: 1.8505 - val_acc: 0.3752\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8596 - acc: 0.3629 - val_loss: 1.8511 - val_acc: 0.3772\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8555 - acc: 0.3640 - val_loss: 1.8500 - val_acc: 0.3791\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8521 - acc: 0.3660 - val_loss: 1.8504 - val_acc: 0.3779\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8488 - acc: 0.3662 - val_loss: 1.8385 - val_acc: 0.3834\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8459 - acc: 0.3679 - val_loss: 1.8380 - val_acc: 0.3794\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8431 - acc: 0.3686 - val_loss: 1.8364 - val_acc: 0.3819\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8401 - acc: 0.3697 - val_loss: 1.8292 - val_acc: 0.3866\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8377 - acc: 0.3702 - val_loss: 1.8296 - val_acc: 0.3847\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8354 - acc: 0.3702 - val_loss: 1.8292 - val_acc: 0.3817\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8333 - acc: 0.3723 - val_loss: 1.8301 - val_acc: 0.3848\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8311 - acc: 0.3721 - val_loss: 1.8342 - val_acc: 0.3820\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8294 - acc: 0.3727 - val_loss: 1.8281 - val_acc: 0.3813\n",
            "32890/32890 [==============================] - 1s 20us/sample - loss: 1.9312 - acc: 0.3264\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 6s 53us/sample - loss: 2.2202 - acc: 0.2381 - val_loss: 2.0438 - val_acc: 0.2795\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.9728 - acc: 0.3234 - val_loss: 1.9422 - val_acc: 0.3400\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9337 - acc: 0.3347 - val_loss: 1.9222 - val_acc: 0.3509\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9197 - acc: 0.3402 - val_loss: 1.9020 - val_acc: 0.3622\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9108 - acc: 0.3433 - val_loss: 1.8935 - val_acc: 0.3639\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.9040 - acc: 0.3440 - val_loss: 1.8770 - val_acc: 0.3688\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8987 - acc: 0.3463 - val_loss: 1.8832 - val_acc: 0.3679\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8938 - acc: 0.3485 - val_loss: 1.8637 - val_acc: 0.3759\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8894 - acc: 0.3500 - val_loss: 1.8669 - val_acc: 0.3752\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8857 - acc: 0.3514 - val_loss: 1.8764 - val_acc: 0.3680\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8821 - acc: 0.3522 - val_loss: 1.8665 - val_acc: 0.3722\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8789 - acc: 0.3534 - val_loss: 1.8618 - val_acc: 0.3720\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8757 - acc: 0.3539 - val_loss: 1.8603 - val_acc: 0.3764\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8727 - acc: 0.3544 - val_loss: 1.8438 - val_acc: 0.3809\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8700 - acc: 0.3560 - val_loss: 1.8450 - val_acc: 0.3815\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8675 - acc: 0.3571 - val_loss: 1.8473 - val_acc: 0.3774\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8652 - acc: 0.3580 - val_loss: 1.8484 - val_acc: 0.3777\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8629 - acc: 0.3581 - val_loss: 1.8460 - val_acc: 0.3775\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8610 - acc: 0.3580 - val_loss: 1.8452 - val_acc: 0.3769\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8588 - acc: 0.3591 - val_loss: 1.8565 - val_acc: 0.3707\n",
            "32890/32890 [==============================] - 1s 19us/sample - loss: 1.8376 - acc: 0.3676\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 6s 54us/sample - loss: 2.2176 - acc: 0.2319 - val_loss: 2.0460 - val_acc: 0.2782\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9671 - acc: 0.3236 - val_loss: 1.9412 - val_acc: 0.3373\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9269 - acc: 0.3394 - val_loss: 1.9209 - val_acc: 0.3512\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9114 - acc: 0.3465 - val_loss: 1.9008 - val_acc: 0.3629\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.9004 - acc: 0.3486 - val_loss: 1.9024 - val_acc: 0.3593\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8923 - acc: 0.3511 - val_loss: 1.8850 - val_acc: 0.3691\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8859 - acc: 0.3525 - val_loss: 1.8828 - val_acc: 0.3684\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8803 - acc: 0.3551 - val_loss: 1.8844 - val_acc: 0.3689\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8757 - acc: 0.3558 - val_loss: 1.8786 - val_acc: 0.3700\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8718 - acc: 0.3567 - val_loss: 1.8915 - val_acc: 0.3615\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8686 - acc: 0.3573 - val_loss: 1.8756 - val_acc: 0.3704\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8656 - acc: 0.3582 - val_loss: 1.8629 - val_acc: 0.3757\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8631 - acc: 0.3591 - val_loss: 1.8700 - val_acc: 0.3733\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8604 - acc: 0.3599 - val_loss: 1.8597 - val_acc: 0.3772\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8583 - acc: 0.3606 - val_loss: 1.8529 - val_acc: 0.3803\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8560 - acc: 0.3608 - val_loss: 1.8580 - val_acc: 0.3754\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8541 - acc: 0.3625 - val_loss: 1.8558 - val_acc: 0.3766\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8522 - acc: 0.3627 - val_loss: 1.8440 - val_acc: 0.3821\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8504 - acc: 0.3621 - val_loss: 1.8492 - val_acc: 0.3794\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8484 - acc: 0.3636 - val_loss: 1.8552 - val_acc: 0.3743\n",
            "32890/32890 [==============================] - 1s 18us/sample - loss: 1.8780 - acc: 0.3501\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 6s 53us/sample - loss: 2.2226 - acc: 0.2301 - val_loss: 2.0460 - val_acc: 0.2763\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.9773 - acc: 0.3190 - val_loss: 1.9320 - val_acc: 0.3215\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.9393 - acc: 0.3335 - val_loss: 1.9194 - val_acc: 0.3301\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.9231 - acc: 0.3394 - val_loss: 1.9061 - val_acc: 0.3360\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.9109 - acc: 0.3434 - val_loss: 1.9006 - val_acc: 0.3378\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.9017 - acc: 0.3461 - val_loss: 1.8883 - val_acc: 0.3448\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8945 - acc: 0.3482 - val_loss: 1.8831 - val_acc: 0.3486\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8885 - acc: 0.3497 - val_loss: 1.8723 - val_acc: 0.3538\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8835 - acc: 0.3513 - val_loss: 1.8697 - val_acc: 0.3511\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 3s 28us/sample - loss: 1.8791 - acc: 0.3520 - val_loss: 1.8778 - val_acc: 0.3487\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 3s 28us/sample - loss: 1.8751 - acc: 0.3537 - val_loss: 1.8672 - val_acc: 0.3546\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8719 - acc: 0.3549 - val_loss: 1.8714 - val_acc: 0.3511\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8688 - acc: 0.3551 - val_loss: 1.8558 - val_acc: 0.3561\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8659 - acc: 0.3563 - val_loss: 1.8479 - val_acc: 0.3612\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8632 - acc: 0.3573 - val_loss: 1.8536 - val_acc: 0.3546\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8610 - acc: 0.3576 - val_loss: 1.8492 - val_acc: 0.3578\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 3s 25us/sample - loss: 1.8587 - acc: 0.3590 - val_loss: 1.8452 - val_acc: 0.3588\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8566 - acc: 0.3597 - val_loss: 1.8402 - val_acc: 0.3606\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8546 - acc: 0.3604 - val_loss: 1.8495 - val_acc: 0.3549\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8528 - acc: 0.3598 - val_loss: 1.8439 - val_acc: 0.3579\n",
            "32889/32889 [==============================] - 1s 19us/sample - loss: 1.8648 - acc: 0.3799\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 5s 52us/sample - loss: 1.9875 - acc: 0.3255 - val_loss: 1.8917 - val_acc: 0.3594\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9327 - acc: 0.3395 - val_loss: 1.8835 - val_acc: 0.3631\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9315 - acc: 0.3406 - val_loss: 1.8799 - val_acc: 0.3649\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9312 - acc: 0.3404 - val_loss: 1.8779 - val_acc: 0.3671\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9313 - acc: 0.3396 - val_loss: 1.8762 - val_acc: 0.3686\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9307 - acc: 0.3410 - val_loss: 1.8880 - val_acc: 0.3643\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9311 - acc: 0.3397 - val_loss: 1.8837 - val_acc: 0.3641\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9306 - acc: 0.3408 - val_loss: 1.8774 - val_acc: 0.3669\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9305 - acc: 0.3395 - val_loss: 1.8809 - val_acc: 0.3666\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9302 - acc: 0.3411 - val_loss: 1.8885 - val_acc: 0.3653\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9304 - acc: 0.3412 - val_loss: 1.8913 - val_acc: 0.3606\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9302 - acc: 0.3402 - val_loss: 1.8786 - val_acc: 0.3679\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9306 - acc: 0.3400 - val_loss: 1.8796 - val_acc: 0.3696\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9305 - acc: 0.3408 - val_loss: 1.8804 - val_acc: 0.3677\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9301 - acc: 0.3404 - val_loss: 1.8802 - val_acc: 0.3622\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9302 - acc: 0.3400 - val_loss: 1.8779 - val_acc: 0.3672\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9305 - acc: 0.3406 - val_loss: 1.8777 - val_acc: 0.3680\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9300 - acc: 0.3398 - val_loss: 1.8844 - val_acc: 0.3671\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9297 - acc: 0.3409 - val_loss: 1.8801 - val_acc: 0.3635\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9298 - acc: 0.3400 - val_loss: 1.8799 - val_acc: 0.3667\n",
            "32890/32890 [==============================] - 1s 18us/sample - loss: 1.9515 - acc: 0.3275\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 6s 53us/sample - loss: 1.9795 - acc: 0.3291 - val_loss: 1.8910 - val_acc: 0.3652\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9169 - acc: 0.3471 - val_loss: 1.8794 - val_acc: 0.3688\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9165 - acc: 0.3468 - val_loss: 1.8899 - val_acc: 0.3591\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9159 - acc: 0.3475 - val_loss: 1.8806 - val_acc: 0.3657\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9158 - acc: 0.3482 - val_loss: 1.8948 - val_acc: 0.3589\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9163 - acc: 0.3465 - val_loss: 1.8843 - val_acc: 0.3644\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9159 - acc: 0.3474 - val_loss: 1.8921 - val_acc: 0.3591\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9156 - acc: 0.3480 - val_loss: 1.8971 - val_acc: 0.3543\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9152 - acc: 0.3478 - val_loss: 1.8894 - val_acc: 0.3585\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9154 - acc: 0.3474 - val_loss: 1.8865 - val_acc: 0.3612\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9152 - acc: 0.3476 - val_loss: 1.8935 - val_acc: 0.3582\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9155 - acc: 0.3476 - val_loss: 1.8811 - val_acc: 0.3662\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9151 - acc: 0.3484 - val_loss: 1.8863 - val_acc: 0.3619\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9151 - acc: 0.3476 - val_loss: 1.8913 - val_acc: 0.3597\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9153 - acc: 0.3482 - val_loss: 1.8922 - val_acc: 0.3578\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9150 - acc: 0.3486 - val_loss: 1.8887 - val_acc: 0.3642\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9150 - acc: 0.3488 - val_loss: 1.8916 - val_acc: 0.3595\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9149 - acc: 0.3491 - val_loss: 1.8849 - val_acc: 0.3648\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9147 - acc: 0.3480 - val_loss: 1.8814 - val_acc: 0.3647\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9146 - acc: 0.3476 - val_loss: 1.9006 - val_acc: 0.3560\n",
            "32890/32890 [==============================] - 1s 18us/sample - loss: 1.9940 - acc: 0.3087\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 6s 53us/sample - loss: 1.9905 - acc: 0.3204 - val_loss: 1.9163 - val_acc: 0.3403\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9410 - acc: 0.3355 - val_loss: 1.9109 - val_acc: 0.3478\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9402 - acc: 0.3360 - val_loss: 1.9121 - val_acc: 0.3477\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9401 - acc: 0.3353 - val_loss: 1.9018 - val_acc: 0.3554\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9395 - acc: 0.3368 - val_loss: 1.9162 - val_acc: 0.3483\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9396 - acc: 0.3358 - val_loss: 1.9059 - val_acc: 0.3513\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9394 - acc: 0.3352 - val_loss: 1.9055 - val_acc: 0.3514\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9392 - acc: 0.3352 - val_loss: 1.9110 - val_acc: 0.3460\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9395 - acc: 0.3353 - val_loss: 1.9165 - val_acc: 0.3443\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9393 - acc: 0.3356 - val_loss: 1.8993 - val_acc: 0.3562\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9390 - acc: 0.3358 - val_loss: 1.9041 - val_acc: 0.3504\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9389 - acc: 0.3354 - val_loss: 1.8941 - val_acc: 0.3556\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9390 - acc: 0.3366 - val_loss: 1.9028 - val_acc: 0.3554\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9388 - acc: 0.3352 - val_loss: 1.9029 - val_acc: 0.3550\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9389 - acc: 0.3364 - val_loss: 1.9080 - val_acc: 0.3492\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9388 - acc: 0.3358 - val_loss: 1.9037 - val_acc: 0.3500\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9388 - acc: 0.3355 - val_loss: 1.9109 - val_acc: 0.3469\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9386 - acc: 0.3364 - val_loss: 1.9015 - val_acc: 0.3559\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9384 - acc: 0.3362 - val_loss: 1.9026 - val_acc: 0.3575\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9388 - acc: 0.3360 - val_loss: 1.9060 - val_acc: 0.3519\n",
            "32890/32890 [==============================] - 1s 18us/sample - loss: 1.9177 - acc: 0.3448\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 6s 53us/sample - loss: 2.0001 - acc: 0.3212 - val_loss: 1.9272 - val_acc: 0.3404\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9330 - acc: 0.3408 - val_loss: 1.9092 - val_acc: 0.3524\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9326 - acc: 0.3409 - val_loss: 1.9062 - val_acc: 0.3580\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9317 - acc: 0.3419 - val_loss: 1.9140 - val_acc: 0.3530\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9316 - acc: 0.3411 - val_loss: 1.9107 - val_acc: 0.3484\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.9316 - acc: 0.3413 - val_loss: 1.9065 - val_acc: 0.3588\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9313 - acc: 0.3410 - val_loss: 1.9152 - val_acc: 0.3539\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.9309 - acc: 0.3418 - val_loss: 1.9138 - val_acc: 0.3558\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9314 - acc: 0.3412 - val_loss: 1.9182 - val_acc: 0.3495\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9310 - acc: 0.3414 - val_loss: 1.9152 - val_acc: 0.3538\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9307 - acc: 0.3412 - val_loss: 1.9045 - val_acc: 0.3585\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9304 - acc: 0.3411 - val_loss: 1.9057 - val_acc: 0.3582\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9310 - acc: 0.3412 - val_loss: 1.9093 - val_acc: 0.3535\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9307 - acc: 0.3404 - val_loss: 1.9091 - val_acc: 0.3554\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9304 - acc: 0.3413 - val_loss: 1.9002 - val_acc: 0.3628\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9306 - acc: 0.3418 - val_loss: 1.9191 - val_acc: 0.3493\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9301 - acc: 0.3408 - val_loss: 1.9146 - val_acc: 0.3541\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9303 - acc: 0.3414 - val_loss: 1.9049 - val_acc: 0.3575\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9299 - acc: 0.3422 - val_loss: 1.9012 - val_acc: 0.3587\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9304 - acc: 0.3411 - val_loss: 1.9140 - val_acc: 0.3523\n",
            "32890/32890 [==============================] - 1s 19us/sample - loss: 1.9538 - acc: 0.3336\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 6s 53us/sample - loss: 2.0037 - acc: 0.3196 - val_loss: 1.9367 - val_acc: 0.3272\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 3s 25us/sample - loss: 1.9388 - acc: 0.3372 - val_loss: 1.9150 - val_acc: 0.3370\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 3s 25us/sample - loss: 1.9381 - acc: 0.3367 - val_loss: 1.9176 - val_acc: 0.3386\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.9375 - acc: 0.3375 - val_loss: 1.9137 - val_acc: 0.3367\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.9376 - acc: 0.3368 - val_loss: 1.9103 - val_acc: 0.3410\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.9375 - acc: 0.3369 - val_loss: 1.9277 - val_acc: 0.3321\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.9377 - acc: 0.3377 - val_loss: 1.9162 - val_acc: 0.3378\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.9372 - acc: 0.3373 - val_loss: 1.9050 - val_acc: 0.3432\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.9373 - acc: 0.3370 - val_loss: 1.9117 - val_acc: 0.3382\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.9372 - acc: 0.3366 - val_loss: 1.9156 - val_acc: 0.3411\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.9367 - acc: 0.3369 - val_loss: 1.9178 - val_acc: 0.3312\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.9372 - acc: 0.3371 - val_loss: 1.9172 - val_acc: 0.3338\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.9370 - acc: 0.3378 - val_loss: 1.9064 - val_acc: 0.3426\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.9367 - acc: 0.3376 - val_loss: 1.9082 - val_acc: 0.3349\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.9369 - acc: 0.3376 - val_loss: 1.9190 - val_acc: 0.3345\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.9367 - acc: 0.3375 - val_loss: 1.9170 - val_acc: 0.3384\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.9370 - acc: 0.3369 - val_loss: 1.9179 - val_acc: 0.3376\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.9366 - acc: 0.3372 - val_loss: 1.9118 - val_acc: 0.3401\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.9366 - acc: 0.3373 - val_loss: 1.9040 - val_acc: 0.3428\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.9364 - acc: 0.3377 - val_loss: 1.9042 - val_acc: 0.3416\n",
            "32889/32889 [==============================] - 1s 21us/sample - loss: 1.9165 - acc: 0.3593\n",
            "Train on 131559 samples, validate on 32890 samples\n",
            "Epoch 1/20\n",
            "131559/131559 [==============================] - 6s 48us/sample - loss: 1.9748 - acc: 0.3202 - val_loss: 1.8887 - val_acc: 0.3709\n",
            "Epoch 2/20\n",
            "131559/131559 [==============================] - 4s 27us/sample - loss: 1.8734 - acc: 0.3534 - val_loss: 1.8712 - val_acc: 0.3742\n",
            "Epoch 3/20\n",
            "131559/131559 [==============================] - 4s 27us/sample - loss: 1.8533 - acc: 0.3602 - val_loss: 1.8566 - val_acc: 0.3826\n",
            "Epoch 4/20\n",
            "131559/131559 [==============================] - 4s 27us/sample - loss: 1.8410 - acc: 0.3639 - val_loss: 1.8502 - val_acc: 0.3831\n",
            "Epoch 5/20\n",
            "131559/131559 [==============================] - 3s 26us/sample - loss: 1.8322 - acc: 0.3659 - val_loss: 1.8454 - val_acc: 0.3851\n",
            "Epoch 6/20\n",
            "131559/131559 [==============================] - 3s 26us/sample - loss: 1.8252 - acc: 0.3685 - val_loss: 1.8463 - val_acc: 0.3854\n",
            "Epoch 7/20\n",
            "131559/131559 [==============================] - 3s 26us/sample - loss: 1.8200 - acc: 0.3704 - val_loss: 1.8309 - val_acc: 0.3882\n",
            "Epoch 8/20\n",
            "131559/131559 [==============================] - 3s 26us/sample - loss: 1.8163 - acc: 0.3711 - val_loss: 1.8343 - val_acc: 0.3900\n",
            "Epoch 9/20\n",
            "131559/131559 [==============================] - 3s 25us/sample - loss: 1.8130 - acc: 0.3723 - val_loss: 1.8330 - val_acc: 0.3872\n",
            "Epoch 10/20\n",
            "131559/131559 [==============================] - 3s 26us/sample - loss: 1.8102 - acc: 0.3736 - val_loss: 1.8302 - val_acc: 0.3881\n",
            "Epoch 11/20\n",
            "131559/131559 [==============================] - 3s 26us/sample - loss: 1.8077 - acc: 0.3748 - val_loss: 1.8297 - val_acc: 0.3897\n",
            "Epoch 12/20\n",
            "131559/131559 [==============================] - 4s 27us/sample - loss: 1.8051 - acc: 0.3758 - val_loss: 1.8249 - val_acc: 0.3919\n",
            "Epoch 13/20\n",
            "131559/131559 [==============================] - 3s 26us/sample - loss: 1.8030 - acc: 0.3754 - val_loss: 1.8251 - val_acc: 0.3914\n",
            "Epoch 14/20\n",
            "131559/131559 [==============================] - 3s 25us/sample - loss: 1.8013 - acc: 0.3756 - val_loss: 1.8416 - val_acc: 0.3857\n",
            "Epoch 15/20\n",
            "131559/131559 [==============================] - 3s 26us/sample - loss: 1.7998 - acc: 0.3761 - val_loss: 1.8274 - val_acc: 0.3909\n",
            "Epoch 16/20\n",
            "131559/131559 [==============================] - 3s 26us/sample - loss: 1.7981 - acc: 0.3767 - val_loss: 1.8224 - val_acc: 0.3919\n",
            "Epoch 17/20\n",
            "131559/131559 [==============================] - 3s 26us/sample - loss: 1.7965 - acc: 0.3781 - val_loss: 1.8291 - val_acc: 0.3892\n",
            "Epoch 18/20\n",
            "131559/131559 [==============================] - 3s 26us/sample - loss: 1.7961 - acc: 0.3784 - val_loss: 1.8331 - val_acc: 0.3882\n",
            "Epoch 19/20\n",
            "131559/131559 [==============================] - 3s 26us/sample - loss: 1.7946 - acc: 0.3778 - val_loss: 1.8219 - val_acc: 0.3929\n",
            "Epoch 20/20\n",
            "131559/131559 [==============================] - 3s 26us/sample - loss: 1.7931 - acc: 0.3792 - val_loss: 1.8281 - val_acc: 0.3882\n",
            "Best: 0.36791354417800903 using {'activation': 'relu'}\n",
            "Means: 0.35356268286705017, Stdev: 0.021356926272654498 with: {'activation': 'softmax'}\n",
            "Means: 0.35864017009735105, Stdev: 0.01484313708250438 with: {'activation': 'softplus'}\n",
            "Means: 0.36677642464637755, Stdev: 0.017185616156274126 with: {'activation': 'softsign'}\n",
            "Means: 0.36791354417800903, Stdev: 0.015593340221463218 with: {'activation': 'relu'}\n",
            "Means: 0.3662170171737671, Stdev: 0.01782096095962322 with: {'activation': 'tanh'}\n",
            "Means: 0.35816590785980223, Stdev: 0.01840979375912378 with: {'activation': 'sigmoid'}\n",
            "Means: 0.35431060791015623, Stdev: 0.01830606583890149 with: {'activation': 'hard_sigmoid'}\n",
            "Means: 0.3347665429115295, Stdev: 0.016938534040297805 with: {'activation': 'linear'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ct5aQnBs4DM",
        "colab_type": "text"
      },
      "source": [
        "### Hidden Neuron Count.*(192)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFqycONPs6OX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function for NN model.\n",
        "def create_model(neurons=1):\n",
        "    # set the model.\n",
        "    model = Sequential()\n",
        "    # hidden layers.\n",
        "    model.add(Dense(neurons, input_dim=13, activation='relu'))\n",
        "    model.add(Dense(neurons, activation='relu'))\n",
        "    model.add(Dense(12,activation='softmax'))\n",
        "    def top_3_accuracy(y_true, y_pred):\n",
        "        return top_k_categorical_accuracy(y_true, y_pred, k=3)\n",
        "\n",
        "    # set the optimizer.\n",
        "    optimizer = Adam(lr=0.001)\n",
        "    # compile the model.\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                        optimizer=optimizer,\n",
        "                        metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRUXe3fAs6V7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set the model with KerasClassifier.\n",
        "model = KerasClassifier(build_fn=create_model, epochs=20, batch_size=200, validation_split=0.2, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiRVX2trs6Tq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set the parameters for neurons.\n",
        "neurons = [12, 24, 48, 96, 192, 384, 768, 1536, 3072, 6144]\n",
        "param_grid = dict(neurons=neurons)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH7a5lI_s6Rb",
        "colab_type": "code",
        "outputId": "4d5420de-8c45-47fc-830e-156967553175",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# set the gridsearch.\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
        "grid_result = grid.fit(X_scaled, y_train)\n",
        "# show the results.\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 6s 53us/sample - loss: 2.1970 - acc: 0.2360 - val_loss: 1.9325 - val_acc: 0.3523\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9387 - acc: 0.3359 - val_loss: 1.8758 - val_acc: 0.3718\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9087 - acc: 0.3465 - val_loss: 1.8558 - val_acc: 0.3814\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8967 - acc: 0.3500 - val_loss: 1.8475 - val_acc: 0.3830\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8888 - acc: 0.3513 - val_loss: 1.8398 - val_acc: 0.3826\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8826 - acc: 0.3534 - val_loss: 1.8421 - val_acc: 0.3798\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8773 - acc: 0.3539 - val_loss: 1.8275 - val_acc: 0.3867\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8723 - acc: 0.3549 - val_loss: 1.8312 - val_acc: 0.3813\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8682 - acc: 0.3558 - val_loss: 1.8293 - val_acc: 0.3816\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8645 - acc: 0.3570 - val_loss: 1.8263 - val_acc: 0.3814\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8614 - acc: 0.3575 - val_loss: 1.8259 - val_acc: 0.3850\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8587 - acc: 0.3581 - val_loss: 1.8272 - val_acc: 0.3806\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8566 - acc: 0.3588 - val_loss: 1.8235 - val_acc: 0.3817\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8546 - acc: 0.3589 - val_loss: 1.8225 - val_acc: 0.3826\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8532 - acc: 0.3597 - val_loss: 1.8145 - val_acc: 0.3890\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8517 - acc: 0.3605 - val_loss: 1.8196 - val_acc: 0.3839\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8501 - acc: 0.3609 - val_loss: 1.8150 - val_acc: 0.3857\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8495 - acc: 0.3604 - val_loss: 1.8163 - val_acc: 0.3837\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8486 - acc: 0.3604 - val_loss: 1.8211 - val_acc: 0.3844\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8478 - acc: 0.3608 - val_loss: 1.8189 - val_acc: 0.3853\n",
            "32890/32890 [==============================] - 1s 21us/sample - loss: 1.8893 - acc: 0.3478\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 6s 54us/sample - loss: 2.1869 - acc: 0.2451 - val_loss: 1.9912 - val_acc: 0.3272\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.9355 - acc: 0.3371 - val_loss: 1.9139 - val_acc: 0.3548\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9003 - acc: 0.3477 - val_loss: 1.8788 - val_acc: 0.3703\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8857 - acc: 0.3526 - val_loss: 1.8792 - val_acc: 0.3674\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8767 - acc: 0.3560 - val_loss: 1.8668 - val_acc: 0.3740\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8706 - acc: 0.3583 - val_loss: 1.8507 - val_acc: 0.3823\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8655 - acc: 0.3602 - val_loss: 1.8464 - val_acc: 0.3850\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8615 - acc: 0.3608 - val_loss: 1.8479 - val_acc: 0.3831\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8580 - acc: 0.3620 - val_loss: 1.8426 - val_acc: 0.3843\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8550 - acc: 0.3634 - val_loss: 1.8480 - val_acc: 0.3818\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8522 - acc: 0.3644 - val_loss: 1.8431 - val_acc: 0.3850\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8501 - acc: 0.3655 - val_loss: 1.8348 - val_acc: 0.3890\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8478 - acc: 0.3650 - val_loss: 1.8409 - val_acc: 0.3869\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8459 - acc: 0.3661 - val_loss: 1.8328 - val_acc: 0.3873\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8441 - acc: 0.3668 - val_loss: 1.8247 - val_acc: 0.3908\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8430 - acc: 0.3673 - val_loss: 1.8324 - val_acc: 0.3860\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8417 - acc: 0.3666 - val_loss: 1.8320 - val_acc: 0.3893\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8406 - acc: 0.3679 - val_loss: 1.8227 - val_acc: 0.3915\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8396 - acc: 0.3680 - val_loss: 1.8321 - val_acc: 0.3873\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8387 - acc: 0.3677 - val_loss: 1.8257 - val_acc: 0.3915\n",
            "32890/32890 [==============================] - 1s 19us/sample - loss: 1.9352 - acc: 0.3261\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 6s 52us/sample - loss: 2.1582 - acc: 0.2480 - val_loss: 1.9615 - val_acc: 0.3178\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9454 - acc: 0.3255 - val_loss: 1.9112 - val_acc: 0.3504\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.9172 - acc: 0.3382 - val_loss: 1.8810 - val_acc: 0.3657\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.9042 - acc: 0.3434 - val_loss: 1.8941 - val_acc: 0.3579\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8960 - acc: 0.3455 - val_loss: 1.8738 - val_acc: 0.3647\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8901 - acc: 0.3470 - val_loss: 1.8712 - val_acc: 0.3635\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8854 - acc: 0.3489 - val_loss: 1.8700 - val_acc: 0.3670\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8813 - acc: 0.3503 - val_loss: 1.8692 - val_acc: 0.3674\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8781 - acc: 0.3518 - val_loss: 1.8611 - val_acc: 0.3706\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8755 - acc: 0.3526 - val_loss: 1.8631 - val_acc: 0.3688\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8733 - acc: 0.3526 - val_loss: 1.8599 - val_acc: 0.3706\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8712 - acc: 0.3545 - val_loss: 1.8575 - val_acc: 0.3726\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8692 - acc: 0.3552 - val_loss: 1.8650 - val_acc: 0.3691\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8678 - acc: 0.3556 - val_loss: 1.8580 - val_acc: 0.3708\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8660 - acc: 0.3560 - val_loss: 1.8514 - val_acc: 0.3740\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8646 - acc: 0.3555 - val_loss: 1.8500 - val_acc: 0.3750\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8636 - acc: 0.3565 - val_loss: 1.8488 - val_acc: 0.3747\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8622 - acc: 0.3568 - val_loss: 1.8512 - val_acc: 0.3734\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8613 - acc: 0.3574 - val_loss: 1.8401 - val_acc: 0.3802\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8602 - acc: 0.3576 - val_loss: 1.8352 - val_acc: 0.3831\n",
            "32890/32890 [==============================] - 1s 20us/sample - loss: 1.8364 - acc: 0.3706\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 6s 55us/sample - loss: 2.1559 - acc: 0.2528 - val_loss: 1.9630 - val_acc: 0.3371\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.9320 - acc: 0.3359 - val_loss: 1.9172 - val_acc: 0.3540\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.9075 - acc: 0.3443 - val_loss: 1.8984 - val_acc: 0.3633\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8962 - acc: 0.3471 - val_loss: 1.8926 - val_acc: 0.3664\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8888 - acc: 0.3485 - val_loss: 1.8807 - val_acc: 0.3714\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8832 - acc: 0.3505 - val_loss: 1.8771 - val_acc: 0.3699\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8780 - acc: 0.3526 - val_loss: 1.8683 - val_acc: 0.3734\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8740 - acc: 0.3528 - val_loss: 1.8824 - val_acc: 0.3672\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8705 - acc: 0.3549 - val_loss: 1.8673 - val_acc: 0.3715\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8673 - acc: 0.3562 - val_loss: 1.8550 - val_acc: 0.3767\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8649 - acc: 0.3575 - val_loss: 1.8697 - val_acc: 0.3699\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8626 - acc: 0.3572 - val_loss: 1.8613 - val_acc: 0.3714\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8606 - acc: 0.3581 - val_loss: 1.8712 - val_acc: 0.3698\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8589 - acc: 0.3588 - val_loss: 1.8552 - val_acc: 0.3758\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8577 - acc: 0.3594 - val_loss: 1.8600 - val_acc: 0.3736\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8563 - acc: 0.3600 - val_loss: 1.8513 - val_acc: 0.3761\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8556 - acc: 0.3604 - val_loss: 1.8643 - val_acc: 0.3700\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8544 - acc: 0.3614 - val_loss: 1.8503 - val_acc: 0.3774\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8532 - acc: 0.3615 - val_loss: 1.8528 - val_acc: 0.3783\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8528 - acc: 0.3610 - val_loss: 1.8590 - val_acc: 0.3742\n",
            "32890/32890 [==============================] - 1s 21us/sample - loss: 1.8790 - acc: 0.3497\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 6s 55us/sample - loss: 2.1772 - acc: 0.2475 - val_loss: 1.9838 - val_acc: 0.3162\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.9524 - acc: 0.3276 - val_loss: 1.9186 - val_acc: 0.3343\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.9221 - acc: 0.3382 - val_loss: 1.9057 - val_acc: 0.3395\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.9086 - acc: 0.3425 - val_loss: 1.8921 - val_acc: 0.3442\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8987 - acc: 0.3457 - val_loss: 1.8927 - val_acc: 0.3430\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.8910 - acc: 0.3478 - val_loss: 1.8749 - val_acc: 0.3528\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8849 - acc: 0.3499 - val_loss: 1.8754 - val_acc: 0.3472\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 3s 28us/sample - loss: 1.8807 - acc: 0.3507 - val_loss: 1.8718 - val_acc: 0.3484\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8774 - acc: 0.3529 - val_loss: 1.8681 - val_acc: 0.3481\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.8747 - acc: 0.3540 - val_loss: 1.8702 - val_acc: 0.3501\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.8726 - acc: 0.3550 - val_loss: 1.8664 - val_acc: 0.3494\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.8706 - acc: 0.3549 - val_loss: 1.8579 - val_acc: 0.3555\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 3s 28us/sample - loss: 1.8687 - acc: 0.3560 - val_loss: 1.8629 - val_acc: 0.3516\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.8671 - acc: 0.3565 - val_loss: 1.8610 - val_acc: 0.3500\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8660 - acc: 0.3567 - val_loss: 1.8534 - val_acc: 0.3531\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.8645 - acc: 0.3570 - val_loss: 1.8593 - val_acc: 0.3477\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8633 - acc: 0.3576 - val_loss: 1.8540 - val_acc: 0.3530\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8625 - acc: 0.3576 - val_loss: 1.8480 - val_acc: 0.3559\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.8613 - acc: 0.3582 - val_loss: 1.8479 - val_acc: 0.3553\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8603 - acc: 0.3579 - val_loss: 1.8521 - val_acc: 0.3555\n",
            "32889/32889 [==============================] - 1s 19us/sample - loss: 1.8570 - acc: 0.3826\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 6s 56us/sample - loss: 2.0726 - acc: 0.2907 - val_loss: 1.8851 - val_acc: 0.3685\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.9032 - acc: 0.3459 - val_loss: 1.8527 - val_acc: 0.3744\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8790 - acc: 0.3529 - val_loss: 1.8310 - val_acc: 0.3833\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8661 - acc: 0.3563 - val_loss: 1.8249 - val_acc: 0.3859\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8576 - acc: 0.3594 - val_loss: 1.8203 - val_acc: 0.3888\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8516 - acc: 0.3608 - val_loss: 1.8237 - val_acc: 0.3839\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8466 - acc: 0.3620 - val_loss: 1.8095 - val_acc: 0.3901\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8423 - acc: 0.3639 - val_loss: 1.8215 - val_acc: 0.3846\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8383 - acc: 0.3655 - val_loss: 1.8012 - val_acc: 0.3934\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8344 - acc: 0.3665 - val_loss: 1.8049 - val_acc: 0.3910\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8318 - acc: 0.3673 - val_loss: 1.8079 - val_acc: 0.3900\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8294 - acc: 0.3681 - val_loss: 1.8040 - val_acc: 0.3917\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8266 - acc: 0.3695 - val_loss: 1.8055 - val_acc: 0.3920\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8245 - acc: 0.3697 - val_loss: 1.8036 - val_acc: 0.3920\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8229 - acc: 0.3712 - val_loss: 1.8117 - val_acc: 0.3877\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8212 - acc: 0.3710 - val_loss: 1.7983 - val_acc: 0.3945\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8195 - acc: 0.3729 - val_loss: 1.7938 - val_acc: 0.3972\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8179 - acc: 0.3720 - val_loss: 1.7923 - val_acc: 0.3969\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8168 - acc: 0.3729 - val_loss: 1.7904 - val_acc: 0.3958\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8158 - acc: 0.3729 - val_loss: 1.8003 - val_acc: 0.3920\n",
            "32890/32890 [==============================] - 1s 19us/sample - loss: 1.8456 - acc: 0.3656\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 6s 55us/sample - loss: 2.0287 - acc: 0.3104 - val_loss: 1.8845 - val_acc: 0.3659\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8833 - acc: 0.3548 - val_loss: 1.8632 - val_acc: 0.3730\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8611 - acc: 0.3617 - val_loss: 1.8356 - val_acc: 0.3813\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8487 - acc: 0.3668 - val_loss: 1.8368 - val_acc: 0.3807\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8402 - acc: 0.3691 - val_loss: 1.8379 - val_acc: 0.3786\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8340 - acc: 0.3705 - val_loss: 1.8303 - val_acc: 0.3829\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8288 - acc: 0.3724 - val_loss: 1.8165 - val_acc: 0.3857\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8243 - acc: 0.3741 - val_loss: 1.8156 - val_acc: 0.3862\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8209 - acc: 0.3746 - val_loss: 1.8068 - val_acc: 0.3918\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8179 - acc: 0.3768 - val_loss: 1.8188 - val_acc: 0.3852\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8147 - acc: 0.3776 - val_loss: 1.8095 - val_acc: 0.3892\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8120 - acc: 0.3787 - val_loss: 1.8159 - val_acc: 0.3821\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8098 - acc: 0.3793 - val_loss: 1.8119 - val_acc: 0.3873\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8078 - acc: 0.3786 - val_loss: 1.8110 - val_acc: 0.3873\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8061 - acc: 0.3799 - val_loss: 1.8109 - val_acc: 0.3874\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8045 - acc: 0.3810 - val_loss: 1.8083 - val_acc: 0.3881\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8030 - acc: 0.3806 - val_loss: 1.8086 - val_acc: 0.3885\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8020 - acc: 0.3812 - val_loss: 1.8073 - val_acc: 0.3880\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8006 - acc: 0.3815 - val_loss: 1.8102 - val_acc: 0.3845\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.7996 - acc: 0.3818 - val_loss: 1.8108 - val_acc: 0.3877\n",
            "32890/32890 [==============================] - 1s 19us/sample - loss: 1.8984 - acc: 0.3385\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 6s 53us/sample - loss: 2.0447 - acc: 0.2975 - val_loss: 1.9075 - val_acc: 0.3560\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9040 - acc: 0.3439 - val_loss: 1.8732 - val_acc: 0.3659\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8830 - acc: 0.3511 - val_loss: 1.8512 - val_acc: 0.3763\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8715 - acc: 0.3539 - val_loss: 1.8578 - val_acc: 0.3726\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8637 - acc: 0.3561 - val_loss: 1.8394 - val_acc: 0.3805\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8575 - acc: 0.3586 - val_loss: 1.8370 - val_acc: 0.3820\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8524 - acc: 0.3598 - val_loss: 1.8326 - val_acc: 0.3819\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8488 - acc: 0.3608 - val_loss: 1.8448 - val_acc: 0.3755\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8452 - acc: 0.3609 - val_loss: 1.8232 - val_acc: 0.3851\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8422 - acc: 0.3630 - val_loss: 1.8246 - val_acc: 0.3859\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8392 - acc: 0.3639 - val_loss: 1.8361 - val_acc: 0.3798\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8373 - acc: 0.3641 - val_loss: 1.8212 - val_acc: 0.3832\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8354 - acc: 0.3644 - val_loss: 1.8347 - val_acc: 0.3759\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8331 - acc: 0.3644 - val_loss: 1.8297 - val_acc: 0.3798\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8315 - acc: 0.3667 - val_loss: 1.8319 - val_acc: 0.3783\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8295 - acc: 0.3668 - val_loss: 1.8219 - val_acc: 0.3822\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8288 - acc: 0.3657 - val_loss: 1.8202 - val_acc: 0.3815\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8273 - acc: 0.3682 - val_loss: 1.8249 - val_acc: 0.3820\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8263 - acc: 0.3682 - val_loss: 1.8234 - val_acc: 0.3830\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8251 - acc: 0.3673 - val_loss: 1.8254 - val_acc: 0.3785\n",
            "32890/32890 [==============================] - 1s 19us/sample - loss: 1.8181 - acc: 0.3754\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 6s 57us/sample - loss: 2.0715 - acc: 0.2846 - val_loss: 1.9090 - val_acc: 0.3596\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.9044 - acc: 0.3469 - val_loss: 1.8887 - val_acc: 0.3624\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8830 - acc: 0.3532 - val_loss: 1.8699 - val_acc: 0.3707\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8703 - acc: 0.3562 - val_loss: 1.8566 - val_acc: 0.3772\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8602 - acc: 0.3599 - val_loss: 1.8563 - val_acc: 0.3733\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8531 - acc: 0.3607 - val_loss: 1.8652 - val_acc: 0.3715\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8476 - acc: 0.3624 - val_loss: 1.8519 - val_acc: 0.3793\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8435 - acc: 0.3637 - val_loss: 1.8368 - val_acc: 0.3806\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8399 - acc: 0.3644 - val_loss: 1.8496 - val_acc: 0.3739\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8367 - acc: 0.3659 - val_loss: 1.8472 - val_acc: 0.3749\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8346 - acc: 0.3660 - val_loss: 1.8473 - val_acc: 0.3755\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8322 - acc: 0.3669 - val_loss: 1.8302 - val_acc: 0.3807\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8299 - acc: 0.3677 - val_loss: 1.8382 - val_acc: 0.3783\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8285 - acc: 0.3687 - val_loss: 1.8470 - val_acc: 0.3760\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8266 - acc: 0.3690 - val_loss: 1.8416 - val_acc: 0.3761\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8251 - acc: 0.3704 - val_loss: 1.8332 - val_acc: 0.3808\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8239 - acc: 0.3703 - val_loss: 1.8217 - val_acc: 0.3855\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8225 - acc: 0.3716 - val_loss: 1.8188 - val_acc: 0.3870\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8210 - acc: 0.3722 - val_loss: 1.8257 - val_acc: 0.3818\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8199 - acc: 0.3726 - val_loss: 1.8218 - val_acc: 0.3867\n",
            "32890/32890 [==============================] - 1s 19us/sample - loss: 1.8409 - acc: 0.3623\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 6s 53us/sample - loss: 2.0454 - acc: 0.3011 - val_loss: 1.9081 - val_acc: 0.3384\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.9057 - acc: 0.3457 - val_loss: 1.8857 - val_acc: 0.3435\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8845 - acc: 0.3517 - val_loss: 1.8741 - val_acc: 0.3494\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.8718 - acc: 0.3558 - val_loss: 1.8617 - val_acc: 0.3489\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 3s 28us/sample - loss: 1.8635 - acc: 0.3581 - val_loss: 1.8555 - val_acc: 0.3559\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8576 - acc: 0.3584 - val_loss: 1.8485 - val_acc: 0.3546\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.8529 - acc: 0.3595 - val_loss: 1.8570 - val_acc: 0.3538\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8486 - acc: 0.3608 - val_loss: 1.8542 - val_acc: 0.3520\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8457 - acc: 0.3627 - val_loss: 1.8415 - val_acc: 0.3575\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8427 - acc: 0.3632 - val_loss: 1.8365 - val_acc: 0.3593\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.8399 - acc: 0.3637 - val_loss: 1.8429 - val_acc: 0.3568\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.8376 - acc: 0.3646 - val_loss: 1.8354 - val_acc: 0.3577\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8352 - acc: 0.3657 - val_loss: 1.8261 - val_acc: 0.3649\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8334 - acc: 0.3667 - val_loss: 1.8389 - val_acc: 0.3615\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8316 - acc: 0.3675 - val_loss: 1.8230 - val_acc: 0.3663\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8300 - acc: 0.3671 - val_loss: 1.8229 - val_acc: 0.3661\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8284 - acc: 0.3681 - val_loss: 1.8285 - val_acc: 0.3664\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8268 - acc: 0.3685 - val_loss: 1.8180 - val_acc: 0.3666\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8253 - acc: 0.3701 - val_loss: 1.8357 - val_acc: 0.3582\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 3s 28us/sample - loss: 1.8241 - acc: 0.3699 - val_loss: 1.8177 - val_acc: 0.3677\n",
            "32889/32889 [==============================] - 1s 22us/sample - loss: 1.8417 - acc: 0.3854\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 6s 55us/sample - loss: 1.9814 - acc: 0.3245 - val_loss: 1.8644 - val_acc: 0.3722\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8702 - acc: 0.3559 - val_loss: 1.8472 - val_acc: 0.3723\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8489 - acc: 0.3638 - val_loss: 1.8232 - val_acc: 0.3828\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8362 - acc: 0.3676 - val_loss: 1.8138 - val_acc: 0.3861\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8276 - acc: 0.3698 - val_loss: 1.8000 - val_acc: 0.3948\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8211 - acc: 0.3724 - val_loss: 1.8009 - val_acc: 0.3923\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8146 - acc: 0.3739 - val_loss: 1.8084 - val_acc: 0.3853\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8099 - acc: 0.3755 - val_loss: 1.7854 - val_acc: 0.3984\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8060 - acc: 0.3767 - val_loss: 1.7818 - val_acc: 0.4003\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8020 - acc: 0.3784 - val_loss: 1.7920 - val_acc: 0.3958\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.7995 - acc: 0.3794 - val_loss: 1.7848 - val_acc: 0.3954\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.7966 - acc: 0.3797 - val_loss: 1.7985 - val_acc: 0.3894\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.7940 - acc: 0.3802 - val_loss: 1.7830 - val_acc: 0.3983\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.7913 - acc: 0.3817 - val_loss: 1.7790 - val_acc: 0.3997\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.7889 - acc: 0.3822 - val_loss: 1.7976 - val_acc: 0.3947\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.7876 - acc: 0.3826 - val_loss: 1.7947 - val_acc: 0.3913\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.7856 - acc: 0.3843 - val_loss: 1.7766 - val_acc: 0.4016\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.7843 - acc: 0.3838 - val_loss: 1.7826 - val_acc: 0.4007\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7827 - acc: 0.3849 - val_loss: 1.7859 - val_acc: 0.3943\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.7812 - acc: 0.3850 - val_loss: 1.7840 - val_acc: 0.3978\n",
            "32890/32890 [==============================] - 1s 20us/sample - loss: 1.8455 - acc: 0.3624\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 6s 59us/sample - loss: 1.9718 - acc: 0.3280 - val_loss: 1.8647 - val_acc: 0.3715\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8535 - acc: 0.3642 - val_loss: 1.8461 - val_acc: 0.3794\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8343 - acc: 0.3703 - val_loss: 1.8233 - val_acc: 0.3835\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8222 - acc: 0.3742 - val_loss: 1.8189 - val_acc: 0.3845\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8132 - acc: 0.3772 - val_loss: 1.8170 - val_acc: 0.3863\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8061 - acc: 0.3798 - val_loss: 1.8133 - val_acc: 0.3858\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8000 - acc: 0.3813 - val_loss: 1.8003 - val_acc: 0.3889\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.7950 - acc: 0.3824 - val_loss: 1.8018 - val_acc: 0.3933\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.7908 - acc: 0.3839 - val_loss: 1.8160 - val_acc: 0.3848\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.7876 - acc: 0.3849 - val_loss: 1.7996 - val_acc: 0.3906\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.7842 - acc: 0.3862 - val_loss: 1.8044 - val_acc: 0.3904\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.7819 - acc: 0.3872 - val_loss: 1.7942 - val_acc: 0.3953\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.7792 - acc: 0.3889 - val_loss: 1.7887 - val_acc: 0.3969\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.7766 - acc: 0.3898 - val_loss: 1.7861 - val_acc: 0.3979\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.7752 - acc: 0.3905 - val_loss: 1.7953 - val_acc: 0.3916\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7728 - acc: 0.3914 - val_loss: 1.7974 - val_acc: 0.3897\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.7712 - acc: 0.3919 - val_loss: 1.7811 - val_acc: 0.3991\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.7691 - acc: 0.3927 - val_loss: 1.7890 - val_acc: 0.3979\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.7680 - acc: 0.3915 - val_loss: 1.7946 - val_acc: 0.3958\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.7662 - acc: 0.3938 - val_loss: 1.7910 - val_acc: 0.3945\n",
            "32890/32890 [==============================] - 1s 19us/sample - loss: 1.8895 - acc: 0.3393\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 6s 59us/sample - loss: 1.9972 - acc: 0.3160 - val_loss: 1.8627 - val_acc: 0.3717\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8780 - acc: 0.3512 - val_loss: 1.8585 - val_acc: 0.3706\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8582 - acc: 0.3571 - val_loss: 1.8501 - val_acc: 0.3685\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8462 - acc: 0.3610 - val_loss: 1.8437 - val_acc: 0.3735\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8382 - acc: 0.3635 - val_loss: 1.8302 - val_acc: 0.3782\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8309 - acc: 0.3667 - val_loss: 1.8219 - val_acc: 0.3805\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8254 - acc: 0.3675 - val_loss: 1.8393 - val_acc: 0.3739\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8211 - acc: 0.3700 - val_loss: 1.8163 - val_acc: 0.3844\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8165 - acc: 0.3717 - val_loss: 1.8158 - val_acc: 0.3806\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8130 - acc: 0.3730 - val_loss: 1.8001 - val_acc: 0.3918\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8095 - acc: 0.3722 - val_loss: 1.8295 - val_acc: 0.3721\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8068 - acc: 0.3740 - val_loss: 1.8086 - val_acc: 0.3835\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8041 - acc: 0.3752 - val_loss: 1.8128 - val_acc: 0.3845\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8016 - acc: 0.3765 - val_loss: 1.8088 - val_acc: 0.3867\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.7998 - acc: 0.3762 - val_loss: 1.8093 - val_acc: 0.3873\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.7979 - acc: 0.3779 - val_loss: 1.8018 - val_acc: 0.3896\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.7960 - acc: 0.3782 - val_loss: 1.8135 - val_acc: 0.3839\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.7948 - acc: 0.3789 - val_loss: 1.8109 - val_acc: 0.3833\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7932 - acc: 0.3785 - val_loss: 1.8111 - val_acc: 0.3827\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7920 - acc: 0.3798 - val_loss: 1.7998 - val_acc: 0.3900\n",
            "32890/32890 [==============================] - 1s 19us/sample - loss: 1.7846 - acc: 0.3901\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 6s 57us/sample - loss: 1.9902 - acc: 0.3169 - val_loss: 1.8866 - val_acc: 0.3644\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8710 - acc: 0.3560 - val_loss: 1.8635 - val_acc: 0.3708\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8516 - acc: 0.3627 - val_loss: 1.8589 - val_acc: 0.3746\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8397 - acc: 0.3663 - val_loss: 1.8384 - val_acc: 0.3845\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8311 - acc: 0.3676 - val_loss: 1.8496 - val_acc: 0.3756\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8242 - acc: 0.3695 - val_loss: 1.8393 - val_acc: 0.3799\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8190 - acc: 0.3716 - val_loss: 1.8329 - val_acc: 0.3824\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8146 - acc: 0.3725 - val_loss: 1.8249 - val_acc: 0.3885\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8111 - acc: 0.3737 - val_loss: 1.8526 - val_acc: 0.3722\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8078 - acc: 0.3752 - val_loss: 1.8269 - val_acc: 0.3808\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8050 - acc: 0.3764 - val_loss: 1.8492 - val_acc: 0.3729\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8026 - acc: 0.3771 - val_loss: 1.8290 - val_acc: 0.3801\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8007 - acc: 0.3780 - val_loss: 1.8371 - val_acc: 0.3775\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7988 - acc: 0.3781 - val_loss: 1.8348 - val_acc: 0.3802\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.7969 - acc: 0.3790 - val_loss: 1.8311 - val_acc: 0.3815\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7950 - acc: 0.3802 - val_loss: 1.8275 - val_acc: 0.3791\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7941 - acc: 0.3798 - val_loss: 1.8377 - val_acc: 0.3764\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.7926 - acc: 0.3806 - val_loss: 1.8162 - val_acc: 0.3876\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7908 - acc: 0.3812 - val_loss: 1.8313 - val_acc: 0.3802\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.7894 - acc: 0.3820 - val_loss: 1.8361 - val_acc: 0.3808\n",
            "32890/32890 [==============================] - 1s 20us/sample - loss: 1.8360 - acc: 0.3678\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 6s 59us/sample - loss: 1.9778 - acc: 0.3193 - val_loss: 1.8799 - val_acc: 0.3450\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.8712 - acc: 0.3535 - val_loss: 1.8425 - val_acc: 0.3623\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.8530 - acc: 0.3591 - val_loss: 1.8338 - val_acc: 0.3593\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.8414 - acc: 0.3634 - val_loss: 1.8227 - val_acc: 0.3628\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.8329 - acc: 0.3660 - val_loss: 1.8248 - val_acc: 0.3604\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.8263 - acc: 0.3681 - val_loss: 1.8259 - val_acc: 0.3578\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.8214 - acc: 0.3691 - val_loss: 1.8101 - val_acc: 0.3696\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.8169 - acc: 0.3705 - val_loss: 1.8168 - val_acc: 0.3639\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 3s 29us/sample - loss: 1.8132 - acc: 0.3721 - val_loss: 1.7997 - val_acc: 0.3707\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.8102 - acc: 0.3740 - val_loss: 1.8069 - val_acc: 0.3675\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 3s 28us/sample - loss: 1.8073 - acc: 0.3747 - val_loss: 1.8195 - val_acc: 0.3628\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 3s 28us/sample - loss: 1.8051 - acc: 0.3750 - val_loss: 1.8060 - val_acc: 0.3677\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 3s 29us/sample - loss: 1.8025 - acc: 0.3765 - val_loss: 1.8051 - val_acc: 0.3668\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 3s 29us/sample - loss: 1.8005 - acc: 0.3759 - val_loss: 1.8175 - val_acc: 0.3658\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 3s 29us/sample - loss: 1.7985 - acc: 0.3764 - val_loss: 1.8129 - val_acc: 0.3655\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 3s 29us/sample - loss: 1.7968 - acc: 0.3777 - val_loss: 1.8130 - val_acc: 0.3592\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.7952 - acc: 0.3778 - val_loss: 1.7980 - val_acc: 0.3747\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.7938 - acc: 0.3785 - val_loss: 1.7976 - val_acc: 0.3701\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.7920 - acc: 0.3788 - val_loss: 1.8030 - val_acc: 0.3721\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.7911 - acc: 0.3792 - val_loss: 1.8013 - val_acc: 0.3693\n",
            "32889/32889 [==============================] - 1s 20us/sample - loss: 1.8332 - acc: 0.3844\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 6s 59us/sample - loss: 1.9358 - acc: 0.3361 - val_loss: 1.8165 - val_acc: 0.3891\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8461 - acc: 0.3633 - val_loss: 1.8222 - val_acc: 0.3825\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8262 - acc: 0.3701 - val_loss: 1.8129 - val_acc: 0.3855\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8139 - acc: 0.3752 - val_loss: 1.8017 - val_acc: 0.3867\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8042 - acc: 0.3774 - val_loss: 1.7969 - val_acc: 0.3915\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7970 - acc: 0.3790 - val_loss: 1.7888 - val_acc: 0.3998\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7914 - acc: 0.3816 - val_loss: 1.7898 - val_acc: 0.3945\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.7854 - acc: 0.3829 - val_loss: 1.7710 - val_acc: 0.4037\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7813 - acc: 0.3838 - val_loss: 1.7790 - val_acc: 0.4025\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7773 - acc: 0.3859 - val_loss: 1.7813 - val_acc: 0.3982\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7737 - acc: 0.3862 - val_loss: 1.7923 - val_acc: 0.3902\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7702 - acc: 0.3882 - val_loss: 1.7641 - val_acc: 0.4061\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7678 - acc: 0.3884 - val_loss: 1.7748 - val_acc: 0.4007\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.7646 - acc: 0.3894 - val_loss: 1.7750 - val_acc: 0.4018\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.7622 - acc: 0.3906 - val_loss: 1.7674 - val_acc: 0.4037\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7598 - acc: 0.3923 - val_loss: 1.7820 - val_acc: 0.3969\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.7578 - acc: 0.3916 - val_loss: 1.7788 - val_acc: 0.3992\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7558 - acc: 0.3924 - val_loss: 1.7720 - val_acc: 0.4056\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.7532 - acc: 0.3924 - val_loss: 1.7781 - val_acc: 0.3996\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7519 - acc: 0.3930 - val_loss: 1.7671 - val_acc: 0.4057\n",
            "32890/32890 [==============================] - 1s 21us/sample - loss: 1.8603 - acc: 0.3584\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 6s 59us/sample - loss: 1.9080 - acc: 0.3497 - val_loss: 1.8412 - val_acc: 0.3760\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8298 - acc: 0.3725 - val_loss: 1.8144 - val_acc: 0.3883\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8100 - acc: 0.3790 - val_loss: 1.8199 - val_acc: 0.3811\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7981 - acc: 0.3817 - val_loss: 1.7939 - val_acc: 0.3931\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7890 - acc: 0.3850 - val_loss: 1.8049 - val_acc: 0.3925\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7812 - acc: 0.3871 - val_loss: 1.7969 - val_acc: 0.3908\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7752 - acc: 0.3901 - val_loss: 1.7986 - val_acc: 0.3876\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7697 - acc: 0.3914 - val_loss: 1.7887 - val_acc: 0.3921\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7655 - acc: 0.3925 - val_loss: 1.7896 - val_acc: 0.3916\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7616 - acc: 0.3947 - val_loss: 1.7842 - val_acc: 0.3980\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.7574 - acc: 0.3957 - val_loss: 1.7694 - val_acc: 0.4034\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.7550 - acc: 0.3964 - val_loss: 1.7836 - val_acc: 0.3958\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7518 - acc: 0.3968 - val_loss: 1.7803 - val_acc: 0.3992\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7494 - acc: 0.3986 - val_loss: 1.8065 - val_acc: 0.3877\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7464 - acc: 0.3985 - val_loss: 1.7895 - val_acc: 0.3977\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7440 - acc: 0.4004 - val_loss: 1.7801 - val_acc: 0.3936\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7412 - acc: 0.4010 - val_loss: 1.7848 - val_acc: 0.4035\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.7400 - acc: 0.4017 - val_loss: 1.7864 - val_acc: 0.3932\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7372 - acc: 0.4019 - val_loss: 1.7918 - val_acc: 0.3930\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.7355 - acc: 0.4019 - val_loss: 1.8056 - val_acc: 0.3891\n",
            "32890/32890 [==============================] - 1s 21us/sample - loss: 1.8767 - acc: 0.3435\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 6s 61us/sample - loss: 1.9312 - acc: 0.3372 - val_loss: 1.8507 - val_acc: 0.3720\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8570 - acc: 0.3583 - val_loss: 1.8506 - val_acc: 0.3663\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8376 - acc: 0.3647 - val_loss: 1.8216 - val_acc: 0.3810\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8247 - acc: 0.3683 - val_loss: 1.8103 - val_acc: 0.3818\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8153 - acc: 0.3717 - val_loss: 1.8216 - val_acc: 0.3715\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8081 - acc: 0.3740 - val_loss: 1.8035 - val_acc: 0.3857\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8026 - acc: 0.3754 - val_loss: 1.8119 - val_acc: 0.3807\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.7972 - acc: 0.3769 - val_loss: 1.7929 - val_acc: 0.3881\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.7924 - acc: 0.3782 - val_loss: 1.8380 - val_acc: 0.3702\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7887 - acc: 0.3806 - val_loss: 1.8049 - val_acc: 0.3841\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.7858 - acc: 0.3799 - val_loss: 1.8028 - val_acc: 0.3810\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.7818 - acc: 0.3825 - val_loss: 1.8081 - val_acc: 0.3822\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7800 - acc: 0.3833 - val_loss: 1.7983 - val_acc: 0.3846\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7763 - acc: 0.3841 - val_loss: 1.8109 - val_acc: 0.3801\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7737 - acc: 0.3848 - val_loss: 1.7933 - val_acc: 0.3915\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7720 - acc: 0.3853 - val_loss: 1.7982 - val_acc: 0.3929\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.7688 - acc: 0.3859 - val_loss: 1.8078 - val_acc: 0.3801\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.7667 - acc: 0.3868 - val_loss: 1.8094 - val_acc: 0.3812\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7651 - acc: 0.3876 - val_loss: 1.8152 - val_acc: 0.3812\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7631 - acc: 0.3885 - val_loss: 1.8080 - val_acc: 0.3803\n",
            "32890/32890 [==============================] - 1s 21us/sample - loss: 1.7820 - acc: 0.3858\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 6s 61us/sample - loss: 1.9332 - acc: 0.3363 - val_loss: 1.8536 - val_acc: 0.3783\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8466 - acc: 0.3641 - val_loss: 1.8323 - val_acc: 0.3813\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.8277 - acc: 0.3692 - val_loss: 1.8178 - val_acc: 0.3876\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8147 - acc: 0.3742 - val_loss: 1.8353 - val_acc: 0.3783\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8055 - acc: 0.3765 - val_loss: 1.8181 - val_acc: 0.3858\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7981 - acc: 0.3789 - val_loss: 1.8334 - val_acc: 0.3788\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7928 - acc: 0.3804 - val_loss: 1.8323 - val_acc: 0.3809\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.7880 - acc: 0.3817 - val_loss: 1.8238 - val_acc: 0.3836\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7833 - acc: 0.3828 - val_loss: 1.8514 - val_acc: 0.3704\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7800 - acc: 0.3839 - val_loss: 1.8291 - val_acc: 0.3825\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7764 - acc: 0.3849 - val_loss: 1.8124 - val_acc: 0.3916\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7733 - acc: 0.3863 - val_loss: 1.8148 - val_acc: 0.3863\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7696 - acc: 0.3873 - val_loss: 1.8204 - val_acc: 0.3812\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7674 - acc: 0.3884 - val_loss: 1.8044 - val_acc: 0.3888\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7637 - acc: 0.3892 - val_loss: 1.8197 - val_acc: 0.3821\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7610 - acc: 0.3900 - val_loss: 1.8109 - val_acc: 0.3888\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7596 - acc: 0.3920 - val_loss: 1.8206 - val_acc: 0.3853\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7579 - acc: 0.3908 - val_loss: 1.8002 - val_acc: 0.3937\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7553 - acc: 0.3919 - val_loss: 1.8299 - val_acc: 0.3781\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7533 - acc: 0.3928 - val_loss: 1.8113 - val_acc: 0.3877\n",
            "32890/32890 [==============================] - 1s 21us/sample - loss: 1.8172 - acc: 0.3749\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 7s 63us/sample - loss: 1.9400 - acc: 0.3335 - val_loss: 1.8631 - val_acc: 0.3454\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 3s 29us/sample - loss: 1.8511 - acc: 0.3618 - val_loss: 1.8258 - val_acc: 0.3604\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 3s 29us/sample - loss: 1.8314 - acc: 0.3670 - val_loss: 1.8121 - val_acc: 0.3682\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 3s 28us/sample - loss: 1.8191 - acc: 0.3706 - val_loss: 1.8078 - val_acc: 0.3665\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 3s 28us/sample - loss: 1.8091 - acc: 0.3739 - val_loss: 1.8045 - val_acc: 0.3721\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 3s 28us/sample - loss: 1.8029 - acc: 0.3753 - val_loss: 1.8073 - val_acc: 0.3687\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 3s 28us/sample - loss: 1.7966 - acc: 0.3770 - val_loss: 1.7962 - val_acc: 0.3753\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 3s 28us/sample - loss: 1.7914 - acc: 0.3788 - val_loss: 1.8075 - val_acc: 0.3679\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 3s 28us/sample - loss: 1.7880 - acc: 0.3809 - val_loss: 1.7842 - val_acc: 0.3777\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 3s 28us/sample - loss: 1.7834 - acc: 0.3826 - val_loss: 1.7849 - val_acc: 0.3792\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 3s 28us/sample - loss: 1.7800 - acc: 0.3837 - val_loss: 1.7893 - val_acc: 0.3731\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 3s 28us/sample - loss: 1.7764 - acc: 0.3850 - val_loss: 1.7876 - val_acc: 0.3756\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 3s 28us/sample - loss: 1.7739 - acc: 0.3862 - val_loss: 1.7994 - val_acc: 0.3730\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 3s 29us/sample - loss: 1.7708 - acc: 0.3857 - val_loss: 1.8003 - val_acc: 0.3718\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 3s 28us/sample - loss: 1.7679 - acc: 0.3874 - val_loss: 1.7871 - val_acc: 0.3777\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 3s 28us/sample - loss: 1.7670 - acc: 0.3874 - val_loss: 1.7994 - val_acc: 0.3734\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 3s 29us/sample - loss: 1.7638 - acc: 0.3892 - val_loss: 1.7904 - val_acc: 0.3752\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 3s 29us/sample - loss: 1.7610 - acc: 0.3898 - val_loss: 1.7845 - val_acc: 0.3794\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 3s 29us/sample - loss: 1.7598 - acc: 0.3908 - val_loss: 1.7830 - val_acc: 0.3804\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 3s 28us/sample - loss: 1.7579 - acc: 0.3908 - val_loss: 1.7879 - val_acc: 0.3815\n",
            "32889/32889 [==============================] - 1s 21us/sample - loss: 1.8153 - acc: 0.3906\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 6s 61us/sample - loss: 1.9017 - acc: 0.3476 - val_loss: 1.8003 - val_acc: 0.3961\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8294 - acc: 0.3691 - val_loss: 1.8026 - val_acc: 0.3924\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8091 - acc: 0.3743 - val_loss: 1.7938 - val_acc: 0.3951\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7956 - acc: 0.3806 - val_loss: 1.7717 - val_acc: 0.4038\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7858 - acc: 0.3822 - val_loss: 1.8034 - val_acc: 0.3899\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7772 - acc: 0.3853 - val_loss: 1.7811 - val_acc: 0.3994\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7718 - acc: 0.3885 - val_loss: 1.7833 - val_acc: 0.4000\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7644 - acc: 0.3900 - val_loss: 1.7753 - val_acc: 0.3985\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7592 - acc: 0.3919 - val_loss: 1.7711 - val_acc: 0.4000\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7538 - acc: 0.3934 - val_loss: 1.7808 - val_acc: 0.4003\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7491 - acc: 0.3951 - val_loss: 1.7739 - val_acc: 0.4043\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7445 - acc: 0.3972 - val_loss: 1.7785 - val_acc: 0.4019\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7401 - acc: 0.3975 - val_loss: 1.7741 - val_acc: 0.4023\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7364 - acc: 0.3998 - val_loss: 1.7687 - val_acc: 0.4053\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.7320 - acc: 0.4013 - val_loss: 1.7843 - val_acc: 0.3975\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.7290 - acc: 0.4010 - val_loss: 1.7904 - val_acc: 0.4006\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7249 - acc: 0.4035 - val_loss: 1.7823 - val_acc: 0.3975\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7219 - acc: 0.4027 - val_loss: 1.7935 - val_acc: 0.3952\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7182 - acc: 0.4053 - val_loss: 1.7765 - val_acc: 0.4013\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7146 - acc: 0.4061 - val_loss: 1.7783 - val_acc: 0.4022\n",
            "32890/32890 [==============================] - 1s 21us/sample - loss: 1.8482 - acc: 0.3636\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 7s 63us/sample - loss: 1.8827 - acc: 0.3570 - val_loss: 1.8231 - val_acc: 0.3788\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8145 - acc: 0.3760 - val_loss: 1.8087 - val_acc: 0.3846\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7942 - acc: 0.3838 - val_loss: 1.7916 - val_acc: 0.3874\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7810 - acc: 0.3876 - val_loss: 1.7914 - val_acc: 0.3932\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7710 - acc: 0.3912 - val_loss: 1.7934 - val_acc: 0.3935\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7637 - acc: 0.3932 - val_loss: 1.7919 - val_acc: 0.3889\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7570 - acc: 0.3950 - val_loss: 1.7798 - val_acc: 0.3973\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7515 - acc: 0.3970 - val_loss: 1.7982 - val_acc: 0.3881\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.7456 - acc: 0.3996 - val_loss: 1.7967 - val_acc: 0.3893\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.7406 - acc: 0.4011 - val_loss: 1.7811 - val_acc: 0.3975\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.7356 - acc: 0.4014 - val_loss: 1.7749 - val_acc: 0.4016\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7305 - acc: 0.4034 - val_loss: 1.7861 - val_acc: 0.3947\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7265 - acc: 0.4059 - val_loss: 1.7961 - val_acc: 0.3956\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.7228 - acc: 0.4052 - val_loss: 1.7730 - val_acc: 0.3989\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7188 - acc: 0.4078 - val_loss: 1.7777 - val_acc: 0.3980\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7162 - acc: 0.4078 - val_loss: 1.7964 - val_acc: 0.3923\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7113 - acc: 0.4092 - val_loss: 1.7867 - val_acc: 0.4008\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7085 - acc: 0.4108 - val_loss: 1.8093 - val_acc: 0.3889\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7050 - acc: 0.4116 - val_loss: 1.7953 - val_acc: 0.3965\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7019 - acc: 0.4132 - val_loss: 1.7903 - val_acc: 0.3938\n",
            "32890/32890 [==============================] - 1s 21us/sample - loss: 1.8751 - acc: 0.3470\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 6s 59us/sample - loss: 1.9077 - acc: 0.3432 - val_loss: 1.8450 - val_acc: 0.3753\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8397 - acc: 0.3634 - val_loss: 1.8249 - val_acc: 0.3793\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8196 - acc: 0.3695 - val_loss: 1.8029 - val_acc: 0.3889\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8061 - acc: 0.3741 - val_loss: 1.7958 - val_acc: 0.3926\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7960 - acc: 0.3768 - val_loss: 1.8091 - val_acc: 0.3834\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7880 - acc: 0.3800 - val_loss: 1.8260 - val_acc: 0.3762\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7808 - acc: 0.3822 - val_loss: 1.7911 - val_acc: 0.3885\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7743 - acc: 0.3850 - val_loss: 1.8039 - val_acc: 0.3883\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7695 - acc: 0.3866 - val_loss: 1.7965 - val_acc: 0.3851\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7642 - acc: 0.3877 - val_loss: 1.8186 - val_acc: 0.3792\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7594 - acc: 0.3896 - val_loss: 1.8092 - val_acc: 0.3824\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7553 - acc: 0.3915 - val_loss: 1.8003 - val_acc: 0.3859\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7509 - acc: 0.3936 - val_loss: 1.8020 - val_acc: 0.3866\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.7471 - acc: 0.3931 - val_loss: 1.8042 - val_acc: 0.3841\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7426 - acc: 0.3951 - val_loss: 1.8162 - val_acc: 0.3778\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7386 - acc: 0.3973 - val_loss: 1.7972 - val_acc: 0.3877\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7355 - acc: 0.3971 - val_loss: 1.8043 - val_acc: 0.3844\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7324 - acc: 0.3988 - val_loss: 1.8122 - val_acc: 0.3856\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7292 - acc: 0.3993 - val_loss: 1.8203 - val_acc: 0.3847\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7260 - acc: 0.3996 - val_loss: 1.8000 - val_acc: 0.3920\n",
            "32890/32890 [==============================] - 1s 20us/sample - loss: 1.7816 - acc: 0.3907\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 6s 60us/sample - loss: 1.8985 - acc: 0.3469 - val_loss: 1.8617 - val_acc: 0.3674\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8302 - acc: 0.3687 - val_loss: 1.8355 - val_acc: 0.3803\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8109 - acc: 0.3756 - val_loss: 1.8278 - val_acc: 0.3834\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7977 - acc: 0.3792 - val_loss: 1.8295 - val_acc: 0.3813\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7887 - acc: 0.3810 - val_loss: 1.8199 - val_acc: 0.3827\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.7806 - acc: 0.3847 - val_loss: 1.8065 - val_acc: 0.3897\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7735 - acc: 0.3876 - val_loss: 1.8073 - val_acc: 0.3897\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7677 - acc: 0.3890 - val_loss: 1.8145 - val_acc: 0.3838\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7616 - acc: 0.3903 - val_loss: 1.8204 - val_acc: 0.3841\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7560 - acc: 0.3933 - val_loss: 1.8111 - val_acc: 0.3915\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7504 - acc: 0.3944 - val_loss: 1.8182 - val_acc: 0.3877\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7461 - acc: 0.3957 - val_loss: 1.8363 - val_acc: 0.3788\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7425 - acc: 0.3981 - val_loss: 1.8361 - val_acc: 0.3829\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7387 - acc: 0.3974 - val_loss: 1.8185 - val_acc: 0.3839\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7341 - acc: 0.4005 - val_loss: 1.8287 - val_acc: 0.3781\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7310 - acc: 0.4006 - val_loss: 1.8223 - val_acc: 0.3836\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7263 - acc: 0.4023 - val_loss: 1.8151 - val_acc: 0.3872\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7224 - acc: 0.4035 - val_loss: 1.8271 - val_acc: 0.3802\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7189 - acc: 0.4059 - val_loss: 1.8241 - val_acc: 0.3886\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.7163 - acc: 0.4053 - val_loss: 1.8430 - val_acc: 0.3755\n",
            "32890/32890 [==============================] - 1s 22us/sample - loss: 1.8263 - acc: 0.3711\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 6s 60us/sample - loss: 1.9043 - acc: 0.3452 - val_loss: 1.8336 - val_acc: 0.3526\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 3s 28us/sample - loss: 1.8367 - acc: 0.3642 - val_loss: 1.8189 - val_acc: 0.3569\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 3s 29us/sample - loss: 1.8179 - acc: 0.3706 - val_loss: 1.8209 - val_acc: 0.3588\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 3s 29us/sample - loss: 1.8052 - acc: 0.3737 - val_loss: 1.8196 - val_acc: 0.3548\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 3s 28us/sample - loss: 1.7954 - acc: 0.3788 - val_loss: 1.7961 - val_acc: 0.3671\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.7879 - acc: 0.3803 - val_loss: 1.7911 - val_acc: 0.3733\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 3s 28us/sample - loss: 1.7807 - acc: 0.3829 - val_loss: 1.8036 - val_acc: 0.3620\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 3s 28us/sample - loss: 1.7748 - acc: 0.3842 - val_loss: 1.8061 - val_acc: 0.3653\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 3s 28us/sample - loss: 1.7686 - acc: 0.3866 - val_loss: 1.7997 - val_acc: 0.3673\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 3s 28us/sample - loss: 1.7639 - acc: 0.3871 - val_loss: 1.7810 - val_acc: 0.3804\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 3s 28us/sample - loss: 1.7588 - acc: 0.3905 - val_loss: 1.8014 - val_acc: 0.3734\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 3s 28us/sample - loss: 1.7536 - acc: 0.3921 - val_loss: 1.7971 - val_acc: 0.3722\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 3s 29us/sample - loss: 1.7496 - acc: 0.3939 - val_loss: 1.7812 - val_acc: 0.3823\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 3s 28us/sample - loss: 1.7453 - acc: 0.3941 - val_loss: 1.7846 - val_acc: 0.3788\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.7407 - acc: 0.3945 - val_loss: 1.7806 - val_acc: 0.3780\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 3s 28us/sample - loss: 1.7364 - acc: 0.3983 - val_loss: 1.7872 - val_acc: 0.3803\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 3s 29us/sample - loss: 1.7335 - acc: 0.3985 - val_loss: 1.7743 - val_acc: 0.3856\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 3s 28us/sample - loss: 1.7296 - acc: 0.3989 - val_loss: 1.8006 - val_acc: 0.3725\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 3s 28us/sample - loss: 1.7263 - acc: 0.4006 - val_loss: 1.8020 - val_acc: 0.3684\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.7227 - acc: 0.4012 - val_loss: 1.7955 - val_acc: 0.3690\n",
            "32889/32889 [==============================] - 1s 21us/sample - loss: 1.8405 - acc: 0.3829\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 6s 61us/sample - loss: 1.8820 - acc: 0.3511 - val_loss: 1.8199 - val_acc: 0.3832\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8189 - acc: 0.3720 - val_loss: 1.7951 - val_acc: 0.3937\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7974 - acc: 0.3777 - val_loss: 1.7796 - val_acc: 0.3959\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7829 - acc: 0.3830 - val_loss: 1.7842 - val_acc: 0.3975\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7720 - acc: 0.3870 - val_loss: 1.7648 - val_acc: 0.4065\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.7629 - acc: 0.3900 - val_loss: 1.7722 - val_acc: 0.3991\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7543 - acc: 0.3926 - val_loss: 1.7846 - val_acc: 0.3963\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7459 - acc: 0.3965 - val_loss: 1.7698 - val_acc: 0.4045\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7391 - acc: 0.3974 - val_loss: 1.7752 - val_acc: 0.4050\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7322 - acc: 0.3996 - val_loss: 1.7950 - val_acc: 0.3930\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7253 - acc: 0.4015 - val_loss: 1.7895 - val_acc: 0.3954\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.7182 - acc: 0.4035 - val_loss: 1.7788 - val_acc: 0.3971\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7107 - acc: 0.4057 - val_loss: 1.7799 - val_acc: 0.4024\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7052 - acc: 0.4083 - val_loss: 1.7830 - val_acc: 0.3981\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.6979 - acc: 0.4102 - val_loss: 1.7939 - val_acc: 0.4003\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.6918 - acc: 0.4128 - val_loss: 1.7924 - val_acc: 0.4003\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.6843 - acc: 0.4143 - val_loss: 1.7927 - val_acc: 0.3955\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.6789 - acc: 0.4150 - val_loss: 1.8216 - val_acc: 0.3866\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.6722 - acc: 0.4199 - val_loss: 1.8119 - val_acc: 0.3931\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.6645 - acc: 0.4219 - val_loss: 1.8199 - val_acc: 0.3897\n",
            "32890/32890 [==============================] - 1s 22us/sample - loss: 1.8557 - acc: 0.3689\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 6s 60us/sample - loss: 1.8638 - acc: 0.3612 - val_loss: 1.7999 - val_acc: 0.3859\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8025 - acc: 0.3813 - val_loss: 1.7929 - val_acc: 0.3965\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7829 - acc: 0.3867 - val_loss: 1.7928 - val_acc: 0.3947\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7687 - acc: 0.3913 - val_loss: 1.7748 - val_acc: 0.3995\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7586 - acc: 0.3943 - val_loss: 1.7802 - val_acc: 0.4002\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7497 - acc: 0.3969 - val_loss: 1.7905 - val_acc: 0.3944\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7408 - acc: 0.3995 - val_loss: 1.8074 - val_acc: 0.3867\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7331 - acc: 0.4015 - val_loss: 1.7868 - val_acc: 0.3985\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7259 - acc: 0.4047 - val_loss: 1.7938 - val_acc: 0.3917\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.7186 - acc: 0.4066 - val_loss: 1.8008 - val_acc: 0.3945\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.7110 - acc: 0.4093 - val_loss: 1.7870 - val_acc: 0.3923\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7047 - acc: 0.4108 - val_loss: 1.7905 - val_acc: 0.3954\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.6972 - acc: 0.4145 - val_loss: 1.7858 - val_acc: 0.3977\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.6907 - acc: 0.4166 - val_loss: 1.7925 - val_acc: 0.3928\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.6848 - acc: 0.4182 - val_loss: 1.8134 - val_acc: 0.3916\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.6773 - acc: 0.4198 - val_loss: 1.7965 - val_acc: 0.3946\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.6709 - acc: 0.4229 - val_loss: 1.8045 - val_acc: 0.3891\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.6644 - acc: 0.4241 - val_loss: 1.8103 - val_acc: 0.3915\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.6572 - acc: 0.4257 - val_loss: 1.8078 - val_acc: 0.3949\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.6509 - acc: 0.4274 - val_loss: 1.8011 - val_acc: 0.3984\n",
            "32890/32890 [==============================] - 1s 22us/sample - loss: 1.8915 - acc: 0.3465\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 6s 61us/sample - loss: 1.8934 - acc: 0.3449 - val_loss: 1.8124 - val_acc: 0.3876\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8309 - acc: 0.3654 - val_loss: 1.8062 - val_acc: 0.3882\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8090 - acc: 0.3732 - val_loss: 1.8139 - val_acc: 0.3805\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7953 - acc: 0.3776 - val_loss: 1.7897 - val_acc: 0.3899\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7845 - acc: 0.3792 - val_loss: 1.8077 - val_acc: 0.3797\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7748 - acc: 0.3840 - val_loss: 1.8007 - val_acc: 0.3857\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7667 - acc: 0.3867 - val_loss: 1.7910 - val_acc: 0.3923\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7587 - acc: 0.3898 - val_loss: 1.7901 - val_acc: 0.3931\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7514 - acc: 0.3913 - val_loss: 1.7953 - val_acc: 0.3902\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7432 - acc: 0.3950 - val_loss: 1.7950 - val_acc: 0.3942\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7363 - acc: 0.3962 - val_loss: 1.7957 - val_acc: 0.3926\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.7295 - acc: 0.3976 - val_loss: 1.8150 - val_acc: 0.3855\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.7230 - acc: 0.4011 - val_loss: 1.8026 - val_acc: 0.3912\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.7153 - acc: 0.4023 - val_loss: 1.8134 - val_acc: 0.3834\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7088 - acc: 0.4046 - val_loss: 1.8080 - val_acc: 0.3820\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7020 - acc: 0.4087 - val_loss: 1.8516 - val_acc: 0.3691\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.6946 - acc: 0.4089 - val_loss: 1.8125 - val_acc: 0.3929\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.6887 - acc: 0.4112 - val_loss: 1.8072 - val_acc: 0.3881\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.6822 - acc: 0.4137 - val_loss: 1.8431 - val_acc: 0.3803\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.6749 - acc: 0.4167 - val_loss: 1.8327 - val_acc: 0.3833\n",
            "32890/32890 [==============================] - 1s 22us/sample - loss: 1.8016 - acc: 0.3809\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 7s 62us/sample - loss: 1.8813 - acc: 0.3525 - val_loss: 1.8445 - val_acc: 0.3716\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8218 - acc: 0.3718 - val_loss: 1.8283 - val_acc: 0.3826\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8017 - acc: 0.3770 - val_loss: 1.8210 - val_acc: 0.3886\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7875 - acc: 0.3820 - val_loss: 1.8383 - val_acc: 0.3840\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7750 - acc: 0.3861 - val_loss: 1.8261 - val_acc: 0.3791\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7662 - acc: 0.3883 - val_loss: 1.8156 - val_acc: 0.3853\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7569 - acc: 0.3911 - val_loss: 1.8423 - val_acc: 0.3837\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.7488 - acc: 0.3940 - val_loss: 1.8228 - val_acc: 0.3856\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.7419 - acc: 0.3967 - val_loss: 1.8107 - val_acc: 0.3847\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7344 - acc: 0.3982 - val_loss: 1.8104 - val_acc: 0.3895\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.7267 - acc: 0.4020 - val_loss: 1.8009 - val_acc: 0.4007\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.7199 - acc: 0.4033 - val_loss: 1.8438 - val_acc: 0.3855\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7124 - acc: 0.4060 - val_loss: 1.8168 - val_acc: 0.3897\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7059 - acc: 0.4075 - val_loss: 1.8019 - val_acc: 0.3975\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.6996 - acc: 0.4102 - val_loss: 1.8331 - val_acc: 0.3860\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.6919 - acc: 0.4120 - val_loss: 1.8365 - val_acc: 0.3810\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.6857 - acc: 0.4145 - val_loss: 1.8298 - val_acc: 0.3905\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.6788 - acc: 0.4160 - val_loss: 1.8357 - val_acc: 0.3791\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.6706 - acc: 0.4191 - val_loss: 1.8384 - val_acc: 0.3878\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.6652 - acc: 0.4207 - val_loss: 1.8477 - val_acc: 0.3838\n",
            "32890/32890 [==============================] - 1s 21us/sample - loss: 1.8416 - acc: 0.3677\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 7s 63us/sample - loss: 1.8885 - acc: 0.3487 - val_loss: 1.8629 - val_acc: 0.3362\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 3s 29us/sample - loss: 1.8275 - acc: 0.3674 - val_loss: 1.8224 - val_acc: 0.3589\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 3s 28us/sample - loss: 1.8053 - acc: 0.3740 - val_loss: 1.7842 - val_acc: 0.3795\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 3s 29us/sample - loss: 1.7921 - acc: 0.3778 - val_loss: 1.7913 - val_acc: 0.3739\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 3s 28us/sample - loss: 1.7797 - acc: 0.3818 - val_loss: 1.7882 - val_acc: 0.3780\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 3s 30us/sample - loss: 1.7700 - acc: 0.3852 - val_loss: 1.7788 - val_acc: 0.3801\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 3s 30us/sample - loss: 1.7614 - acc: 0.3875 - val_loss: 1.8037 - val_acc: 0.3700\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 3s 29us/sample - loss: 1.7534 - acc: 0.3911 - val_loss: 1.7824 - val_acc: 0.3759\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 3s 29us/sample - loss: 1.7459 - acc: 0.3933 - val_loss: 1.7999 - val_acc: 0.3691\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 3s 28us/sample - loss: 1.7384 - acc: 0.3939 - val_loss: 1.8043 - val_acc: 0.3706\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 3s 29us/sample - loss: 1.7312 - acc: 0.3974 - val_loss: 1.7897 - val_acc: 0.3723\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 3s 29us/sample - loss: 1.7243 - acc: 0.4007 - val_loss: 1.8061 - val_acc: 0.3696\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 3s 28us/sample - loss: 1.7175 - acc: 0.4021 - val_loss: 1.8232 - val_acc: 0.3633\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 3s 28us/sample - loss: 1.7102 - acc: 0.4032 - val_loss: 1.8025 - val_acc: 0.3723\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 3s 29us/sample - loss: 1.7034 - acc: 0.4065 - val_loss: 1.8177 - val_acc: 0.3645\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 3s 29us/sample - loss: 1.6971 - acc: 0.4072 - val_loss: 1.8117 - val_acc: 0.3717\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 3s 29us/sample - loss: 1.6899 - acc: 0.4096 - val_loss: 1.8243 - val_acc: 0.3649\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 3s 28us/sample - loss: 1.6838 - acc: 0.4130 - val_loss: 1.8171 - val_acc: 0.3682\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 3s 28us/sample - loss: 1.6763 - acc: 0.4150 - val_loss: 1.8148 - val_acc: 0.3643\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 3s 29us/sample - loss: 1.6697 - acc: 0.4164 - val_loss: 1.8233 - val_acc: 0.3669\n",
            "32889/32889 [==============================] - 1s 22us/sample - loss: 1.8568 - acc: 0.3806\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 7s 63us/sample - loss: 1.8714 - acc: 0.3543 - val_loss: 1.8043 - val_acc: 0.3903\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.8121 - acc: 0.3745 - val_loss: 1.7813 - val_acc: 0.3994\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.7907 - acc: 0.3815 - val_loss: 1.7833 - val_acc: 0.3988\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7766 - acc: 0.3856 - val_loss: 1.7709 - val_acc: 0.4045\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7635 - acc: 0.3893 - val_loss: 1.7934 - val_acc: 0.3959\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7543 - acc: 0.3926 - val_loss: 1.7735 - val_acc: 0.4034\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7422 - acc: 0.3957 - val_loss: 1.7945 - val_acc: 0.3952\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7317 - acc: 0.3995 - val_loss: 1.7850 - val_acc: 0.3959\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7214 - acc: 0.4017 - val_loss: 1.7886 - val_acc: 0.4041\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7109 - acc: 0.4047 - val_loss: 1.7869 - val_acc: 0.4019\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.6999 - acc: 0.4080 - val_loss: 1.7874 - val_acc: 0.4021\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.6881 - acc: 0.4132 - val_loss: 1.8115 - val_acc: 0.3930\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.6766 - acc: 0.4168 - val_loss: 1.8066 - val_acc: 0.3928\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.6635 - acc: 0.4199 - val_loss: 1.8309 - val_acc: 0.3900\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.6509 - acc: 0.4237 - val_loss: 1.8226 - val_acc: 0.3903\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.6387 - acc: 0.4273 - val_loss: 1.8311 - val_acc: 0.3930\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.6253 - acc: 0.4309 - val_loss: 1.8484 - val_acc: 0.3849\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.6101 - acc: 0.4367 - val_loss: 1.8574 - val_acc: 0.3792\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.5962 - acc: 0.4410 - val_loss: 1.8497 - val_acc: 0.3893\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.5803 - acc: 0.4472 - val_loss: 1.8550 - val_acc: 0.3851\n",
            "32890/32890 [==============================] - 1s 19us/sample - loss: 1.9240 - acc: 0.3514\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 7s 62us/sample - loss: 1.8545 - acc: 0.3633 - val_loss: 1.7948 - val_acc: 0.3894\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.7983 - acc: 0.3806 - val_loss: 1.7937 - val_acc: 0.3964\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.7767 - acc: 0.3875 - val_loss: 1.8099 - val_acc: 0.3890\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7615 - acc: 0.3931 - val_loss: 1.7849 - val_acc: 0.3940\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7498 - acc: 0.3975 - val_loss: 1.7972 - val_acc: 0.3901\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.7381 - acc: 0.4005 - val_loss: 1.7960 - val_acc: 0.3819\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7279 - acc: 0.4035 - val_loss: 1.7889 - val_acc: 0.3926\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.7165 - acc: 0.4075 - val_loss: 1.8020 - val_acc: 0.3847\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.7065 - acc: 0.4102 - val_loss: 1.7846 - val_acc: 0.3924\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.6963 - acc: 0.4130 - val_loss: 1.7886 - val_acc: 0.3977\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.6867 - acc: 0.4153 - val_loss: 1.7854 - val_acc: 0.3976\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.6740 - acc: 0.4197 - val_loss: 1.7964 - val_acc: 0.3991\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.6618 - acc: 0.4228 - val_loss: 1.7976 - val_acc: 0.3994\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.6497 - acc: 0.4274 - val_loss: 1.8119 - val_acc: 0.3933\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.6377 - acc: 0.4293 - val_loss: 1.8336 - val_acc: 0.3844\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.6247 - acc: 0.4350 - val_loss: 1.8068 - val_acc: 0.4008\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.6103 - acc: 0.4377 - val_loss: 1.8373 - val_acc: 0.3915\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.5977 - acc: 0.4431 - val_loss: 1.8428 - val_acc: 0.3910\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.5824 - acc: 0.4479 - val_loss: 1.8719 - val_acc: 0.3798\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.5686 - acc: 0.4516 - val_loss: 1.8791 - val_acc: 0.3804\n",
            "32890/32890 [==============================] - 1s 21us/sample - loss: 1.9653 - acc: 0.3366\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 7s 62us/sample - loss: 1.8814 - acc: 0.3491 - val_loss: 1.8336 - val_acc: 0.3766\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.8235 - acc: 0.3681 - val_loss: 1.8259 - val_acc: 0.3808\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8019 - acc: 0.3751 - val_loss: 1.8110 - val_acc: 0.3833\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7872 - acc: 0.3795 - val_loss: 1.8092 - val_acc: 0.3795\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.7728 - acc: 0.3854 - val_loss: 1.8099 - val_acc: 0.3857\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7629 - acc: 0.3870 - val_loss: 1.8212 - val_acc: 0.3776\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.7518 - acc: 0.3908 - val_loss: 1.8136 - val_acc: 0.3799\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.7424 - acc: 0.3951 - val_loss: 1.8105 - val_acc: 0.3849\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7315 - acc: 0.3984 - val_loss: 1.7976 - val_acc: 0.3875\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.7218 - acc: 0.3995 - val_loss: 1.8101 - val_acc: 0.3878\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.7100 - acc: 0.4038 - val_loss: 1.8147 - val_acc: 0.3897\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.6988 - acc: 0.4082 - val_loss: 1.8106 - val_acc: 0.3875\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.6859 - acc: 0.4129 - val_loss: 1.8437 - val_acc: 0.3726\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.6739 - acc: 0.4163 - val_loss: 1.8315 - val_acc: 0.3824\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.6617 - acc: 0.4196 - val_loss: 1.8569 - val_acc: 0.3733\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.6480 - acc: 0.4222 - val_loss: 1.8372 - val_acc: 0.3844\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.6341 - acc: 0.4279 - val_loss: 1.8808 - val_acc: 0.3701\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.6195 - acc: 0.4309 - val_loss: 1.8662 - val_acc: 0.3791\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.6051 - acc: 0.4367 - val_loss: 1.8759 - val_acc: 0.3718\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.5912 - acc: 0.4410 - val_loss: 1.9068 - val_acc: 0.3670\n",
            "32890/32890 [==============================] - 1s 22us/sample - loss: 1.8623 - acc: 0.3687\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 7s 64us/sample - loss: 1.8711 - acc: 0.3569 - val_loss: 1.8357 - val_acc: 0.3754\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8155 - acc: 0.3725 - val_loss: 1.8228 - val_acc: 0.3910\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.7952 - acc: 0.3802 - val_loss: 1.8488 - val_acc: 0.3695\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.7792 - acc: 0.3840 - val_loss: 1.8201 - val_acc: 0.3840\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.7666 - acc: 0.3893 - val_loss: 1.8211 - val_acc: 0.3829\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.7533 - acc: 0.3908 - val_loss: 1.8282 - val_acc: 0.3817\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.7437 - acc: 0.3964 - val_loss: 1.8127 - val_acc: 0.3923\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.7324 - acc: 0.3986 - val_loss: 1.8175 - val_acc: 0.3860\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.7215 - acc: 0.4023 - val_loss: 1.8413 - val_acc: 0.3816\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.7099 - acc: 0.4053 - val_loss: 1.8487 - val_acc: 0.3767\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.6983 - acc: 0.4093 - val_loss: 1.8396 - val_acc: 0.3834\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.6866 - acc: 0.4130 - val_loss: 1.8441 - val_acc: 0.3806\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.6743 - acc: 0.4170 - val_loss: 1.8442 - val_acc: 0.3828\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.6622 - acc: 0.4201 - val_loss: 1.8589 - val_acc: 0.3809\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.6492 - acc: 0.4235 - val_loss: 1.8698 - val_acc: 0.3784\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.6355 - acc: 0.4305 - val_loss: 1.8720 - val_acc: 0.3754\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.6204 - acc: 0.4344 - val_loss: 1.8673 - val_acc: 0.3778\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.6064 - acc: 0.4372 - val_loss: 1.9024 - val_acc: 0.3774\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.5915 - acc: 0.4445 - val_loss: 1.8974 - val_acc: 0.3767\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.5765 - acc: 0.4476 - val_loss: 1.9246 - val_acc: 0.3618\n",
            "32890/32890 [==============================] - 1s 22us/sample - loss: 1.8974 - acc: 0.3553\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 7s 63us/sample - loss: 1.8786 - acc: 0.3509 - val_loss: 1.8275 - val_acc: 0.3581\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 3s 30us/sample - loss: 1.8190 - acc: 0.3696 - val_loss: 1.8141 - val_acc: 0.3694\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 3s 31us/sample - loss: 1.7992 - acc: 0.3762 - val_loss: 1.7999 - val_acc: 0.3714\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 3s 30us/sample - loss: 1.7831 - acc: 0.3810 - val_loss: 1.7909 - val_acc: 0.3736\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 3s 31us/sample - loss: 1.7698 - acc: 0.3873 - val_loss: 1.7944 - val_acc: 0.3749\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 3s 32us/sample - loss: 1.7589 - acc: 0.3900 - val_loss: 1.7986 - val_acc: 0.3758\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 3s 29us/sample - loss: 1.7481 - acc: 0.3920 - val_loss: 1.7856 - val_acc: 0.3749\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 3s 30us/sample - loss: 1.7371 - acc: 0.3951 - val_loss: 1.7755 - val_acc: 0.3829\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 3s 30us/sample - loss: 1.7258 - acc: 0.3991 - val_loss: 1.7759 - val_acc: 0.3887\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 3s 30us/sample - loss: 1.7144 - acc: 0.4035 - val_loss: 1.7933 - val_acc: 0.3754\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 3s 30us/sample - loss: 1.7034 - acc: 0.4056 - val_loss: 1.8086 - val_acc: 0.3703\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 3s 30us/sample - loss: 1.6921 - acc: 0.4099 - val_loss: 1.7985 - val_acc: 0.3725\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 3s 29us/sample - loss: 1.6800 - acc: 0.4132 - val_loss: 1.8125 - val_acc: 0.3719\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 3s 29us/sample - loss: 1.6682 - acc: 0.4175 - val_loss: 1.8257 - val_acc: 0.3700\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 3s 30us/sample - loss: 1.6557 - acc: 0.4204 - val_loss: 1.8167 - val_acc: 0.3718\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 3s 29us/sample - loss: 1.6415 - acc: 0.4252 - val_loss: 1.8198 - val_acc: 0.3753\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 3s 29us/sample - loss: 1.6281 - acc: 0.4299 - val_loss: 1.8475 - val_acc: 0.3725\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 3s 31us/sample - loss: 1.6136 - acc: 0.4346 - val_loss: 1.8342 - val_acc: 0.3726\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 3s 30us/sample - loss: 1.5981 - acc: 0.4384 - val_loss: 1.8659 - val_acc: 0.3593\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 3s 30us/sample - loss: 1.5838 - acc: 0.4431 - val_loss: 1.8756 - val_acc: 0.3601\n",
            "32889/32889 [==============================] - 1s 21us/sample - loss: 1.9172 - acc: 0.3739\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 7s 68us/sample - loss: 1.8672 - acc: 0.3559 - val_loss: 1.8116 - val_acc: 0.3886\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.8108 - acc: 0.3751 - val_loss: 1.8055 - val_acc: 0.3867\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.7888 - acc: 0.3815 - val_loss: 1.8011 - val_acc: 0.3893\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.7730 - acc: 0.3864 - val_loss: 1.7794 - val_acc: 0.4016\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.7582 - acc: 0.3912 - val_loss: 1.7992 - val_acc: 0.3945\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.7464 - acc: 0.3935 - val_loss: 1.7745 - val_acc: 0.4016\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.7311 - acc: 0.3982 - val_loss: 1.7898 - val_acc: 0.4021\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.7185 - acc: 0.4014 - val_loss: 1.7954 - val_acc: 0.3957\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.7030 - acc: 0.4072 - val_loss: 1.7801 - val_acc: 0.4057\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.6874 - acc: 0.4119 - val_loss: 1.7883 - val_acc: 0.4009\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.6696 - acc: 0.4173 - val_loss: 1.8148 - val_acc: 0.3884\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.6499 - acc: 0.4233 - val_loss: 1.8011 - val_acc: 0.4064\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.6308 - acc: 0.4294 - val_loss: 1.8237 - val_acc: 0.3944\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.6072 - acc: 0.4375 - val_loss: 1.8633 - val_acc: 0.3827\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.5856 - acc: 0.4438 - val_loss: 1.8577 - val_acc: 0.3892\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.5589 - acc: 0.4518 - val_loss: 1.8765 - val_acc: 0.3880\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 4s 33us/sample - loss: 1.5327 - acc: 0.4613 - val_loss: 1.9046 - val_acc: 0.3760\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.5042 - acc: 0.4693 - val_loss: 1.9366 - val_acc: 0.3794\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.4725 - acc: 0.4803 - val_loss: 1.9924 - val_acc: 0.3636\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.4416 - acc: 0.4914 - val_loss: 2.0122 - val_acc: 0.3644\n",
            "32890/32890 [==============================] - 1s 23us/sample - loss: 2.0514 - acc: 0.3419\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 7s 67us/sample - loss: 1.8498 - acc: 0.3652 - val_loss: 1.7916 - val_acc: 0.3976\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.7956 - acc: 0.3828 - val_loss: 1.7837 - val_acc: 0.3960\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 4s 33us/sample - loss: 1.7738 - acc: 0.3902 - val_loss: 1.7874 - val_acc: 0.3915\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.7580 - acc: 0.3947 - val_loss: 1.7895 - val_acc: 0.3939\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.7438 - acc: 0.3987 - val_loss: 1.7861 - val_acc: 0.4012\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.7314 - acc: 0.4030 - val_loss: 1.7931 - val_acc: 0.3977\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.7180 - acc: 0.4059 - val_loss: 1.7847 - val_acc: 0.4006\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.7045 - acc: 0.4093 - val_loss: 1.8130 - val_acc: 0.3886\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.6903 - acc: 0.4141 - val_loss: 1.7934 - val_acc: 0.3960\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.6727 - acc: 0.4198 - val_loss: 1.7997 - val_acc: 0.3950\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.6566 - acc: 0.4240 - val_loss: 1.8552 - val_acc: 0.3797\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.6364 - acc: 0.4313 - val_loss: 1.8170 - val_acc: 0.3969\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.6168 - acc: 0.4361 - val_loss: 1.8333 - val_acc: 0.3894\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.5953 - acc: 0.4422 - val_loss: 1.8642 - val_acc: 0.3864\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.5704 - acc: 0.4507 - val_loss: 1.8652 - val_acc: 0.3833\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.5444 - acc: 0.4578 - val_loss: 1.9108 - val_acc: 0.3886\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.5188 - acc: 0.4671 - val_loss: 1.9033 - val_acc: 0.3856\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.4894 - acc: 0.4752 - val_loss: 1.9532 - val_acc: 0.3700\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.4592 - acc: 0.4868 - val_loss: 1.9660 - val_acc: 0.3724\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.4286 - acc: 0.4958 - val_loss: 1.9969 - val_acc: 0.3713\n",
            "32890/32890 [==============================] - 1s 22us/sample - loss: 2.0722 - acc: 0.3313\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 7s 67us/sample - loss: 1.8790 - acc: 0.3484 - val_loss: 1.8218 - val_acc: 0.3821\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.8208 - acc: 0.3681 - val_loss: 1.8153 - val_acc: 0.3836\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.7990 - acc: 0.3749 - val_loss: 1.8155 - val_acc: 0.3821\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.7831 - acc: 0.3817 - val_loss: 1.8080 - val_acc: 0.3859\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.7687 - acc: 0.3848 - val_loss: 1.7966 - val_acc: 0.3897\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.7559 - acc: 0.3892 - val_loss: 1.8207 - val_acc: 0.3810\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.7418 - acc: 0.3941 - val_loss: 1.8162 - val_acc: 0.3772\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.7285 - acc: 0.3972 - val_loss: 1.8149 - val_acc: 0.3851\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.7143 - acc: 0.4025 - val_loss: 1.8167 - val_acc: 0.3883\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.6969 - acc: 0.4077 - val_loss: 1.8579 - val_acc: 0.3736\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.6797 - acc: 0.4140 - val_loss: 1.8329 - val_acc: 0.3861\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.6606 - acc: 0.4180 - val_loss: 1.8591 - val_acc: 0.3725\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.6401 - acc: 0.4242 - val_loss: 1.8705 - val_acc: 0.3806\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.6167 - acc: 0.4312 - val_loss: 1.8771 - val_acc: 0.3671\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.5927 - acc: 0.4393 - val_loss: 1.8935 - val_acc: 0.3707\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.5672 - acc: 0.4486 - val_loss: 1.9415 - val_acc: 0.3589\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.5398 - acc: 0.4571 - val_loss: 1.9700 - val_acc: 0.3509\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.5097 - acc: 0.4674 - val_loss: 1.9604 - val_acc: 0.3676\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.4799 - acc: 0.4758 - val_loss: 2.0288 - val_acc: 0.3554\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.4484 - acc: 0.4886 - val_loss: 2.0376 - val_acc: 0.3596\n",
            "32890/32890 [==============================] - 1s 23us/sample - loss: 1.9805 - acc: 0.3594\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 7s 69us/sample - loss: 1.8673 - acc: 0.3568 - val_loss: 1.8349 - val_acc: 0.3758\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.8124 - acc: 0.3752 - val_loss: 1.8389 - val_acc: 0.3807\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.7912 - acc: 0.3804 - val_loss: 1.8271 - val_acc: 0.3848\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.7732 - acc: 0.3869 - val_loss: 1.8221 - val_acc: 0.3913\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.7602 - acc: 0.3901 - val_loss: 1.8133 - val_acc: 0.3878\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.7461 - acc: 0.3937 - val_loss: 1.8373 - val_acc: 0.3801\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.7316 - acc: 0.3994 - val_loss: 1.8322 - val_acc: 0.3857\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.7162 - acc: 0.4042 - val_loss: 1.8298 - val_acc: 0.3877\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.7006 - acc: 0.4084 - val_loss: 1.8226 - val_acc: 0.3934\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.6837 - acc: 0.4122 - val_loss: 1.8429 - val_acc: 0.3861\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.6662 - acc: 0.4189 - val_loss: 1.9024 - val_acc: 0.3594\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 4s 33us/sample - loss: 1.6455 - acc: 0.4249 - val_loss: 1.8733 - val_acc: 0.3769\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.6236 - acc: 0.4304 - val_loss: 1.8730 - val_acc: 0.3766\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.5999 - acc: 0.4402 - val_loss: 1.8823 - val_acc: 0.3772\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.5737 - acc: 0.4495 - val_loss: 1.9018 - val_acc: 0.3764\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.5485 - acc: 0.4548 - val_loss: 1.9160 - val_acc: 0.3827\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 4s 33us/sample - loss: 1.5195 - acc: 0.4656 - val_loss: 1.9859 - val_acc: 0.3605\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.4889 - acc: 0.4751 - val_loss: 1.9888 - val_acc: 0.3607\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.4566 - acc: 0.4864 - val_loss: 2.0241 - val_acc: 0.3605\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.4247 - acc: 0.4967 - val_loss: 2.0480 - val_acc: 0.3650\n",
            "32890/32890 [==============================] - 1s 22us/sample - loss: 2.0182 - acc: 0.3462\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 7s 67us/sample - loss: 1.8717 - acc: 0.3532 - val_loss: 1.8258 - val_acc: 0.3576\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 3s 33us/sample - loss: 1.8179 - acc: 0.3699 - val_loss: 1.8036 - val_acc: 0.3691\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 3s 33us/sample - loss: 1.7965 - acc: 0.3770 - val_loss: 1.7978 - val_acc: 0.3724\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 3s 33us/sample - loss: 1.7795 - acc: 0.3826 - val_loss: 1.7779 - val_acc: 0.3830\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 3s 33us/sample - loss: 1.7663 - acc: 0.3857 - val_loss: 1.7729 - val_acc: 0.3869\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 4s 34us/sample - loss: 1.7516 - acc: 0.3912 - val_loss: 1.8001 - val_acc: 0.3731\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 3s 33us/sample - loss: 1.7374 - acc: 0.3959 - val_loss: 1.8002 - val_acc: 0.3691\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 3s 33us/sample - loss: 1.7225 - acc: 0.3991 - val_loss: 1.8061 - val_acc: 0.3657\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 3s 33us/sample - loss: 1.7073 - acc: 0.4042 - val_loss: 1.8058 - val_acc: 0.3748\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 4s 34us/sample - loss: 1.6917 - acc: 0.4098 - val_loss: 1.7908 - val_acc: 0.3842\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 4s 34us/sample - loss: 1.6733 - acc: 0.4142 - val_loss: 1.8243 - val_acc: 0.3691\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 4s 34us/sample - loss: 1.6543 - acc: 0.4198 - val_loss: 1.8317 - val_acc: 0.3708\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 3s 33us/sample - loss: 1.6322 - acc: 0.4258 - val_loss: 1.8535 - val_acc: 0.3640\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 3s 33us/sample - loss: 1.6103 - acc: 0.4343 - val_loss: 1.8501 - val_acc: 0.3683\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 3s 33us/sample - loss: 1.5851 - acc: 0.4422 - val_loss: 1.8736 - val_acc: 0.3640\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 3s 33us/sample - loss: 1.5584 - acc: 0.4501 - val_loss: 1.8779 - val_acc: 0.3677\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 3s 33us/sample - loss: 1.5294 - acc: 0.4594 - val_loss: 1.9094 - val_acc: 0.3593\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 3s 33us/sample - loss: 1.5000 - acc: 0.4694 - val_loss: 1.9171 - val_acc: 0.3582\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 3s 33us/sample - loss: 1.4702 - acc: 0.4802 - val_loss: 1.9429 - val_acc: 0.3581\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 3s 33us/sample - loss: 1.4370 - acc: 0.4891 - val_loss: 1.9957 - val_acc: 0.3426\n",
            "32889/32889 [==============================] - 1s 22us/sample - loss: 2.0620 - acc: 0.3511\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 8s 79us/sample - loss: 1.8679 - acc: 0.3570 - val_loss: 1.8082 - val_acc: 0.3923\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 5s 45us/sample - loss: 1.8121 - acc: 0.3739 - val_loss: 1.7974 - val_acc: 0.3971\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 5s 47us/sample - loss: 1.7895 - acc: 0.3814 - val_loss: 1.7906 - val_acc: 0.3985\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 5s 47us/sample - loss: 1.7733 - acc: 0.3862 - val_loss: 1.7872 - val_acc: 0.3969\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 5s 45us/sample - loss: 1.7571 - acc: 0.3929 - val_loss: 1.7941 - val_acc: 0.3965\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.7416 - acc: 0.3953 - val_loss: 1.7764 - val_acc: 0.4038\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.7246 - acc: 0.4009 - val_loss: 1.7970 - val_acc: 0.3978\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.7070 - acc: 0.4055 - val_loss: 1.8109 - val_acc: 0.3929\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.6869 - acc: 0.4118 - val_loss: 1.8001 - val_acc: 0.4025\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.6642 - acc: 0.4178 - val_loss: 1.8182 - val_acc: 0.3922\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.6380 - acc: 0.4263 - val_loss: 1.8390 - val_acc: 0.3886\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.6104 - acc: 0.4333 - val_loss: 1.8728 - val_acc: 0.3839\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 5s 45us/sample - loss: 1.5788 - acc: 0.4441 - val_loss: 1.8796 - val_acc: 0.3872\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.5421 - acc: 0.4551 - val_loss: 1.9154 - val_acc: 0.3822\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.5041 - acc: 0.4680 - val_loss: 1.9421 - val_acc: 0.3843\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 5s 45us/sample - loss: 1.4604 - acc: 0.4820 - val_loss: 1.9770 - val_acc: 0.3785\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 5s 45us/sample - loss: 1.4127 - acc: 0.4992 - val_loss: 2.0262 - val_acc: 0.3640\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 5s 45us/sample - loss: 1.3663 - acc: 0.5137 - val_loss: 2.1027 - val_acc: 0.3583\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.3160 - acc: 0.5332 - val_loss: 2.1344 - val_acc: 0.3600\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.2635 - acc: 0.5504 - val_loss: 2.2395 - val_acc: 0.3534\n",
            "32890/32890 [==============================] - 1s 26us/sample - loss: 2.2575 - acc: 0.3286\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 8s 78us/sample - loss: 1.8511 - acc: 0.3643 - val_loss: 1.8126 - val_acc: 0.3851\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.7967 - acc: 0.3813 - val_loss: 1.8316 - val_acc: 0.3752\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.7752 - acc: 0.3899 - val_loss: 1.7997 - val_acc: 0.3959\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 5s 45us/sample - loss: 1.7567 - acc: 0.3943 - val_loss: 1.8219 - val_acc: 0.3817\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 5s 45us/sample - loss: 1.7419 - acc: 0.3993 - val_loss: 1.8176 - val_acc: 0.3840\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 5s 45us/sample - loss: 1.7274 - acc: 0.4033 - val_loss: 1.7831 - val_acc: 0.3975\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 5s 45us/sample - loss: 1.7102 - acc: 0.4089 - val_loss: 1.7940 - val_acc: 0.3908\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.6927 - acc: 0.4139 - val_loss: 1.8018 - val_acc: 0.3927\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.6737 - acc: 0.4187 - val_loss: 1.8130 - val_acc: 0.3910\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.6517 - acc: 0.4258 - val_loss: 1.8411 - val_acc: 0.3889\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.6260 - acc: 0.4330 - val_loss: 1.8708 - val_acc: 0.3797\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.5956 - acc: 0.4412 - val_loss: 1.8450 - val_acc: 0.3951\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.5623 - acc: 0.4521 - val_loss: 1.9044 - val_acc: 0.3755\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.5262 - acc: 0.4633 - val_loss: 1.9322 - val_acc: 0.3684\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.4860 - acc: 0.4760 - val_loss: 1.9707 - val_acc: 0.3604\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.4408 - acc: 0.4903 - val_loss: 1.9754 - val_acc: 0.3769\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.3953 - acc: 0.5070 - val_loss: 2.0465 - val_acc: 0.3644\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.3458 - acc: 0.5245 - val_loss: 2.0982 - val_acc: 0.3652\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 5s 45us/sample - loss: 1.2938 - acc: 0.5416 - val_loss: 2.1555 - val_acc: 0.3528\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 5s 45us/sample - loss: 1.2394 - acc: 0.5605 - val_loss: 2.2426 - val_acc: 0.3485\n",
            "32890/32890 [==============================] - 1s 26us/sample - loss: 2.3166 - acc: 0.3111\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 8s 79us/sample - loss: 1.8765 - acc: 0.3511 - val_loss: 1.8245 - val_acc: 0.3794\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.8238 - acc: 0.3681 - val_loss: 1.7988 - val_acc: 0.3926\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.8004 - acc: 0.3753 - val_loss: 1.8105 - val_acc: 0.3833\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 5s 45us/sample - loss: 1.7834 - acc: 0.3812 - val_loss: 1.8239 - val_acc: 0.3804\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.7682 - acc: 0.3861 - val_loss: 1.8337 - val_acc: 0.3682\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.7518 - acc: 0.3898 - val_loss: 1.7960 - val_acc: 0.3931\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 5s 45us/sample - loss: 1.7358 - acc: 0.3950 - val_loss: 1.8329 - val_acc: 0.3737\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.7175 - acc: 0.4007 - val_loss: 1.8167 - val_acc: 0.3874\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.6979 - acc: 0.4070 - val_loss: 1.8265 - val_acc: 0.3790\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 5s 45us/sample - loss: 1.6755 - acc: 0.4126 - val_loss: 1.8521 - val_acc: 0.3724\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 5s 46us/sample - loss: 1.6494 - acc: 0.4213 - val_loss: 1.8716 - val_acc: 0.3810\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 5s 45us/sample - loss: 1.6199 - acc: 0.4301 - val_loss: 1.8940 - val_acc: 0.3627\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.5869 - acc: 0.4410 - val_loss: 1.9359 - val_acc: 0.3596\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.5508 - acc: 0.4517 - val_loss: 1.9171 - val_acc: 0.3733\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 5s 45us/sample - loss: 1.5105 - acc: 0.4652 - val_loss: 1.9972 - val_acc: 0.3532\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.4651 - acc: 0.4813 - val_loss: 2.0047 - val_acc: 0.3620\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.4200 - acc: 0.4956 - val_loss: 2.0858 - val_acc: 0.3577\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.3688 - acc: 0.5139 - val_loss: 2.1447 - val_acc: 0.3498\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.3189 - acc: 0.5318 - val_loss: 2.1633 - val_acc: 0.3537\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 5s 45us/sample - loss: 1.2663 - acc: 0.5514 - val_loss: 2.2379 - val_acc: 0.3430\n",
            "32890/32890 [==============================] - 1s 26us/sample - loss: 2.1859 - acc: 0.3396\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 9s 81us/sample - loss: 1.8668 - acc: 0.3548 - val_loss: 1.8494 - val_acc: 0.3773\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 5s 46us/sample - loss: 1.8166 - acc: 0.3716 - val_loss: 1.8492 - val_acc: 0.3730\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 5s 46us/sample - loss: 1.7930 - acc: 0.3790 - val_loss: 1.8358 - val_acc: 0.3812\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 5s 45us/sample - loss: 1.7746 - acc: 0.3832 - val_loss: 1.8229 - val_acc: 0.3862\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 5s 45us/sample - loss: 1.7577 - acc: 0.3908 - val_loss: 1.8317 - val_acc: 0.3896\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.7419 - acc: 0.3960 - val_loss: 1.8337 - val_acc: 0.3832\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.7252 - acc: 0.3999 - val_loss: 1.8373 - val_acc: 0.3837\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.7072 - acc: 0.4052 - val_loss: 1.8523 - val_acc: 0.3774\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.6865 - acc: 0.4103 - val_loss: 1.8588 - val_acc: 0.3793\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 5s 45us/sample - loss: 1.6649 - acc: 0.4185 - val_loss: 1.8731 - val_acc: 0.3773\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.6384 - acc: 0.4260 - val_loss: 1.8746 - val_acc: 0.3804\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.6096 - acc: 0.4347 - val_loss: 1.9375 - val_acc: 0.3618\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 5s 45us/sample - loss: 1.5779 - acc: 0.4458 - val_loss: 1.9598 - val_acc: 0.3626\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 5s 46us/sample - loss: 1.5417 - acc: 0.4577 - val_loss: 1.9575 - val_acc: 0.3704\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 5s 45us/sample - loss: 1.5019 - acc: 0.4694 - val_loss: 1.9930 - val_acc: 0.3592\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.4597 - acc: 0.4841 - val_loss: 2.0392 - val_acc: 0.3568\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.4108 - acc: 0.4997 - val_loss: 2.0943 - val_acc: 0.3584\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.3644 - acc: 0.5167 - val_loss: 2.1511 - val_acc: 0.3490\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 5s 45us/sample - loss: 1.3104 - acc: 0.5361 - val_loss: 2.1885 - val_acc: 0.3514\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.2591 - acc: 0.5530 - val_loss: 2.2459 - val_acc: 0.3452\n",
            "32890/32890 [==============================] - 1s 26us/sample - loss: 2.2133 - acc: 0.3257\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 9s 82us/sample - loss: 1.8717 - acc: 0.3541 - val_loss: 1.8267 - val_acc: 0.3585\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 5s 46us/sample - loss: 1.8188 - acc: 0.3700 - val_loss: 1.8587 - val_acc: 0.3455\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 5s 46us/sample - loss: 1.7963 - acc: 0.3770 - val_loss: 1.8106 - val_acc: 0.3764\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 5s 46us/sample - loss: 1.7790 - acc: 0.3831 - val_loss: 1.8046 - val_acc: 0.3645\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 5s 45us/sample - loss: 1.7631 - acc: 0.3867 - val_loss: 1.8078 - val_acc: 0.3619\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 5s 44us/sample - loss: 1.7473 - acc: 0.3917 - val_loss: 1.7889 - val_acc: 0.3744\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 5s 44us/sample - loss: 1.7290 - acc: 0.3968 - val_loss: 1.8009 - val_acc: 0.3740\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 5s 44us/sample - loss: 1.7118 - acc: 0.4031 - val_loss: 1.7937 - val_acc: 0.3825\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 5s 44us/sample - loss: 1.6914 - acc: 0.4082 - val_loss: 1.8406 - val_acc: 0.3567\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 5s 44us/sample - loss: 1.6681 - acc: 0.4153 - val_loss: 1.8318 - val_acc: 0.3725\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 5s 44us/sample - loss: 1.6411 - acc: 0.4243 - val_loss: 1.8266 - val_acc: 0.3698\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 5s 44us/sample - loss: 1.6114 - acc: 0.4320 - val_loss: 1.8547 - val_acc: 0.3671\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 5s 45us/sample - loss: 1.5795 - acc: 0.4420 - val_loss: 1.8853 - val_acc: 0.3557\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 5s 45us/sample - loss: 1.5410 - acc: 0.4559 - val_loss: 1.9381 - val_acc: 0.3513\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 5s 44us/sample - loss: 1.5005 - acc: 0.4700 - val_loss: 1.9460 - val_acc: 0.3573\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 5s 46us/sample - loss: 1.4571 - acc: 0.4831 - val_loss: 2.0049 - val_acc: 0.3456\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 5s 45us/sample - loss: 1.4097 - acc: 0.4975 - val_loss: 2.0243 - val_acc: 0.3411\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 5s 45us/sample - loss: 1.3580 - acc: 0.5183 - val_loss: 2.0768 - val_acc: 0.3404\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 5s 45us/sample - loss: 1.3082 - acc: 0.5359 - val_loss: 2.1320 - val_acc: 0.3367\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 5s 44us/sample - loss: 1.2535 - acc: 0.5538 - val_loss: 2.1809 - val_acc: 0.3303\n",
            "32889/32889 [==============================] - 1s 27us/sample - loss: 2.3172 - acc: 0.3169\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 13s 119us/sample - loss: 1.8749 - acc: 0.3534 - val_loss: 1.8021 - val_acc: 0.3966\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 9s 85us/sample - loss: 1.8158 - acc: 0.3726 - val_loss: 1.7961 - val_acc: 0.3999\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 9s 85us/sample - loss: 1.7929 - acc: 0.3795 - val_loss: 1.7985 - val_acc: 0.3894\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 9s 86us/sample - loss: 1.7742 - acc: 0.3862 - val_loss: 1.7855 - val_acc: 0.3940\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 9s 84us/sample - loss: 1.7575 - acc: 0.3899 - val_loss: 1.8097 - val_acc: 0.3920\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 9s 85us/sample - loss: 1.7415 - acc: 0.3955 - val_loss: 1.7871 - val_acc: 0.3997\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 9s 85us/sample - loss: 1.7236 - acc: 0.4012 - val_loss: 1.7824 - val_acc: 0.4036\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 9s 84us/sample - loss: 1.7031 - acc: 0.4069 - val_loss: 1.8114 - val_acc: 0.3932\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 9s 85us/sample - loss: 1.6799 - acc: 0.4129 - val_loss: 1.8306 - val_acc: 0.3922\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 9s 85us/sample - loss: 1.6525 - acc: 0.4208 - val_loss: 1.8412 - val_acc: 0.3957\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 9s 85us/sample - loss: 1.6202 - acc: 0.4315 - val_loss: 1.8466 - val_acc: 0.3878\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 9s 85us/sample - loss: 1.5829 - acc: 0.4440 - val_loss: 1.8784 - val_acc: 0.3877\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 9s 86us/sample - loss: 1.5428 - acc: 0.4539 - val_loss: 1.9318 - val_acc: 0.3812\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 9s 85us/sample - loss: 1.4932 - acc: 0.4705 - val_loss: 1.9910 - val_acc: 0.3665\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 9s 85us/sample - loss: 1.4405 - acc: 0.4878 - val_loss: 2.0226 - val_acc: 0.3706\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 9s 84us/sample - loss: 1.3841 - acc: 0.5096 - val_loss: 2.1079 - val_acc: 0.3658\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 9s 85us/sample - loss: 1.3222 - acc: 0.5311 - val_loss: 2.1658 - val_acc: 0.3641\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 9s 85us/sample - loss: 1.2580 - acc: 0.5522 - val_loss: 2.2396 - val_acc: 0.3547\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 9s 84us/sample - loss: 1.1957 - acc: 0.5741 - val_loss: 2.3371 - val_acc: 0.3494\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 9s 84us/sample - loss: 1.1289 - acc: 0.5994 - val_loss: 2.4261 - val_acc: 0.3470\n",
            "32890/32890 [==============================] - 1s 36us/sample - loss: 2.4763 - acc: 0.3178\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 13s 121us/sample - loss: 1.8543 - acc: 0.3634 - val_loss: 1.8170 - val_acc: 0.3854\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 9s 85us/sample - loss: 1.8011 - acc: 0.3816 - val_loss: 1.7881 - val_acc: 0.4011\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 9s 87us/sample - loss: 1.7782 - acc: 0.3881 - val_loss: 1.7962 - val_acc: 0.3920\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 9s 84us/sample - loss: 1.7592 - acc: 0.3942 - val_loss: 1.8215 - val_acc: 0.3821\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 9s 85us/sample - loss: 1.7449 - acc: 0.3974 - val_loss: 1.8012 - val_acc: 0.3949\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 9s 85us/sample - loss: 1.7266 - acc: 0.4030 - val_loss: 1.8016 - val_acc: 0.3929\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 9s 85us/sample - loss: 1.7095 - acc: 0.4089 - val_loss: 1.8103 - val_acc: 0.3889\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 9s 85us/sample - loss: 1.6892 - acc: 0.4147 - val_loss: 1.7933 - val_acc: 0.4000\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 9s 86us/sample - loss: 1.6660 - acc: 0.4193 - val_loss: 1.7998 - val_acc: 0.3993\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 9s 85us/sample - loss: 1.6380 - acc: 0.4285 - val_loss: 1.8587 - val_acc: 0.3837\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 9s 85us/sample - loss: 1.6068 - acc: 0.4380 - val_loss: 1.8803 - val_acc: 0.3820\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 9s 85us/sample - loss: 1.5703 - acc: 0.4489 - val_loss: 1.8916 - val_acc: 0.3860\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 9s 84us/sample - loss: 1.5264 - acc: 0.4633 - val_loss: 1.9182 - val_acc: 0.3750\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 9s 85us/sample - loss: 1.4807 - acc: 0.4774 - val_loss: 1.9832 - val_acc: 0.3668\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 9s 85us/sample - loss: 1.4280 - acc: 0.4948 - val_loss: 2.0425 - val_acc: 0.3649\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 9s 86us/sample - loss: 1.3705 - acc: 0.5160 - val_loss: 2.1034 - val_acc: 0.3661\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 9s 86us/sample - loss: 1.3118 - acc: 0.5349 - val_loss: 2.1686 - val_acc: 0.3605\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 9s 85us/sample - loss: 1.2462 - acc: 0.5584 - val_loss: 2.2620 - val_acc: 0.3548\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 9s 85us/sample - loss: 1.1811 - acc: 0.5801 - val_loss: 2.3343 - val_acc: 0.3415\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 9s 86us/sample - loss: 1.1157 - acc: 0.6048 - val_loss: 2.4498 - val_acc: 0.3437\n",
            "32890/32890 [==============================] - 1s 36us/sample - loss: 2.5293 - acc: 0.3028\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 13s 124us/sample - loss: 1.8815 - acc: 0.3502 - val_loss: 1.8567 - val_acc: 0.3602\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 9s 86us/sample - loss: 1.8244 - acc: 0.3669 - val_loss: 1.8112 - val_acc: 0.3861\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 9s 85us/sample - loss: 1.8024 - acc: 0.3746 - val_loss: 1.8395 - val_acc: 0.3742\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 9s 86us/sample - loss: 1.7838 - acc: 0.3810 - val_loss: 1.8176 - val_acc: 0.3827\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 9s 87us/sample - loss: 1.7681 - acc: 0.3847 - val_loss: 1.8101 - val_acc: 0.3882\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 9s 86us/sample - loss: 1.7513 - acc: 0.3913 - val_loss: 1.8245 - val_acc: 0.3818\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 9s 86us/sample - loss: 1.7312 - acc: 0.3966 - val_loss: 1.8304 - val_acc: 0.3793\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 9s 88us/sample - loss: 1.7099 - acc: 0.4030 - val_loss: 1.8233 - val_acc: 0.3864\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 9s 86us/sample - loss: 1.6864 - acc: 0.4097 - val_loss: 1.8483 - val_acc: 0.3802\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 10s 91us/sample - loss: 1.6578 - acc: 0.4184 - val_loss: 1.8762 - val_acc: 0.3748\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 10s 93us/sample - loss: 1.6247 - acc: 0.4293 - val_loss: 1.9052 - val_acc: 0.3677\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 10s 92us/sample - loss: 1.5852 - acc: 0.4403 - val_loss: 1.9204 - val_acc: 0.3707\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 10s 92us/sample - loss: 1.5425 - acc: 0.4544 - val_loss: 1.9784 - val_acc: 0.3601\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 10s 93us/sample - loss: 1.4934 - acc: 0.4708 - val_loss: 2.0330 - val_acc: 0.3471\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 9s 87us/sample - loss: 1.4395 - acc: 0.4889 - val_loss: 2.0518 - val_acc: 0.3650\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 9s 85us/sample - loss: 1.3817 - acc: 0.5093 - val_loss: 2.1744 - val_acc: 0.3406\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 9s 86us/sample - loss: 1.3180 - acc: 0.5321 - val_loss: 2.1932 - val_acc: 0.3487\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 9s 85us/sample - loss: 1.2545 - acc: 0.5544 - val_loss: 2.2804 - val_acc: 0.3346\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 9s 85us/sample - loss: 1.1884 - acc: 0.5773 - val_loss: 2.4071 - val_acc: 0.3304\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 9s 86us/sample - loss: 1.1227 - acc: 0.6008 - val_loss: 2.4927 - val_acc: 0.3307\n",
            "32890/32890 [==============================] - 1s 37us/sample - loss: 2.3954 - acc: 0.3306\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 14s 129us/sample - loss: 1.8728 - acc: 0.3552 - val_loss: 1.8466 - val_acc: 0.3756\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 10s 91us/sample - loss: 1.8185 - acc: 0.3726 - val_loss: 1.8438 - val_acc: 0.3812\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 10s 91us/sample - loss: 1.7932 - acc: 0.3782 - val_loss: 1.8468 - val_acc: 0.3759\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 10s 92us/sample - loss: 1.7765 - acc: 0.3859 - val_loss: 1.8222 - val_acc: 0.3887\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 10s 91us/sample - loss: 1.7605 - acc: 0.3891 - val_loss: 1.8123 - val_acc: 0.3899\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 9s 88us/sample - loss: 1.7420 - acc: 0.3955 - val_loss: 1.8451 - val_acc: 0.3779\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 9s 85us/sample - loss: 1.7244 - acc: 0.3997 - val_loss: 1.8348 - val_acc: 0.3811\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 9s 85us/sample - loss: 1.7041 - acc: 0.4057 - val_loss: 1.8572 - val_acc: 0.3728\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 9s 85us/sample - loss: 1.6807 - acc: 0.4124 - val_loss: 1.8713 - val_acc: 0.3733\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 9s 85us/sample - loss: 1.6535 - acc: 0.4220 - val_loss: 1.8930 - val_acc: 0.3783\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 9s 85us/sample - loss: 1.6214 - acc: 0.4314 - val_loss: 1.8827 - val_acc: 0.3846\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 9s 88us/sample - loss: 1.5845 - acc: 0.4418 - val_loss: 1.9544 - val_acc: 0.3710\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 9s 89us/sample - loss: 1.5421 - acc: 0.4563 - val_loss: 1.9816 - val_acc: 0.3653\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 10s 92us/sample - loss: 1.4961 - acc: 0.4706 - val_loss: 2.0403 - val_acc: 0.3490\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 10s 91us/sample - loss: 1.4439 - acc: 0.4874 - val_loss: 2.0836 - val_acc: 0.3564\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 10s 92us/sample - loss: 1.3873 - acc: 0.5082 - val_loss: 2.1512 - val_acc: 0.3446\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 9s 87us/sample - loss: 1.3267 - acc: 0.5292 - val_loss: 2.2089 - val_acc: 0.3492\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 9s 85us/sample - loss: 1.2650 - acc: 0.5502 - val_loss: 2.2924 - val_acc: 0.3456\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 9s 86us/sample - loss: 1.1983 - acc: 0.5749 - val_loss: 2.3975 - val_acc: 0.3439\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 9s 85us/sample - loss: 1.1334 - acc: 0.5972 - val_loss: 2.4853 - val_acc: 0.3267\n",
            "32890/32890 [==============================] - 1s 36us/sample - loss: 2.3965 - acc: 0.3090\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 13s 122us/sample - loss: 1.8810 - acc: 0.3511 - val_loss: 1.8147 - val_acc: 0.3661\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 9s 85us/sample - loss: 1.8241 - acc: 0.3687 - val_loss: 1.7902 - val_acc: 0.3808\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 9s 85us/sample - loss: 1.7997 - acc: 0.3761 - val_loss: 1.7781 - val_acc: 0.3839\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 9s 87us/sample - loss: 1.7802 - acc: 0.3817 - val_loss: 1.8039 - val_acc: 0.3698\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 10s 91us/sample - loss: 1.7640 - acc: 0.3849 - val_loss: 1.7918 - val_acc: 0.3744\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 10s 91us/sample - loss: 1.7472 - acc: 0.3906 - val_loss: 1.7965 - val_acc: 0.3717\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 10s 91us/sample - loss: 1.7287 - acc: 0.3965 - val_loss: 1.7880 - val_acc: 0.3788\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 10s 91us/sample - loss: 1.7075 - acc: 0.4033 - val_loss: 1.8242 - val_acc: 0.3650\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 10s 92us/sample - loss: 1.6832 - acc: 0.4108 - val_loss: 1.8133 - val_acc: 0.3774\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 10s 90us/sample - loss: 1.6561 - acc: 0.4196 - val_loss: 1.8397 - val_acc: 0.3655\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 9s 85us/sample - loss: 1.6244 - acc: 0.4287 - val_loss: 1.8632 - val_acc: 0.3635\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 9s 85us/sample - loss: 1.5874 - acc: 0.4393 - val_loss: 1.8876 - val_acc: 0.3578\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 9s 85us/sample - loss: 1.5462 - acc: 0.4539 - val_loss: 1.9211 - val_acc: 0.3572\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 9s 85us/sample - loss: 1.4969 - acc: 0.4702 - val_loss: 1.9572 - val_acc: 0.3465\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 9s 85us/sample - loss: 1.4461 - acc: 0.4850 - val_loss: 2.0040 - val_acc: 0.3390\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 9s 85us/sample - loss: 1.3884 - acc: 0.5063 - val_loss: 2.0815 - val_acc: 0.3337\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 9s 87us/sample - loss: 1.3295 - acc: 0.5278 - val_loss: 2.1194 - val_acc: 0.3395\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 9s 85us/sample - loss: 1.2660 - acc: 0.5490 - val_loss: 2.2004 - val_acc: 0.3348\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 9s 85us/sample - loss: 1.2007 - acc: 0.5721 - val_loss: 2.2836 - val_acc: 0.3265\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 9s 85us/sample - loss: 1.1348 - acc: 0.5963 - val_loss: 2.3780 - val_acc: 0.3251\n",
            "32889/32889 [==============================] - 1s 37us/sample - loss: 2.4881 - acc: 0.3277\n",
            "Train on 131559 samples, validate on 32890 samples\n",
            "Epoch 1/20\n",
            "131559/131559 [==============================] - 8s 61us/sample - loss: 1.8855 - acc: 0.3486 - val_loss: 1.8555 - val_acc: 0.3803\n",
            "Epoch 2/20\n",
            "131559/131559 [==============================] - 4s 31us/sample - loss: 1.8240 - acc: 0.3677 - val_loss: 1.8146 - val_acc: 0.3929\n",
            "Epoch 3/20\n",
            "131559/131559 [==============================] - 4s 31us/sample - loss: 1.8045 - acc: 0.3754 - val_loss: 1.8199 - val_acc: 0.3943\n",
            "Epoch 4/20\n",
            "131559/131559 [==============================] - 4s 31us/sample - loss: 1.7927 - acc: 0.3792 - val_loss: 1.8260 - val_acc: 0.3874\n",
            "Epoch 5/20\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7832 - acc: 0.3818 - val_loss: 1.8178 - val_acc: 0.3928\n",
            "Epoch 6/20\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7751 - acc: 0.3840 - val_loss: 1.8265 - val_acc: 0.3886\n",
            "Epoch 7/20\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7685 - acc: 0.3859 - val_loss: 1.8118 - val_acc: 0.3926\n",
            "Epoch 8/20\n",
            "131559/131559 [==============================] - 4s 30us/sample - loss: 1.7630 - acc: 0.3878 - val_loss: 1.8168 - val_acc: 0.3915\n",
            "Epoch 9/20\n",
            "131559/131559 [==============================] - 4s 31us/sample - loss: 1.7578 - acc: 0.3899 - val_loss: 1.8106 - val_acc: 0.3908\n",
            "Epoch 10/20\n",
            "131559/131559 [==============================] - 4s 31us/sample - loss: 1.7526 - acc: 0.3911 - val_loss: 1.8100 - val_acc: 0.3882\n",
            "Epoch 11/20\n",
            "131559/131559 [==============================] - 4s 31us/sample - loss: 1.7491 - acc: 0.3920 - val_loss: 1.8237 - val_acc: 0.3879\n",
            "Epoch 12/20\n",
            "131559/131559 [==============================] - 4s 31us/sample - loss: 1.7450 - acc: 0.3934 - val_loss: 1.8001 - val_acc: 0.3975\n",
            "Epoch 13/20\n",
            "131559/131559 [==============================] - 4s 31us/sample - loss: 1.7412 - acc: 0.3939 - val_loss: 1.8065 - val_acc: 0.3978\n",
            "Epoch 14/20\n",
            "131559/131559 [==============================] - 4s 31us/sample - loss: 1.7368 - acc: 0.3955 - val_loss: 1.8196 - val_acc: 0.3938\n",
            "Epoch 15/20\n",
            "131559/131559 [==============================] - 4s 31us/sample - loss: 1.7338 - acc: 0.3973 - val_loss: 1.7976 - val_acc: 0.3945\n",
            "Epoch 16/20\n",
            "131559/131559 [==============================] - 4s 30us/sample - loss: 1.7305 - acc: 0.3979 - val_loss: 1.8226 - val_acc: 0.3916\n",
            "Epoch 17/20\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7273 - acc: 0.3994 - val_loss: 1.8013 - val_acc: 0.3985\n",
            "Epoch 18/20\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7242 - acc: 0.3996 - val_loss: 1.8049 - val_acc: 0.3945\n",
            "Epoch 19/20\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7212 - acc: 0.4010 - val_loss: 1.8147 - val_acc: 0.3930\n",
            "Epoch 20/20\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7178 - acc: 0.4025 - val_loss: 1.8170 - val_acc: 0.3923\n",
            "Best: 0.371045196056366 using {'neurons': 192}\n",
            "Means: 0.3553504467010498, Stdev: 0.01960054176866872 with: {'neurons': 12}\n",
            "Means: 0.36543865203857423, Stdev: 0.015721954176948803 with: {'neurons': 24}\n",
            "Means: 0.36880743503570557, Stdev: 0.017939938452548616 with: {'neurons': 48}\n",
            "Means: 0.37064998149871825, Stdev: 0.017484044476873312 with: {'neurons': 96}\n",
            "Means: 0.371045196056366, Stdev: 0.015233364816380783 with: {'neurons': 192}\n",
            "Means: 0.36891688108444215, Stdev: 0.012507986792589531 with: {'neurons': 384}\n",
            "Means: 0.357168585062027, Stdev: 0.01319083311351565 with: {'neurons': 768}\n",
            "Means: 0.3459735572338104, Stdev: 0.009374468116650929 with: {'neurons': 1536}\n",
            "Means: 0.3243862330913544, Stdev: 0.009843500622420622 with: {'neurons': 3072}\n",
            "Means: 0.3175817966461182, Stdev: 0.010624445379553104 with: {'neurons': 6144}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwyAEfJese0z",
        "colab_type": "text"
      },
      "source": [
        "### Weight Initialization.*(normal)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xBn5uu-sinP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function for NN model.\n",
        "def create_model(weight_init='uniform'):\n",
        "    # set the model.\n",
        "    model = Sequential()\n",
        "    # hidden layers.\n",
        "    model.add(Dense(192, input_dim=13, kernel_initializer=weight_init, activation='relu'))\n",
        "    model.add(Dense(96, kernel_initializer=weight_init, activation='relu'))\n",
        "    model.add(Dense(48, kernel_initializer=weight_init, activation='relu'))\n",
        "    model.add(Dense(24, kernel_initializer=weight_init, activation='relu'))\n",
        "    model.add(Dense(12, kernel_initializer=weight_init, activation='softmax'))\n",
        "    # function for top 3.\n",
        "    def top_3_accuracy(y_true, y_pred):\n",
        "        return top_k_categorical_accuracy(y_true, y_pred, k=3)\n",
        "\n",
        "    # set the optimizer.\n",
        "    optimizer = Adam(lr=0.001)\n",
        "    # compile the model.\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                        optimizer=optimizer,\n",
        "                        metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zgKBxZRsitW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set the KerasClassifier model.\n",
        "model = KerasClassifier(build_fn=create_model, epochs=20, batch_size=200, validation_split=0.2, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJCRxG0BsisJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set the parameters for weight initializer.\n",
        "weight_init = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
        "param_grid = dict(weight_init=weight_init)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvB4BlPIsiqI",
        "colab_type": "code",
        "outputId": "29d91ca8-cc53-44e9-f1e1-2527423e99c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# set the gridsearch.\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
        "grid_result = grid.fit(X_scaled, y_train)\n",
        "# show the results.\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 8s 73us/sample - loss: 2.0559 - acc: 0.2841 - val_loss: 1.8959 - val_acc: 0.3597\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.9016 - acc: 0.3407 - val_loss: 1.8477 - val_acc: 0.3811\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8808 - acc: 0.3476 - val_loss: 1.8512 - val_acc: 0.3779\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8667 - acc: 0.3534 - val_loss: 1.8404 - val_acc: 0.3859\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8525 - acc: 0.3596 - val_loss: 1.8063 - val_acc: 0.3957\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8423 - acc: 0.3641 - val_loss: 1.8197 - val_acc: 0.3868\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8334 - acc: 0.3683 - val_loss: 1.8203 - val_acc: 0.3880\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.8232 - acc: 0.3725 - val_loss: 1.8077 - val_acc: 0.3983\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.8167 - acc: 0.3752 - val_loss: 1.8039 - val_acc: 0.3888\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8104 - acc: 0.3769 - val_loss: 1.7789 - val_acc: 0.4057\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8043 - acc: 0.3791 - val_loss: 1.7920 - val_acc: 0.3997\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7985 - acc: 0.3806 - val_loss: 1.7926 - val_acc: 0.4006\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.7942 - acc: 0.3839 - val_loss: 1.7851 - val_acc: 0.4019\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.7892 - acc: 0.3834 - val_loss: 1.7774 - val_acc: 0.4047\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7849 - acc: 0.3859 - val_loss: 1.7837 - val_acc: 0.4029\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.7813 - acc: 0.3854 - val_loss: 1.7800 - val_acc: 0.4010\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.7779 - acc: 0.3868 - val_loss: 1.7708 - val_acc: 0.4054\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.7748 - acc: 0.3886 - val_loss: 1.7783 - val_acc: 0.4032\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7709 - acc: 0.3899 - val_loss: 1.7616 - val_acc: 0.4086\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7678 - acc: 0.3909 - val_loss: 1.7767 - val_acc: 0.4036\n",
            "32890/32890 [==============================] - 1s 25us/sample - loss: 1.8301 - acc: 0.3688\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 8s 78us/sample - loss: 2.0458 - acc: 0.2876 - val_loss: 1.8808 - val_acc: 0.3616\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.8846 - acc: 0.3486 - val_loss: 1.8660 - val_acc: 0.3633\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.8671 - acc: 0.3545 - val_loss: 1.8573 - val_acc: 0.3719\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.8561 - acc: 0.3579 - val_loss: 1.8497 - val_acc: 0.3748\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.8478 - acc: 0.3614 - val_loss: 1.8328 - val_acc: 0.3804\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8392 - acc: 0.3654 - val_loss: 1.8451 - val_acc: 0.3770\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.8309 - acc: 0.3694 - val_loss: 1.8431 - val_acc: 0.3783\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.8188 - acc: 0.3760 - val_loss: 1.8236 - val_acc: 0.3899\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8085 - acc: 0.3798 - val_loss: 1.8096 - val_acc: 0.3915\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.8020 - acc: 0.3819 - val_loss: 1.8007 - val_acc: 0.3954\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.7956 - acc: 0.3837 - val_loss: 1.8087 - val_acc: 0.3897\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.7909 - acc: 0.3867 - val_loss: 1.7999 - val_acc: 0.3910\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7851 - acc: 0.3882 - val_loss: 1.8065 - val_acc: 0.3908\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7808 - acc: 0.3906 - val_loss: 1.8087 - val_acc: 0.3888\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.7778 - acc: 0.3912 - val_loss: 1.7839 - val_acc: 0.3965\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7744 - acc: 0.3922 - val_loss: 1.7791 - val_acc: 0.3998\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7706 - acc: 0.3939 - val_loss: 1.8007 - val_acc: 0.3941\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7672 - acc: 0.3939 - val_loss: 1.7782 - val_acc: 0.3955\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7637 - acc: 0.3953 - val_loss: 1.7918 - val_acc: 0.3931\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7613 - acc: 0.3960 - val_loss: 1.7856 - val_acc: 0.3946\n",
            "32890/32890 [==============================] - 1s 25us/sample - loss: 1.8878 - acc: 0.3443\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 8s 76us/sample - loss: 2.0893 - acc: 0.2745 - val_loss: 1.9400 - val_acc: 0.3446\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.9312 - acc: 0.3318 - val_loss: 1.8946 - val_acc: 0.3643\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.9039 - acc: 0.3385 - val_loss: 1.8864 - val_acc: 0.3652\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.8833 - acc: 0.3454 - val_loss: 1.8528 - val_acc: 0.3777\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.8677 - acc: 0.3525 - val_loss: 1.8437 - val_acc: 0.3799\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.8530 - acc: 0.3579 - val_loss: 1.8302 - val_acc: 0.3862\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.8428 - acc: 0.3627 - val_loss: 1.8130 - val_acc: 0.3965\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.8342 - acc: 0.3667 - val_loss: 1.8311 - val_acc: 0.3839\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.8277 - acc: 0.3686 - val_loss: 1.8298 - val_acc: 0.3845\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.8217 - acc: 0.3713 - val_loss: 1.8176 - val_acc: 0.3885\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.8147 - acc: 0.3733 - val_loss: 1.8211 - val_acc: 0.3858\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.8093 - acc: 0.3748 - val_loss: 1.8374 - val_acc: 0.3734\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.8057 - acc: 0.3761 - val_loss: 1.8004 - val_acc: 0.3877\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.8014 - acc: 0.3789 - val_loss: 1.8078 - val_acc: 0.3861\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7966 - acc: 0.3780 - val_loss: 1.8085 - val_acc: 0.3870\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7926 - acc: 0.3806 - val_loss: 1.7884 - val_acc: 0.3936\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7899 - acc: 0.3820 - val_loss: 1.8402 - val_acc: 0.3795\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7869 - acc: 0.3825 - val_loss: 1.7745 - val_acc: 0.3972\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7842 - acc: 0.3827 - val_loss: 1.7984 - val_acc: 0.3878\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7798 - acc: 0.3847 - val_loss: 1.8280 - val_acc: 0.3733\n",
            "32890/32890 [==============================] - 1s 25us/sample - loss: 1.7938 - acc: 0.3832\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 8s 79us/sample - loss: 2.0565 - acc: 0.2796 - val_loss: 1.8948 - val_acc: 0.3563\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.8979 - acc: 0.3457 - val_loss: 1.8944 - val_acc: 0.3613\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.8772 - acc: 0.3521 - val_loss: 1.8615 - val_acc: 0.3800\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.8618 - acc: 0.3590 - val_loss: 1.8578 - val_acc: 0.3804\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.8502 - acc: 0.3627 - val_loss: 1.8546 - val_acc: 0.3790\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.8385 - acc: 0.3671 - val_loss: 1.8641 - val_acc: 0.3740\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.8282 - acc: 0.3703 - val_loss: 1.8474 - val_acc: 0.3766\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.8209 - acc: 0.3720 - val_loss: 1.8544 - val_acc: 0.3708\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.8148 - acc: 0.3735 - val_loss: 1.8305 - val_acc: 0.3888\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8096 - acc: 0.3767 - val_loss: 1.8294 - val_acc: 0.3847\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.8045 - acc: 0.3771 - val_loss: 1.8013 - val_acc: 0.3942\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8000 - acc: 0.3799 - val_loss: 1.8484 - val_acc: 0.3798\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.7956 - acc: 0.3826 - val_loss: 1.8621 - val_acc: 0.3710\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7911 - acc: 0.3830 - val_loss: 1.8062 - val_acc: 0.3910\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7879 - acc: 0.3850 - val_loss: 1.8095 - val_acc: 0.3909\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7840 - acc: 0.3872 - val_loss: 1.8013 - val_acc: 0.3954\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7805 - acc: 0.3881 - val_loss: 1.8046 - val_acc: 0.3932\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7782 - acc: 0.3886 - val_loss: 1.8171 - val_acc: 0.3838\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7749 - acc: 0.3888 - val_loss: 1.8015 - val_acc: 0.3991\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7718 - acc: 0.3908 - val_loss: 1.8248 - val_acc: 0.3862\n",
            "32890/32890 [==============================] - 1s 25us/sample - loss: 1.8250 - acc: 0.3696\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 8s 78us/sample - loss: 2.0760 - acc: 0.2721 - val_loss: 1.9220 - val_acc: 0.3310\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 4s 35us/sample - loss: 1.9034 - acc: 0.3406 - val_loss: 1.8795 - val_acc: 0.3481\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 4s 35us/sample - loss: 1.8817 - acc: 0.3482 - val_loss: 1.8793 - val_acc: 0.3422\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 4s 36us/sample - loss: 1.8692 - acc: 0.3538 - val_loss: 1.8713 - val_acc: 0.3400\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 4s 35us/sample - loss: 1.8588 - acc: 0.3581 - val_loss: 1.8581 - val_acc: 0.3482\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 4s 36us/sample - loss: 1.8509 - acc: 0.3615 - val_loss: 1.8508 - val_acc: 0.3542\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 4s 35us/sample - loss: 1.8417 - acc: 0.3652 - val_loss: 1.8316 - val_acc: 0.3593\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 4s 35us/sample - loss: 1.8323 - acc: 0.3661 - val_loss: 1.8169 - val_acc: 0.3686\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 4s 36us/sample - loss: 1.8269 - acc: 0.3686 - val_loss: 1.8260 - val_acc: 0.3599\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 4s 37us/sample - loss: 1.8208 - acc: 0.3707 - val_loss: 1.8467 - val_acc: 0.3533\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 4s 38us/sample - loss: 1.8157 - acc: 0.3728 - val_loss: 1.8151 - val_acc: 0.3653\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 4s 36us/sample - loss: 1.8117 - acc: 0.3740 - val_loss: 1.8064 - val_acc: 0.3738\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 4s 35us/sample - loss: 1.8061 - acc: 0.3756 - val_loss: 1.8103 - val_acc: 0.3645\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 4s 36us/sample - loss: 1.8032 - acc: 0.3770 - val_loss: 1.7947 - val_acc: 0.3755\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 4s 35us/sample - loss: 1.7997 - acc: 0.3778 - val_loss: 1.8041 - val_acc: 0.3683\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 4s 35us/sample - loss: 1.7954 - acc: 0.3798 - val_loss: 1.7829 - val_acc: 0.3796\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 4s 35us/sample - loss: 1.7921 - acc: 0.3805 - val_loss: 1.7847 - val_acc: 0.3799\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 4s 35us/sample - loss: 1.7891 - acc: 0.3819 - val_loss: 1.7986 - val_acc: 0.3728\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 4s 36us/sample - loss: 1.7865 - acc: 0.3825 - val_loss: 1.7968 - val_acc: 0.3732\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 4s 35us/sample - loss: 1.7825 - acc: 0.3837 - val_loss: 1.7899 - val_acc: 0.3776\n",
            "32889/32889 [==============================] - 1s 26us/sample - loss: 1.8121 - acc: 0.3940\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 8s 79us/sample - loss: 1.9336 - acc: 0.3363 - val_loss: 1.8205 - val_acc: 0.3896\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.8450 - acc: 0.3654 - val_loss: 1.8127 - val_acc: 0.3895\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.8238 - acc: 0.3710 - val_loss: 1.7944 - val_acc: 0.3951\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.8096 - acc: 0.3754 - val_loss: 1.8012 - val_acc: 0.3950\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7991 - acc: 0.3788 - val_loss: 1.8076 - val_acc: 0.3846\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7904 - acc: 0.3814 - val_loss: 1.7784 - val_acc: 0.4040\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7835 - acc: 0.3838 - val_loss: 1.7905 - val_acc: 0.4008\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7770 - acc: 0.3876 - val_loss: 1.8167 - val_acc: 0.3915\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7714 - acc: 0.3885 - val_loss: 1.7954 - val_acc: 0.3969\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7669 - acc: 0.3905 - val_loss: 1.8011 - val_acc: 0.3910\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7618 - acc: 0.3908 - val_loss: 1.7863 - val_acc: 0.4008\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7568 - acc: 0.3929 - val_loss: 1.7928 - val_acc: 0.4005\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7532 - acc: 0.3952 - val_loss: 1.7778 - val_acc: 0.4063\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7499 - acc: 0.3946 - val_loss: 1.7782 - val_acc: 0.4061\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7458 - acc: 0.3959 - val_loss: 1.7871 - val_acc: 0.4022\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7424 - acc: 0.3974 - val_loss: 1.7951 - val_acc: 0.3987\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7383 - acc: 0.3991 - val_loss: 1.7888 - val_acc: 0.3974\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7354 - acc: 0.3993 - val_loss: 1.7977 - val_acc: 0.3963\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.7334 - acc: 0.4002 - val_loss: 1.8154 - val_acc: 0.3936\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7302 - acc: 0.4022 - val_loss: 1.7875 - val_acc: 0.4039\n",
            "32890/32890 [==============================] - 1s 25us/sample - loss: 1.8505 - acc: 0.3651\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 8s 74us/sample - loss: 1.9220 - acc: 0.3427 - val_loss: 1.8183 - val_acc: 0.3838\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8251 - acc: 0.3741 - val_loss: 1.8260 - val_acc: 0.3860\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8066 - acc: 0.3794 - val_loss: 1.8066 - val_acc: 0.3874\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7943 - acc: 0.3833 - val_loss: 1.8137 - val_acc: 0.3847\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7858 - acc: 0.3874 - val_loss: 1.8165 - val_acc: 0.3845\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.7771 - acc: 0.3889 - val_loss: 1.8086 - val_acc: 0.3848\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7703 - acc: 0.3923 - val_loss: 1.7873 - val_acc: 0.3979\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7638 - acc: 0.3944 - val_loss: 1.7965 - val_acc: 0.3913\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.7596 - acc: 0.3954 - val_loss: 1.8056 - val_acc: 0.3874\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.7537 - acc: 0.3977 - val_loss: 1.8016 - val_acc: 0.3942\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.7487 - acc: 0.3991 - val_loss: 1.8046 - val_acc: 0.3884\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7453 - acc: 0.3993 - val_loss: 1.7891 - val_acc: 0.3939\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7420 - acc: 0.4015 - val_loss: 1.8131 - val_acc: 0.3928\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.7374 - acc: 0.4019 - val_loss: 1.8064 - val_acc: 0.3877\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.7339 - acc: 0.4041 - val_loss: 1.7789 - val_acc: 0.4006\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7307 - acc: 0.4032 - val_loss: 1.7843 - val_acc: 0.3988\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.7276 - acc: 0.4059 - val_loss: 1.8048 - val_acc: 0.3889\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.7235 - acc: 0.4066 - val_loss: 1.8106 - val_acc: 0.3809\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.7198 - acc: 0.4091 - val_loss: 1.8012 - val_acc: 0.3924\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.7179 - acc: 0.4091 - val_loss: 1.7947 - val_acc: 0.3935\n",
            "32890/32890 [==============================] - 1s 26us/sample - loss: 1.8838 - acc: 0.3441\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 8s 75us/sample - loss: 1.9528 - acc: 0.3302 - val_loss: 1.8645 - val_acc: 0.3642\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8538 - acc: 0.3590 - val_loss: 1.8286 - val_acc: 0.3783\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8340 - acc: 0.3646 - val_loss: 1.8459 - val_acc: 0.3582\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8218 - acc: 0.3693 - val_loss: 1.8413 - val_acc: 0.3718\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.8103 - acc: 0.3745 - val_loss: 1.8099 - val_acc: 0.3888\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.8024 - acc: 0.3748 - val_loss: 1.8077 - val_acc: 0.3863\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.7939 - acc: 0.3781 - val_loss: 1.7995 - val_acc: 0.3940\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.7884 - acc: 0.3800 - val_loss: 1.8041 - val_acc: 0.3838\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.7828 - acc: 0.3818 - val_loss: 1.8110 - val_acc: 0.3833\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.7777 - acc: 0.3835 - val_loss: 1.8223 - val_acc: 0.3780\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.7727 - acc: 0.3845 - val_loss: 1.8078 - val_acc: 0.3853\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.7685 - acc: 0.3870 - val_loss: 1.8212 - val_acc: 0.3783\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.7639 - acc: 0.3890 - val_loss: 1.8220 - val_acc: 0.3798\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.7606 - acc: 0.3904 - val_loss: 1.8174 - val_acc: 0.3792\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 4s 33us/sample - loss: 1.7562 - acc: 0.3924 - val_loss: 1.8093 - val_acc: 0.3886\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.7532 - acc: 0.3928 - val_loss: 1.8398 - val_acc: 0.3745\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7489 - acc: 0.3950 - val_loss: 1.8112 - val_acc: 0.3861\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.7454 - acc: 0.3953 - val_loss: 1.8248 - val_acc: 0.3789\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 4s 33us/sample - loss: 1.7424 - acc: 0.3957 - val_loss: 1.8516 - val_acc: 0.3584\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 4s 33us/sample - loss: 1.7396 - acc: 0.3972 - val_loss: 1.8238 - val_acc: 0.3785\n",
            "32890/32890 [==============================] - 1s 25us/sample - loss: 1.7990 - acc: 0.3791\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 8s 76us/sample - loss: 1.9296 - acc: 0.3385 - val_loss: 1.8674 - val_acc: 0.3713\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.8466 - acc: 0.3645 - val_loss: 1.8601 - val_acc: 0.3738\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.8276 - acc: 0.3699 - val_loss: 1.8616 - val_acc: 0.3754\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 4s 33us/sample - loss: 1.8153 - acc: 0.3744 - val_loss: 1.8411 - val_acc: 0.3788\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.8060 - acc: 0.3770 - val_loss: 1.8111 - val_acc: 0.3864\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.7974 - acc: 0.3800 - val_loss: 1.8741 - val_acc: 0.3652\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.7910 - acc: 0.3823 - val_loss: 1.8053 - val_acc: 0.3948\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7846 - acc: 0.3833 - val_loss: 1.8166 - val_acc: 0.3896\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.7794 - acc: 0.3859 - val_loss: 1.8101 - val_acc: 0.3929\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.7738 - acc: 0.3875 - val_loss: 1.8308 - val_acc: 0.3892\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7697 - acc: 0.3895 - val_loss: 1.8417 - val_acc: 0.3779\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7643 - acc: 0.3900 - val_loss: 1.8139 - val_acc: 0.3901\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7616 - acc: 0.3909 - val_loss: 1.8325 - val_acc: 0.3795\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7566 - acc: 0.3926 - val_loss: 1.8363 - val_acc: 0.3784\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7537 - acc: 0.3939 - val_loss: 1.8345 - val_acc: 0.3833\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7493 - acc: 0.3948 - val_loss: 1.8441 - val_acc: 0.3717\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.7459 - acc: 0.3960 - val_loss: 1.8401 - val_acc: 0.3803\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7431 - acc: 0.3977 - val_loss: 1.8394 - val_acc: 0.3795\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.7398 - acc: 0.3990 - val_loss: 1.8198 - val_acc: 0.3874\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7367 - acc: 0.3981 - val_loss: 1.8303 - val_acc: 0.3816\n",
            "32890/32890 [==============================] - 1s 26us/sample - loss: 1.8356 - acc: 0.3680\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 8s 77us/sample - loss: 1.9494 - acc: 0.3310 - val_loss: 1.8590 - val_acc: 0.3486\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 4s 35us/sample - loss: 1.8516 - acc: 0.3603 - val_loss: 1.8451 - val_acc: 0.3593\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 4s 36us/sample - loss: 1.8309 - acc: 0.3667 - val_loss: 1.8170 - val_acc: 0.3624\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 4s 34us/sample - loss: 1.8174 - acc: 0.3714 - val_loss: 1.8185 - val_acc: 0.3612\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 4s 35us/sample - loss: 1.8067 - acc: 0.3753 - val_loss: 1.8101 - val_acc: 0.3679\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 4s 35us/sample - loss: 1.7997 - acc: 0.3771 - val_loss: 1.8069 - val_acc: 0.3640\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 4s 36us/sample - loss: 1.7922 - acc: 0.3803 - val_loss: 1.8003 - val_acc: 0.3693\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 4s 36us/sample - loss: 1.7861 - acc: 0.3804 - val_loss: 1.7929 - val_acc: 0.3742\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 4s 36us/sample - loss: 1.7808 - acc: 0.3828 - val_loss: 1.7961 - val_acc: 0.3721\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 4s 35us/sample - loss: 1.7752 - acc: 0.3853 - val_loss: 1.8218 - val_acc: 0.3639\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 4s 35us/sample - loss: 1.7720 - acc: 0.3852 - val_loss: 1.7958 - val_acc: 0.3756\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 4s 34us/sample - loss: 1.7662 - acc: 0.3876 - val_loss: 1.8139 - val_acc: 0.3643\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 4s 35us/sample - loss: 1.7626 - acc: 0.3879 - val_loss: 1.7991 - val_acc: 0.3748\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 4s 35us/sample - loss: 1.7596 - acc: 0.3898 - val_loss: 1.7854 - val_acc: 0.3778\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 4s 35us/sample - loss: 1.7547 - acc: 0.3906 - val_loss: 1.8219 - val_acc: 0.3624\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 4s 35us/sample - loss: 1.7510 - acc: 0.3921 - val_loss: 1.7944 - val_acc: 0.3778\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 4s 34us/sample - loss: 1.7484 - acc: 0.3929 - val_loss: 1.8021 - val_acc: 0.3721\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 4s 34us/sample - loss: 1.7441 - acc: 0.3947 - val_loss: 1.7959 - val_acc: 0.3718\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 4s 35us/sample - loss: 1.7418 - acc: 0.3959 - val_loss: 1.8143 - val_acc: 0.3670\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 4s 35us/sample - loss: 1.7386 - acc: 0.3974 - val_loss: 1.8037 - val_acc: 0.3718\n",
            "32889/32889 [==============================] - 1s 26us/sample - loss: 1.8165 - acc: 0.3912\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:143: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 9s 81us/sample - loss: 1.9925 - acc: 0.3096 - val_loss: 1.8583 - val_acc: 0.3793\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.8833 - acc: 0.3492 - val_loss: 1.8391 - val_acc: 0.3836\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.8644 - acc: 0.3565 - val_loss: 1.8461 - val_acc: 0.3826\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.8489 - acc: 0.3630 - val_loss: 1.8110 - val_acc: 0.3907\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.8362 - acc: 0.3659 - val_loss: 1.7999 - val_acc: 0.3925\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.8248 - acc: 0.3697 - val_loss: 1.8017 - val_acc: 0.3955\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.8154 - acc: 0.3729 - val_loss: 1.7951 - val_acc: 0.3960\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.8076 - acc: 0.3759 - val_loss: 1.8088 - val_acc: 0.3920\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.8029 - acc: 0.3770 - val_loss: 1.7947 - val_acc: 0.4003\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7977 - acc: 0.3802 - val_loss: 1.7773 - val_acc: 0.4008\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7915 - acc: 0.3812 - val_loss: 1.7726 - val_acc: 0.4056\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7873 - acc: 0.3836 - val_loss: 1.7925 - val_acc: 0.3959\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7840 - acc: 0.3851 - val_loss: 1.7840 - val_acc: 0.3983\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.7802 - acc: 0.3848 - val_loss: 1.7708 - val_acc: 0.4034\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7768 - acc: 0.3868 - val_loss: 1.7704 - val_acc: 0.4054\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7741 - acc: 0.3880 - val_loss: 1.7754 - val_acc: 0.4018\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7703 - acc: 0.3892 - val_loss: 1.7672 - val_acc: 0.4058\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7672 - acc: 0.3904 - val_loss: 1.7721 - val_acc: 0.4036\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7650 - acc: 0.3909 - val_loss: 1.7664 - val_acc: 0.4099\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7623 - acc: 0.3913 - val_loss: 1.7696 - val_acc: 0.4017\n",
            "32890/32890 [==============================] - 1s 26us/sample - loss: 1.8386 - acc: 0.3660\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 8s 78us/sample - loss: 1.9686 - acc: 0.3198 - val_loss: 1.8921 - val_acc: 0.3638\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.8636 - acc: 0.3581 - val_loss: 1.8291 - val_acc: 0.3826\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.8458 - acc: 0.3648 - val_loss: 1.8419 - val_acc: 0.3802\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.8316 - acc: 0.3706 - val_loss: 1.8163 - val_acc: 0.3922\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.8181 - acc: 0.3762 - val_loss: 1.8056 - val_acc: 0.3958\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.8088 - acc: 0.3794 - val_loss: 1.7980 - val_acc: 0.3954\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.8009 - acc: 0.3817 - val_loss: 1.8311 - val_acc: 0.3794\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7940 - acc: 0.3844 - val_loss: 1.7848 - val_acc: 0.3989\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7884 - acc: 0.3862 - val_loss: 1.8235 - val_acc: 0.3807\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7833 - acc: 0.3881 - val_loss: 1.7739 - val_acc: 0.4007\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7780 - acc: 0.3908 - val_loss: 1.8050 - val_acc: 0.3910\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7719 - acc: 0.3929 - val_loss: 1.8002 - val_acc: 0.3881\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7687 - acc: 0.3919 - val_loss: 1.8013 - val_acc: 0.3929\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7642 - acc: 0.3961 - val_loss: 1.7929 - val_acc: 0.3957\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7601 - acc: 0.3959 - val_loss: 1.7891 - val_acc: 0.3942\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7566 - acc: 0.3963 - val_loss: 1.7715 - val_acc: 0.4025\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7529 - acc: 0.3980 - val_loss: 1.7827 - val_acc: 0.3984\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.7499 - acc: 0.3985 - val_loss: 1.7795 - val_acc: 0.4007\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7465 - acc: 0.3997 - val_loss: 1.7865 - val_acc: 0.4002\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7446 - acc: 0.4015 - val_loss: 1.8011 - val_acc: 0.3914\n",
            "32890/32890 [==============================] - 1s 26us/sample - loss: 1.8754 - acc: 0.3465\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 9s 82us/sample - loss: 1.9957 - acc: 0.3062 - val_loss: 1.9020 - val_acc: 0.3574\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.8834 - acc: 0.3474 - val_loss: 1.8613 - val_acc: 0.3700\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.8628 - acc: 0.3556 - val_loss: 1.8522 - val_acc: 0.3798\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.8499 - acc: 0.3595 - val_loss: 1.8597 - val_acc: 0.3723\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.8413 - acc: 0.3613 - val_loss: 1.8430 - val_acc: 0.3795\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.8320 - acc: 0.3664 - val_loss: 1.8226 - val_acc: 0.3818\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.8235 - acc: 0.3691 - val_loss: 1.8062 - val_acc: 0.3921\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.8170 - acc: 0.3707 - val_loss: 1.8125 - val_acc: 0.3881\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.8105 - acc: 0.3735 - val_loss: 1.7919 - val_acc: 0.3952\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.8048 - acc: 0.3735 - val_loss: 1.8018 - val_acc: 0.3899\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.8000 - acc: 0.3766 - val_loss: 1.8127 - val_acc: 0.3815\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7953 - acc: 0.3782 - val_loss: 1.8053 - val_acc: 0.3878\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7899 - acc: 0.3809 - val_loss: 1.8170 - val_acc: 0.3836\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7873 - acc: 0.3812 - val_loss: 1.7913 - val_acc: 0.3984\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7837 - acc: 0.3828 - val_loss: 1.8224 - val_acc: 0.3821\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7803 - acc: 0.3844 - val_loss: 1.8021 - val_acc: 0.3843\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7757 - acc: 0.3858 - val_loss: 1.7821 - val_acc: 0.3998\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7729 - acc: 0.3861 - val_loss: 1.7869 - val_acc: 0.3942\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7700 - acc: 0.3855 - val_loss: 1.7856 - val_acc: 0.3910\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7681 - acc: 0.3884 - val_loss: 1.7886 - val_acc: 0.3933\n",
            "32890/32890 [==============================] - 1s 26us/sample - loss: 1.7765 - acc: 0.3889\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 8s 79us/sample - loss: 1.9809 - acc: 0.3153 - val_loss: 1.8727 - val_acc: 0.3731\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.8765 - acc: 0.3526 - val_loss: 1.8800 - val_acc: 0.3679\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.8615 - acc: 0.3576 - val_loss: 1.8632 - val_acc: 0.3743\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.8484 - acc: 0.3640 - val_loss: 1.8731 - val_acc: 0.3655\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.8355 - acc: 0.3670 - val_loss: 1.8420 - val_acc: 0.3811\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.8254 - acc: 0.3711 - val_loss: 1.8469 - val_acc: 0.3747\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.8162 - acc: 0.3745 - val_loss: 1.8448 - val_acc: 0.3776\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.8076 - acc: 0.3769 - val_loss: 1.8734 - val_acc: 0.3679\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.8021 - acc: 0.3796 - val_loss: 1.8068 - val_acc: 0.3909\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7966 - acc: 0.3809 - val_loss: 1.8207 - val_acc: 0.3883\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7921 - acc: 0.3843 - val_loss: 1.8193 - val_acc: 0.3868\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7880 - acc: 0.3845 - val_loss: 1.8081 - val_acc: 0.3921\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7833 - acc: 0.3851 - val_loss: 1.8117 - val_acc: 0.3939\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7805 - acc: 0.3867 - val_loss: 1.8300 - val_acc: 0.3823\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7760 - acc: 0.3882 - val_loss: 1.8098 - val_acc: 0.3929\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7732 - acc: 0.3892 - val_loss: 1.8573 - val_acc: 0.3725\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7704 - acc: 0.3897 - val_loss: 1.8252 - val_acc: 0.3913\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7672 - acc: 0.3916 - val_loss: 1.8168 - val_acc: 0.3889\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7655 - acc: 0.3920 - val_loss: 1.8153 - val_acc: 0.3880\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7614 - acc: 0.3927 - val_loss: 1.8217 - val_acc: 0.3853\n",
            "32890/32890 [==============================] - 1s 27us/sample - loss: 1.8180 - acc: 0.3739\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 9s 83us/sample - loss: 2.0011 - acc: 0.3074 - val_loss: 1.8588 - val_acc: 0.3543\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 4s 37us/sample - loss: 1.8855 - acc: 0.3490 - val_loss: 1.8589 - val_acc: 0.3466\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 4s 36us/sample - loss: 1.8677 - acc: 0.3557 - val_loss: 1.8723 - val_acc: 0.3455\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 4s 36us/sample - loss: 1.8535 - acc: 0.3608 - val_loss: 1.8477 - val_acc: 0.3574\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 4s 36us/sample - loss: 1.8406 - acc: 0.3645 - val_loss: 1.8511 - val_acc: 0.3539\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 4s 36us/sample - loss: 1.8313 - acc: 0.3679 - val_loss: 1.8258 - val_acc: 0.3650\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 4s 36us/sample - loss: 1.8227 - acc: 0.3709 - val_loss: 1.8179 - val_acc: 0.3636\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 4s 36us/sample - loss: 1.8140 - acc: 0.3740 - val_loss: 1.8096 - val_acc: 0.3666\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 4s 36us/sample - loss: 1.8070 - acc: 0.3760 - val_loss: 1.8189 - val_acc: 0.3649\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 4s 36us/sample - loss: 1.8015 - acc: 0.3775 - val_loss: 1.7998 - val_acc: 0.3757\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 4s 38us/sample - loss: 1.7957 - acc: 0.3788 - val_loss: 1.7921 - val_acc: 0.3779\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 4s 39us/sample - loss: 1.7920 - acc: 0.3808 - val_loss: 1.7863 - val_acc: 0.3785\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 4s 38us/sample - loss: 1.7874 - acc: 0.3816 - val_loss: 1.7943 - val_acc: 0.3747\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 4s 37us/sample - loss: 1.7830 - acc: 0.3832 - val_loss: 1.7958 - val_acc: 0.3705\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 4s 37us/sample - loss: 1.7808 - acc: 0.3830 - val_loss: 1.7822 - val_acc: 0.3794\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 4s 37us/sample - loss: 1.7762 - acc: 0.3852 - val_loss: 1.7936 - val_acc: 0.3779\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 4s 38us/sample - loss: 1.7724 - acc: 0.3867 - val_loss: 1.7849 - val_acc: 0.3757\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 4s 38us/sample - loss: 1.7705 - acc: 0.3870 - val_loss: 1.8014 - val_acc: 0.3684\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 4s 36us/sample - loss: 1.7667 - acc: 0.3893 - val_loss: 1.7988 - val_acc: 0.3710\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 4s 37us/sample - loss: 1.7638 - acc: 0.3897 - val_loss: 1.7775 - val_acc: 0.3856\n",
            "32889/32889 [==============================] - 1s 27us/sample - loss: 1.8157 - acc: 0.3961\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 9s 82us/sample - loss: 2.4516 - acc: 0.1216 - val_loss: 2.4739 - val_acc: 0.0609\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 2.4300 - acc: 0.1225 - val_loss: 2.4752 - val_acc: 0.0609\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 2.4257 - acc: 0.1225 - val_loss: 2.4742 - val_acc: 0.0609\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 2.4245 - acc: 0.1225 - val_loss: 2.4742 - val_acc: 0.0609\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 2.4241 - acc: 0.1218 - val_loss: 2.4750 - val_acc: 0.0609\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 2.4240 - acc: 0.1225 - val_loss: 2.4737 - val_acc: 0.0609\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 2.4240 - acc: 0.1225 - val_loss: 2.4744 - val_acc: 0.0609\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 2.4240 - acc: 0.1225 - val_loss: 2.4752 - val_acc: 0.0609\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 4s 41us/sample - loss: 2.4240 - acc: 0.1224 - val_loss: 2.4736 - val_acc: 0.1306\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 2.4240 - acc: 0.1225 - val_loss: 2.4749 - val_acc: 0.0609\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 2.4240 - acc: 0.1225 - val_loss: 2.4744 - val_acc: 0.0609\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 2.4240 - acc: 0.1222 - val_loss: 2.4735 - val_acc: 0.0609\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 2.4240 - acc: 0.1221 - val_loss: 2.4733 - val_acc: 0.0609\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 2.4240 - acc: 0.1225 - val_loss: 2.4742 - val_acc: 0.0609\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 2.4240 - acc: 0.1217 - val_loss: 2.4743 - val_acc: 0.0609\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 2.4240 - acc: 0.1225 - val_loss: 2.4749 - val_acc: 0.0609\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 2.4240 - acc: 0.1225 - val_loss: 2.4746 - val_acc: 0.0609\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 2.4240 - acc: 0.1221 - val_loss: 2.4739 - val_acc: 0.0609\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 2.4240 - acc: 0.1221 - val_loss: 2.4743 - val_acc: 0.0609\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 2.4240 - acc: 0.1225 - val_loss: 2.4741 - val_acc: 0.0609\n",
            "32890/32890 [==============================] - 1s 29us/sample - loss: 2.3666 - acc: 0.1687\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 9s 86us/sample - loss: 2.4412 - acc: 0.1389 - val_loss: 2.4811 - val_acc: 0.0609\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 2.4108 - acc: 0.1405 - val_loss: 2.4915 - val_acc: 0.0609\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 2.4053 - acc: 0.1405 - val_loss: 2.4927 - val_acc: 0.0609\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 2.4038 - acc: 0.1405 - val_loss: 2.4939 - val_acc: 0.0609\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 2.4033 - acc: 0.1405 - val_loss: 2.4922 - val_acc: 0.0609\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 2.4032 - acc: 0.1405 - val_loss: 2.4938 - val_acc: 0.0609\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 2.4032 - acc: 0.1405 - val_loss: 2.4939 - val_acc: 0.0609\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 2.4032 - acc: 0.1405 - val_loss: 2.4931 - val_acc: 0.0609\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 2.4032 - acc: 0.1405 - val_loss: 2.4939 - val_acc: 0.0609\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 2.4032 - acc: 0.1405 - val_loss: 2.4929 - val_acc: 0.0609\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 2.4032 - acc: 0.1405 - val_loss: 2.4930 - val_acc: 0.0609\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 2.4032 - acc: 0.1405 - val_loss: 2.4933 - val_acc: 0.0609\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 2.4032 - acc: 0.1405 - val_loss: 2.4932 - val_acc: 0.0609\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 2.4032 - acc: 0.1405 - val_loss: 2.4944 - val_acc: 0.0609\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 2.4032 - acc: 0.1405 - val_loss: 2.4929 - val_acc: 0.0609\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 2.4032 - acc: 0.1405 - val_loss: 2.4932 - val_acc: 0.0609\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 2.4032 - acc: 0.1405 - val_loss: 2.4931 - val_acc: 0.0609\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 2.4032 - acc: 0.1405 - val_loss: 2.4921 - val_acc: 0.0609\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 2.4032 - acc: 0.1405 - val_loss: 2.4927 - val_acc: 0.0609\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 2.4032 - acc: 0.1405 - val_loss: 2.4930 - val_acc: 0.0609\n",
            "32890/32890 [==============================] - 1s 28us/sample - loss: 2.4333 - acc: 0.1111\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 9s 84us/sample - loss: 2.4408 - acc: 0.1347 - val_loss: 2.4879 - val_acc: 0.0609\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 4s 41us/sample - loss: 2.4103 - acc: 0.1397 - val_loss: 2.5031 - val_acc: 0.0609\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 2.4049 - acc: 0.1397 - val_loss: 2.5064 - val_acc: 0.0609\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 2.4033 - acc: 0.1397 - val_loss: 2.5051 - val_acc: 0.0609\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 4s 41us/sample - loss: 2.4029 - acc: 0.1397 - val_loss: 2.5046 - val_acc: 0.0609\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 2.4028 - acc: 0.1397 - val_loss: 2.5046 - val_acc: 0.0609\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 2.4027 - acc: 0.1397 - val_loss: 2.5069 - val_acc: 0.0609\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 2.4027 - acc: 0.1397 - val_loss: 2.5050 - val_acc: 0.0609\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 2.4027 - acc: 0.1397 - val_loss: 2.5061 - val_acc: 0.0609\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 2.4027 - acc: 0.1397 - val_loss: 2.5066 - val_acc: 0.0609\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 2.4027 - acc: 0.1397 - val_loss: 2.5067 - val_acc: 0.0609\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 2.4027 - acc: 0.1397 - val_loss: 2.5057 - val_acc: 0.0609\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 4s 41us/sample - loss: 2.4027 - acc: 0.1397 - val_loss: 2.5042 - val_acc: 0.0609\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 2.4027 - acc: 0.1397 - val_loss: 2.5057 - val_acc: 0.0609\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 2.4027 - acc: 0.1397 - val_loss: 2.5035 - val_acc: 0.0609\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 4s 42us/sample - loss: 2.4027 - acc: 0.1397 - val_loss: 2.5045 - val_acc: 0.0609\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 2.4027 - acc: 0.1397 - val_loss: 2.5052 - val_acc: 0.0609\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 2.4027 - acc: 0.1397 - val_loss: 2.5056 - val_acc: 0.0609\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 2.4027 - acc: 0.1397 - val_loss: 2.5046 - val_acc: 0.0609\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 2.4027 - acc: 0.1397 - val_loss: 2.5057 - val_acc: 0.0609\n",
            "32890/32890 [==============================] - 1s 28us/sample - loss: 2.4311 - acc: 0.1138\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 9s 83us/sample - loss: 2.4370 - acc: 0.1402 - val_loss: 2.4866 - val_acc: 0.0609\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 2.4038 - acc: 0.1403 - val_loss: 2.4996 - val_acc: 0.0609\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 2.3977 - acc: 0.1403 - val_loss: 2.5016 - val_acc: 0.0609\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 2.3960 - acc: 0.1403 - val_loss: 2.5013 - val_acc: 0.0609\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 2.3955 - acc: 0.1403 - val_loss: 2.5027 - val_acc: 0.0609\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 2.3953 - acc: 0.1403 - val_loss: 2.5029 - val_acc: 0.0609\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 2.3953 - acc: 0.1403 - val_loss: 2.5029 - val_acc: 0.0609\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 2.3953 - acc: 0.1403 - val_loss: 2.5025 - val_acc: 0.0609\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 2.3953 - acc: 0.1403 - val_loss: 2.5035 - val_acc: 0.0609\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 2.3953 - acc: 0.1403 - val_loss: 2.5028 - val_acc: 0.0609\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 2.3953 - acc: 0.1403 - val_loss: 2.5030 - val_acc: 0.0609\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 2.3953 - acc: 0.1403 - val_loss: 2.5036 - val_acc: 0.0609\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 2.3953 - acc: 0.1403 - val_loss: 2.5036 - val_acc: 0.0609\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 2.3953 - acc: 0.1403 - val_loss: 2.5033 - val_acc: 0.0609\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 2.3952 - acc: 0.1403 - val_loss: 2.5031 - val_acc: 0.0609\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 2.3953 - acc: 0.1403 - val_loss: 2.5027 - val_acc: 0.0609\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 2.3953 - acc: 0.1403 - val_loss: 2.5029 - val_acc: 0.0609\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 2.3952 - acc: 0.1403 - val_loss: 2.5024 - val_acc: 0.0609\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 2.3953 - acc: 0.1403 - val_loss: 2.5015 - val_acc: 0.0609\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 2.3953 - acc: 0.1403 - val_loss: 2.5032 - val_acc: 0.0609\n",
            "32890/32890 [==============================] - 1s 27us/sample - loss: 2.4525 - acc: 0.1120\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 9s 83us/sample - loss: 2.4388 - acc: 0.1385 - val_loss: 2.4484 - val_acc: 0.0694\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 4s 38us/sample - loss: 2.4069 - acc: 0.1407 - val_loss: 2.4473 - val_acc: 0.0694\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 4s 39us/sample - loss: 2.4009 - acc: 0.1407 - val_loss: 2.4456 - val_acc: 0.0694\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 4s 38us/sample - loss: 2.3990 - acc: 0.1407 - val_loss: 2.4425 - val_acc: 0.0694\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 4s 37us/sample - loss: 2.3985 - acc: 0.1407 - val_loss: 2.4413 - val_acc: 0.0694\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 4s 37us/sample - loss: 2.3983 - acc: 0.1407 - val_loss: 2.4412 - val_acc: 0.0694\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 4s 37us/sample - loss: 2.3983 - acc: 0.1407 - val_loss: 2.4412 - val_acc: 0.0694\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 4s 37us/sample - loss: 2.3982 - acc: 0.1407 - val_loss: 2.4416 - val_acc: 0.0694\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 4s 37us/sample - loss: 2.3982 - acc: 0.1407 - val_loss: 2.4413 - val_acc: 0.0694\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 4s 38us/sample - loss: 2.3982 - acc: 0.1407 - val_loss: 2.4424 - val_acc: 0.0694\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 4s 37us/sample - loss: 2.3982 - acc: 0.1407 - val_loss: 2.4418 - val_acc: 0.0694\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 4s 37us/sample - loss: 2.3982 - acc: 0.1407 - val_loss: 2.4413 - val_acc: 0.0694\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 4s 37us/sample - loss: 2.3982 - acc: 0.1407 - val_loss: 2.4418 - val_acc: 0.0694\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 4s 39us/sample - loss: 2.3982 - acc: 0.1407 - val_loss: 2.4415 - val_acc: 0.0694\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 4s 37us/sample - loss: 2.3982 - acc: 0.1407 - val_loss: 2.4419 - val_acc: 0.0694\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 4s 37us/sample - loss: 2.3982 - acc: 0.1407 - val_loss: 2.4422 - val_acc: 0.0694\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 4s 39us/sample - loss: 2.3982 - acc: 0.1407 - val_loss: 2.4423 - val_acc: 0.0694\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 4s 39us/sample - loss: 2.3982 - acc: 0.1407 - val_loss: 2.4415 - val_acc: 0.0694\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 4s 39us/sample - loss: 2.3982 - acc: 0.1407 - val_loss: 2.4405 - val_acc: 0.0694\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 4s 37us/sample - loss: 2.3982 - acc: 0.1407 - val_loss: 2.4417 - val_acc: 0.0694\n",
            "32889/32889 [==============================] - 1s 27us/sample - loss: 2.4888 - acc: 0.1040\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 9s 84us/sample - loss: 1.9315 - acc: 0.3371 - val_loss: 1.8141 - val_acc: 0.3870\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.8422 - acc: 0.3653 - val_loss: 1.8099 - val_acc: 0.3852\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.8201 - acc: 0.3716 - val_loss: 1.8120 - val_acc: 0.3870\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.8065 - acc: 0.3763 - val_loss: 1.7801 - val_acc: 0.3996\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.7960 - acc: 0.3787 - val_loss: 1.7850 - val_acc: 0.3990\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7876 - acc: 0.3828 - val_loss: 1.7884 - val_acc: 0.3986\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7807 - acc: 0.3852 - val_loss: 1.7875 - val_acc: 0.3972\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7760 - acc: 0.3859 - val_loss: 1.7848 - val_acc: 0.4012\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.7691 - acc: 0.3890 - val_loss: 1.7932 - val_acc: 0.3954\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7644 - acc: 0.3897 - val_loss: 1.7767 - val_acc: 0.4018\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7599 - acc: 0.3906 - val_loss: 1.7970 - val_acc: 0.3952\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7548 - acc: 0.3935 - val_loss: 1.7800 - val_acc: 0.4033\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7521 - acc: 0.3947 - val_loss: 1.7765 - val_acc: 0.4059\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7482 - acc: 0.3956 - val_loss: 1.7831 - val_acc: 0.3993\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7445 - acc: 0.3976 - val_loss: 1.7697 - val_acc: 0.4056\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7417 - acc: 0.3977 - val_loss: 1.7725 - val_acc: 0.4064\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.7380 - acc: 0.3978 - val_loss: 1.7905 - val_acc: 0.3941\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7365 - acc: 0.3982 - val_loss: 1.8146 - val_acc: 0.3878\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7321 - acc: 0.3997 - val_loss: 1.7817 - val_acc: 0.4013\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.7301 - acc: 0.4011 - val_loss: 1.7900 - val_acc: 0.3994\n",
            "32890/32890 [==============================] - 1s 29us/sample - loss: 1.8526 - acc: 0.3621\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 9s 85us/sample - loss: 1.9214 - acc: 0.3447 - val_loss: 1.8548 - val_acc: 0.3664\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.8251 - acc: 0.3735 - val_loss: 1.8209 - val_acc: 0.3846\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.8046 - acc: 0.3808 - val_loss: 1.8236 - val_acc: 0.3796\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.7911 - acc: 0.3856 - val_loss: 1.8017 - val_acc: 0.3855\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.7823 - acc: 0.3894 - val_loss: 1.8078 - val_acc: 0.3869\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.7730 - acc: 0.3915 - val_loss: 1.8001 - val_acc: 0.3881\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.7658 - acc: 0.3933 - val_loss: 1.7951 - val_acc: 0.3894\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7613 - acc: 0.3958 - val_loss: 1.7919 - val_acc: 0.3959\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7557 - acc: 0.3975 - val_loss: 1.7701 - val_acc: 0.4018\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7506 - acc: 0.3981 - val_loss: 1.7792 - val_acc: 0.4002\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.7464 - acc: 0.3994 - val_loss: 1.7792 - val_acc: 0.4019\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 4s 41us/sample - loss: 1.7425 - acc: 0.4012 - val_loss: 1.7924 - val_acc: 0.3929\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7375 - acc: 0.4016 - val_loss: 1.7774 - val_acc: 0.4018\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 1.7335 - acc: 0.4039 - val_loss: 1.8070 - val_acc: 0.3908\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.7309 - acc: 0.4033 - val_loss: 1.7731 - val_acc: 0.4033\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7273 - acc: 0.4048 - val_loss: 1.7809 - val_acc: 0.3993\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7236 - acc: 0.4058 - val_loss: 1.8067 - val_acc: 0.3872\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7206 - acc: 0.4069 - val_loss: 1.7773 - val_acc: 0.4038\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.7184 - acc: 0.4083 - val_loss: 1.7780 - val_acc: 0.3973\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7146 - acc: 0.4089 - val_loss: 1.7877 - val_acc: 0.3976\n",
            "32890/32890 [==============================] - 1s 28us/sample - loss: 1.8839 - acc: 0.3457\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 9s 85us/sample - loss: 1.9492 - acc: 0.3297 - val_loss: 1.8409 - val_acc: 0.3791\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.8535 - acc: 0.3593 - val_loss: 1.8483 - val_acc: 0.3747\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 1.8304 - acc: 0.3675 - val_loss: 1.8459 - val_acc: 0.3740\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.8175 - acc: 0.3707 - val_loss: 1.8298 - val_acc: 0.3757\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.8069 - acc: 0.3741 - val_loss: 1.8076 - val_acc: 0.3870\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7975 - acc: 0.3782 - val_loss: 1.8135 - val_acc: 0.3834\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7905 - acc: 0.3806 - val_loss: 1.8095 - val_acc: 0.3840\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7858 - acc: 0.3809 - val_loss: 1.8091 - val_acc: 0.3847\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.7801 - acc: 0.3828 - val_loss: 1.8200 - val_acc: 0.3796\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.7758 - acc: 0.3844 - val_loss: 1.8258 - val_acc: 0.3758\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7709 - acc: 0.3859 - val_loss: 1.8038 - val_acc: 0.3893\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7671 - acc: 0.3884 - val_loss: 1.8138 - val_acc: 0.3812\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7640 - acc: 0.3884 - val_loss: 1.8192 - val_acc: 0.3788\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7601 - acc: 0.3899 - val_loss: 1.8280 - val_acc: 0.3779\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.7565 - acc: 0.3908 - val_loss: 1.8155 - val_acc: 0.3845\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.7533 - acc: 0.3924 - val_loss: 1.8091 - val_acc: 0.3859\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.7508 - acc: 0.3937 - val_loss: 1.8256 - val_acc: 0.3779\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7476 - acc: 0.3935 - val_loss: 1.8048 - val_acc: 0.3883\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.7441 - acc: 0.3954 - val_loss: 1.8183 - val_acc: 0.3788\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.7427 - acc: 0.3952 - val_loss: 1.8212 - val_acc: 0.3775\n",
            "32890/32890 [==============================] - 1s 28us/sample - loss: 1.8043 - acc: 0.3772\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 9s 86us/sample - loss: 1.9445 - acc: 0.3300 - val_loss: 1.8972 - val_acc: 0.3575\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.8436 - acc: 0.3652 - val_loss: 1.8431 - val_acc: 0.3782\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.8249 - acc: 0.3712 - val_loss: 1.8206 - val_acc: 0.3860\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.8104 - acc: 0.3756 - val_loss: 1.8683 - val_acc: 0.3717\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.7992 - acc: 0.3797 - val_loss: 1.8241 - val_acc: 0.3842\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7910 - acc: 0.3822 - val_loss: 1.8134 - val_acc: 0.3912\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7846 - acc: 0.3845 - val_loss: 1.8220 - val_acc: 0.3846\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7787 - acc: 0.3865 - val_loss: 1.8173 - val_acc: 0.3847\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 1.7730 - acc: 0.3879 - val_loss: 1.8190 - val_acc: 0.3886\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.7684 - acc: 0.3909 - val_loss: 1.8096 - val_acc: 0.3924\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7640 - acc: 0.3916 - val_loss: 1.8211 - val_acc: 0.3880\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7590 - acc: 0.3934 - val_loss: 1.8174 - val_acc: 0.3912\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 1.7549 - acc: 0.3931 - val_loss: 1.8044 - val_acc: 0.3932\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 1.7510 - acc: 0.3948 - val_loss: 1.8117 - val_acc: 0.3908\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7477 - acc: 0.3953 - val_loss: 1.8451 - val_acc: 0.3780\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7445 - acc: 0.3972 - val_loss: 1.8179 - val_acc: 0.3884\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7413 - acc: 0.3979 - val_loss: 1.8249 - val_acc: 0.3861\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.7383 - acc: 0.3996 - val_loss: 1.8130 - val_acc: 0.3928\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7358 - acc: 0.3997 - val_loss: 1.8373 - val_acc: 0.3805\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7323 - acc: 0.4013 - val_loss: 1.8207 - val_acc: 0.3850\n",
            "32890/32890 [==============================] - 1s 28us/sample - loss: 1.8331 - acc: 0.3671\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 9s 84us/sample - loss: 1.9279 - acc: 0.3375 - val_loss: 1.8470 - val_acc: 0.3542\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 4s 36us/sample - loss: 1.8451 - acc: 0.3619 - val_loss: 1.8357 - val_acc: 0.3584\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 4s 38us/sample - loss: 1.8253 - acc: 0.3691 - val_loss: 1.8012 - val_acc: 0.3735\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 4s 38us/sample - loss: 1.8139 - acc: 0.3712 - val_loss: 1.8296 - val_acc: 0.3585\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 4s 37us/sample - loss: 1.8038 - acc: 0.3750 - val_loss: 1.8024 - val_acc: 0.3710\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 4s 37us/sample - loss: 1.7963 - acc: 0.3765 - val_loss: 1.8010 - val_acc: 0.3712\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 4s 35us/sample - loss: 1.7894 - acc: 0.3791 - val_loss: 1.8032 - val_acc: 0.3684\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 4s 37us/sample - loss: 1.7837 - acc: 0.3810 - val_loss: 1.7984 - val_acc: 0.3725\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 4s 36us/sample - loss: 1.7773 - acc: 0.3827 - val_loss: 1.7955 - val_acc: 0.3709\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 4s 36us/sample - loss: 1.7718 - acc: 0.3855 - val_loss: 1.7872 - val_acc: 0.3785\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 4s 37us/sample - loss: 1.7673 - acc: 0.3864 - val_loss: 1.8239 - val_acc: 0.3665\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 4s 36us/sample - loss: 1.7630 - acc: 0.3872 - val_loss: 1.7953 - val_acc: 0.3778\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 4s 36us/sample - loss: 1.7586 - acc: 0.3902 - val_loss: 1.7754 - val_acc: 0.3812\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 4s 35us/sample - loss: 1.7538 - acc: 0.3904 - val_loss: 1.8003 - val_acc: 0.3720\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 4s 36us/sample - loss: 1.7519 - acc: 0.3916 - val_loss: 1.7908 - val_acc: 0.3798\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 4s 39us/sample - loss: 1.7471 - acc: 0.3935 - val_loss: 1.8030 - val_acc: 0.3704\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 4s 38us/sample - loss: 1.7439 - acc: 0.3937 - val_loss: 1.7813 - val_acc: 0.3823\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 4s 38us/sample - loss: 1.7403 - acc: 0.3951 - val_loss: 1.7856 - val_acc: 0.3792\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 4s 40us/sample - loss: 1.7386 - acc: 0.3966 - val_loss: 1.8250 - val_acc: 0.3676\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 4s 40us/sample - loss: 1.7343 - acc: 0.3971 - val_loss: 1.7906 - val_acc: 0.3796\n",
            "32889/32889 [==============================] - 1s 27us/sample - loss: 1.8251 - acc: 0.3877\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 9s 82us/sample - loss: 1.9330 - acc: 0.3386 - val_loss: 1.8200 - val_acc: 0.3923\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.8411 - acc: 0.3665 - val_loss: 1.8042 - val_acc: 0.3899\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.8180 - acc: 0.3731 - val_loss: 1.7999 - val_acc: 0.3872\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.8061 - acc: 0.3770 - val_loss: 1.7863 - val_acc: 0.3996\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7967 - acc: 0.3800 - val_loss: 1.7956 - val_acc: 0.3947\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7900 - acc: 0.3829 - val_loss: 1.8014 - val_acc: 0.3929\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7825 - acc: 0.3850 - val_loss: 1.7897 - val_acc: 0.3974\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7777 - acc: 0.3857 - val_loss: 1.7702 - val_acc: 0.4060\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7714 - acc: 0.3881 - val_loss: 1.7645 - val_acc: 0.4087\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7677 - acc: 0.3892 - val_loss: 1.7745 - val_acc: 0.4026\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.7632 - acc: 0.3912 - val_loss: 1.7777 - val_acc: 0.4023\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.7604 - acc: 0.3917 - val_loss: 1.7948 - val_acc: 0.3977\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7568 - acc: 0.3928 - val_loss: 1.7723 - val_acc: 0.4066\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7523 - acc: 0.3925 - val_loss: 1.7844 - val_acc: 0.4000\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7493 - acc: 0.3950 - val_loss: 1.8000 - val_acc: 0.3913\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7467 - acc: 0.3961 - val_loss: 1.7741 - val_acc: 0.4054\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7431 - acc: 0.3968 - val_loss: 1.7802 - val_acc: 0.4022\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7405 - acc: 0.3978 - val_loss: 1.7743 - val_acc: 0.4059\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7379 - acc: 0.3992 - val_loss: 1.7686 - val_acc: 0.4072\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7353 - acc: 0.3995 - val_loss: 1.7854 - val_acc: 0.3964\n",
            "32890/32890 [==============================] - 1s 26us/sample - loss: 1.8350 - acc: 0.3712\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 8s 80us/sample - loss: 1.9080 - acc: 0.3491 - val_loss: 1.8223 - val_acc: 0.3856\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.8244 - acc: 0.3746 - val_loss: 1.8435 - val_acc: 0.3751\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.8037 - acc: 0.3814 - val_loss: 1.8021 - val_acc: 0.3911\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7901 - acc: 0.3863 - val_loss: 1.8154 - val_acc: 0.3820\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7810 - acc: 0.3878 - val_loss: 1.7925 - val_acc: 0.3952\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7723 - acc: 0.3916 - val_loss: 1.8224 - val_acc: 0.3782\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7658 - acc: 0.3933 - val_loss: 1.8073 - val_acc: 0.3886\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7595 - acc: 0.3947 - val_loss: 1.8054 - val_acc: 0.3873\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7544 - acc: 0.3968 - val_loss: 1.7996 - val_acc: 0.3946\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7515 - acc: 0.3982 - val_loss: 1.7908 - val_acc: 0.3925\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7453 - acc: 0.3981 - val_loss: 1.7867 - val_acc: 0.3922\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7421 - acc: 0.4006 - val_loss: 1.7858 - val_acc: 0.3917\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7373 - acc: 0.4021 - val_loss: 1.7903 - val_acc: 0.3923\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7340 - acc: 0.4033 - val_loss: 1.7997 - val_acc: 0.3869\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7312 - acc: 0.4041 - val_loss: 1.7915 - val_acc: 0.3916\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7274 - acc: 0.4047 - val_loss: 1.7789 - val_acc: 0.4024\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7238 - acc: 0.4058 - val_loss: 1.8023 - val_acc: 0.3924\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7207 - acc: 0.4079 - val_loss: 1.8095 - val_acc: 0.3889\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.7177 - acc: 0.4084 - val_loss: 1.7974 - val_acc: 0.3903\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7144 - acc: 0.4084 - val_loss: 1.7917 - val_acc: 0.3961\n",
            "32890/32890 [==============================] - 1s 26us/sample - loss: 1.8854 - acc: 0.3457\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 8s 81us/sample - loss: 1.9476 - acc: 0.3341 - val_loss: 1.8568 - val_acc: 0.3714\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.8521 - acc: 0.3596 - val_loss: 1.8274 - val_acc: 0.3768\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.8304 - acc: 0.3666 - val_loss: 1.8136 - val_acc: 0.3836\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.8166 - acc: 0.3712 - val_loss: 1.8248 - val_acc: 0.3790\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.8075 - acc: 0.3741 - val_loss: 1.8207 - val_acc: 0.3846\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7988 - acc: 0.3781 - val_loss: 1.8070 - val_acc: 0.3890\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.7931 - acc: 0.3785 - val_loss: 1.8032 - val_acc: 0.3864\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7869 - acc: 0.3813 - val_loss: 1.8170 - val_acc: 0.3823\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7823 - acc: 0.3821 - val_loss: 1.8067 - val_acc: 0.3869\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7769 - acc: 0.3837 - val_loss: 1.8264 - val_acc: 0.3753\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7737 - acc: 0.3846 - val_loss: 1.8345 - val_acc: 0.3746\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.7695 - acc: 0.3874 - val_loss: 1.7952 - val_acc: 0.3905\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7651 - acc: 0.3883 - val_loss: 1.7956 - val_acc: 0.3937\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7615 - acc: 0.3900 - val_loss: 1.8128 - val_acc: 0.3810\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7580 - acc: 0.3907 - val_loss: 1.8014 - val_acc: 0.3904\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7548 - acc: 0.3904 - val_loss: 1.8138 - val_acc: 0.3836\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7512 - acc: 0.3927 - val_loss: 1.8213 - val_acc: 0.3812\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7487 - acc: 0.3932 - val_loss: 1.8092 - val_acc: 0.3859\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7454 - acc: 0.3937 - val_loss: 1.8088 - val_acc: 0.3903\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7439 - acc: 0.3942 - val_loss: 1.8095 - val_acc: 0.3848\n",
            "32890/32890 [==============================] - 1s 26us/sample - loss: 1.7873 - acc: 0.3852\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 9s 81us/sample - loss: 1.9346 - acc: 0.3373 - val_loss: 1.8722 - val_acc: 0.3756\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.8448 - acc: 0.3637 - val_loss: 1.8478 - val_acc: 0.3780\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.8231 - acc: 0.3730 - val_loss: 1.8397 - val_acc: 0.3767\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.8093 - acc: 0.3749 - val_loss: 1.8168 - val_acc: 0.3899\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7984 - acc: 0.3798 - val_loss: 1.8015 - val_acc: 0.3977\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7893 - acc: 0.3821 - val_loss: 1.8325 - val_acc: 0.3856\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7832 - acc: 0.3844 - val_loss: 1.8121 - val_acc: 0.3913\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7765 - acc: 0.3871 - val_loss: 1.8043 - val_acc: 0.3936\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7711 - acc: 0.3877 - val_loss: 1.8248 - val_acc: 0.3803\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7661 - acc: 0.3888 - val_loss: 1.7990 - val_acc: 0.3946\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7626 - acc: 0.3905 - val_loss: 1.8317 - val_acc: 0.3826\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7582 - acc: 0.3927 - val_loss: 1.8160 - val_acc: 0.3884\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7539 - acc: 0.3941 - val_loss: 1.8240 - val_acc: 0.3825\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7502 - acc: 0.3952 - val_loss: 1.8256 - val_acc: 0.3814\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7458 - acc: 0.3957 - val_loss: 1.8001 - val_acc: 0.3935\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7435 - acc: 0.3966 - val_loss: 1.8108 - val_acc: 0.3880\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7387 - acc: 0.3980 - val_loss: 1.8182 - val_acc: 0.3871\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.7373 - acc: 0.3977 - val_loss: 1.8323 - val_acc: 0.3803\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7340 - acc: 0.4002 - val_loss: 1.8456 - val_acc: 0.3744\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7299 - acc: 0.4002 - val_loss: 1.8215 - val_acc: 0.3805\n",
            "32890/32890 [==============================] - 1s 27us/sample - loss: 1.8146 - acc: 0.3715\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 9s 81us/sample - loss: 1.9447 - acc: 0.3338 - val_loss: 1.8688 - val_acc: 0.3451\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 4s 36us/sample - loss: 1.8511 - acc: 0.3617 - val_loss: 1.8327 - val_acc: 0.3642\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 4s 36us/sample - loss: 1.8304 - acc: 0.3686 - val_loss: 1.8108 - val_acc: 0.3645\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 4s 36us/sample - loss: 1.8175 - acc: 0.3719 - val_loss: 1.8180 - val_acc: 0.3622\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 4s 35us/sample - loss: 1.8067 - acc: 0.3763 - val_loss: 1.8013 - val_acc: 0.3689\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 4s 37us/sample - loss: 1.7986 - acc: 0.3787 - val_loss: 1.8062 - val_acc: 0.3685\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 4s 37us/sample - loss: 1.7922 - acc: 0.3798 - val_loss: 1.7859 - val_acc: 0.3744\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 4s 36us/sample - loss: 1.7858 - acc: 0.3823 - val_loss: 1.8088 - val_acc: 0.3661\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 4s 37us/sample - loss: 1.7805 - acc: 0.3826 - val_loss: 1.8108 - val_acc: 0.3670\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 4s 38us/sample - loss: 1.7750 - acc: 0.3841 - val_loss: 1.8008 - val_acc: 0.3715\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 4s 38us/sample - loss: 1.7704 - acc: 0.3862 - val_loss: 1.7947 - val_acc: 0.3690\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 4s 38us/sample - loss: 1.7665 - acc: 0.3883 - val_loss: 1.7944 - val_acc: 0.3712\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 4s 36us/sample - loss: 1.7627 - acc: 0.3909 - val_loss: 1.7763 - val_acc: 0.3840\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 4s 37us/sample - loss: 1.7579 - acc: 0.3916 - val_loss: 1.7924 - val_acc: 0.3728\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 4s 36us/sample - loss: 1.7537 - acc: 0.3914 - val_loss: 1.7878 - val_acc: 0.3750\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 4s 36us/sample - loss: 1.7509 - acc: 0.3941 - val_loss: 1.7865 - val_acc: 0.3777\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 4s 36us/sample - loss: 1.7468 - acc: 0.3943 - val_loss: 1.7809 - val_acc: 0.3831\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 4s 36us/sample - loss: 1.7442 - acc: 0.3939 - val_loss: 1.7832 - val_acc: 0.3757\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 4s 36us/sample - loss: 1.7409 - acc: 0.3958 - val_loss: 1.7990 - val_acc: 0.3714\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 4s 36us/sample - loss: 1.7370 - acc: 0.3974 - val_loss: 1.8097 - val_acc: 0.3739\n",
            "32889/32889 [==============================] - 1s 27us/sample - loss: 1.8416 - acc: 0.3862\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 9s 82us/sample - loss: 1.9860 - acc: 0.3246 - val_loss: 1.8440 - val_acc: 0.3836\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.8573 - acc: 0.3623 - val_loss: 1.8295 - val_acc: 0.3856\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.8346 - acc: 0.3692 - val_loss: 1.8196 - val_acc: 0.3907\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.8204 - acc: 0.3731 - val_loss: 1.8033 - val_acc: 0.3913\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.8101 - acc: 0.3769 - val_loss: 1.8166 - val_acc: 0.3858\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.8012 - acc: 0.3794 - val_loss: 1.7981 - val_acc: 0.3934\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7945 - acc: 0.3816 - val_loss: 1.7901 - val_acc: 0.3987\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7878 - acc: 0.3836 - val_loss: 1.7946 - val_acc: 0.4002\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7816 - acc: 0.3859 - val_loss: 1.7846 - val_acc: 0.4008\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7772 - acc: 0.3861 - val_loss: 1.8039 - val_acc: 0.3917\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7718 - acc: 0.3882 - val_loss: 1.7909 - val_acc: 0.4023\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7680 - acc: 0.3898 - val_loss: 1.7841 - val_acc: 0.4039\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7626 - acc: 0.3912 - val_loss: 1.7970 - val_acc: 0.3973\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7583 - acc: 0.3936 - val_loss: 1.8128 - val_acc: 0.3878\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7544 - acc: 0.3943 - val_loss: 1.8110 - val_acc: 0.3931\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7515 - acc: 0.3950 - val_loss: 1.8062 - val_acc: 0.3938\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7473 - acc: 0.3970 - val_loss: 1.7965 - val_acc: 0.3986\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7447 - acc: 0.3972 - val_loss: 1.8030 - val_acc: 0.3993\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.7401 - acc: 0.3995 - val_loss: 1.7969 - val_acc: 0.3990\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7374 - acc: 0.3996 - val_loss: 1.8126 - val_acc: 0.3917\n",
            "32890/32890 [==============================] - 1s 27us/sample - loss: 1.8654 - acc: 0.3588\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 9s 81us/sample - loss: 1.9858 - acc: 0.3190 - val_loss: 1.8935 - val_acc: 0.3515\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.8468 - acc: 0.3686 - val_loss: 1.8538 - val_acc: 0.3720\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.8210 - acc: 0.3768 - val_loss: 1.8415 - val_acc: 0.3795\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.8069 - acc: 0.3812 - val_loss: 1.8231 - val_acc: 0.3878\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7951 - acc: 0.3840 - val_loss: 1.8021 - val_acc: 0.3958\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7859 - acc: 0.3885 - val_loss: 1.8210 - val_acc: 0.3866\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7795 - acc: 0.3894 - val_loss: 1.8253 - val_acc: 0.3880\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7724 - acc: 0.3925 - val_loss: 1.7997 - val_acc: 0.3950\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.7668 - acc: 0.3930 - val_loss: 1.8096 - val_acc: 0.3899\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7617 - acc: 0.3953 - val_loss: 1.7855 - val_acc: 0.3962\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.7572 - acc: 0.3960 - val_loss: 1.8033 - val_acc: 0.3926\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7531 - acc: 0.3981 - val_loss: 1.7999 - val_acc: 0.3923\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7483 - acc: 0.3991 - val_loss: 1.8066 - val_acc: 0.3907\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7444 - acc: 0.4000 - val_loss: 1.8151 - val_acc: 0.3836\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7403 - acc: 0.4011 - val_loss: 1.7865 - val_acc: 0.4007\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7378 - acc: 0.4023 - val_loss: 1.8118 - val_acc: 0.3937\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7336 - acc: 0.4041 - val_loss: 1.8217 - val_acc: 0.3871\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7293 - acc: 0.4046 - val_loss: 1.8113 - val_acc: 0.3915\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7274 - acc: 0.4053 - val_loss: 1.8185 - val_acc: 0.3858\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.7234 - acc: 0.4062 - val_loss: 1.8048 - val_acc: 0.3921\n",
            "32890/32890 [==============================] - 1s 26us/sample - loss: 1.8929 - acc: 0.3422\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 9s 83us/sample - loss: 1.9835 - acc: 0.3216 - val_loss: 1.8712 - val_acc: 0.3613\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.8648 - acc: 0.3568 - val_loss: 1.8723 - val_acc: 0.3593\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.8405 - acc: 0.3647 - val_loss: 1.8275 - val_acc: 0.3807\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.8258 - acc: 0.3691 - val_loss: 1.8409 - val_acc: 0.3726\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.8151 - acc: 0.3725 - val_loss: 1.8354 - val_acc: 0.3773\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.8056 - acc: 0.3743 - val_loss: 1.8328 - val_acc: 0.3783\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7996 - acc: 0.3771 - val_loss: 1.8381 - val_acc: 0.3745\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7925 - acc: 0.3798 - val_loss: 1.8198 - val_acc: 0.3777\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.7878 - acc: 0.3803 - val_loss: 1.8209 - val_acc: 0.3818\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7820 - acc: 0.3828 - val_loss: 1.8172 - val_acc: 0.3834\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7762 - acc: 0.3845 - val_loss: 1.8499 - val_acc: 0.3675\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7724 - acc: 0.3856 - val_loss: 1.8119 - val_acc: 0.3877\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.7686 - acc: 0.3869 - val_loss: 1.8378 - val_acc: 0.3737\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7646 - acc: 0.3882 - val_loss: 1.8173 - val_acc: 0.3837\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7611 - acc: 0.3892 - val_loss: 1.8279 - val_acc: 0.3807\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7575 - acc: 0.3915 - val_loss: 1.8185 - val_acc: 0.3820\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.7538 - acc: 0.3919 - val_loss: 1.8214 - val_acc: 0.3829\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.7497 - acc: 0.3938 - val_loss: 1.8178 - val_acc: 0.3864\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7472 - acc: 0.3936 - val_loss: 1.8368 - val_acc: 0.3777\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7441 - acc: 0.3957 - val_loss: 1.8284 - val_acc: 0.3788\n",
            "32890/32890 [==============================] - 1s 27us/sample - loss: 1.7963 - acc: 0.3840\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 9s 87us/sample - loss: 1.9738 - acc: 0.3280 - val_loss: 1.8786 - val_acc: 0.3687\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.8589 - acc: 0.3615 - val_loss: 1.8627 - val_acc: 0.3801\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.8366 - acc: 0.3683 - val_loss: 1.8532 - val_acc: 0.3797\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.8208 - acc: 0.3733 - val_loss: 1.8408 - val_acc: 0.3799\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.8101 - acc: 0.3766 - val_loss: 1.8231 - val_acc: 0.3902\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7999 - acc: 0.3784 - val_loss: 1.8369 - val_acc: 0.3812\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.7929 - acc: 0.3805 - val_loss: 1.8620 - val_acc: 0.3764\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 1.7853 - acc: 0.3837 - val_loss: 1.8370 - val_acc: 0.3830\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 4s 41us/sample - loss: 1.7788 - acc: 0.3859 - val_loss: 1.8210 - val_acc: 0.3896\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 4s 41us/sample - loss: 1.7749 - acc: 0.3862 - val_loss: 1.8414 - val_acc: 0.3842\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7684 - acc: 0.3901 - val_loss: 1.8411 - val_acc: 0.3782\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7640 - acc: 0.3909 - val_loss: 1.8356 - val_acc: 0.3839\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.7604 - acc: 0.3910 - val_loss: 1.8248 - val_acc: 0.3886\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7556 - acc: 0.3925 - val_loss: 1.8315 - val_acc: 0.3834\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7519 - acc: 0.3940 - val_loss: 1.8430 - val_acc: 0.3754\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7490 - acc: 0.3949 - val_loss: 1.8160 - val_acc: 0.3893\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7455 - acc: 0.3966 - val_loss: 1.8313 - val_acc: 0.3816\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 4s 41us/sample - loss: 1.7424 - acc: 0.3964 - val_loss: 1.8601 - val_acc: 0.3707\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 4s 42us/sample - loss: 1.7385 - acc: 0.3992 - val_loss: 1.8355 - val_acc: 0.3842\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7350 - acc: 0.3997 - val_loss: 1.8630 - val_acc: 0.3747\n",
            "32890/32890 [==============================] - 1s 28us/sample - loss: 1.8516 - acc: 0.3625\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 10s 91us/sample - loss: 1.9692 - acc: 0.3266 - val_loss: 1.8962 - val_acc: 0.3416\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 4s 39us/sample - loss: 1.8590 - acc: 0.3597 - val_loss: 1.8590 - val_acc: 0.3518\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 4s 38us/sample - loss: 1.8374 - acc: 0.3651 - val_loss: 1.8302 - val_acc: 0.3568\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 4s 41us/sample - loss: 1.8229 - acc: 0.3693 - val_loss: 1.8238 - val_acc: 0.3594\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 4s 40us/sample - loss: 1.8122 - acc: 0.3729 - val_loss: 1.8156 - val_acc: 0.3588\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 4s 40us/sample - loss: 1.8046 - acc: 0.3750 - val_loss: 1.8118 - val_acc: 0.3652\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 4s 39us/sample - loss: 1.7968 - acc: 0.3791 - val_loss: 1.8084 - val_acc: 0.3668\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 4s 39us/sample - loss: 1.7892 - acc: 0.3798 - val_loss: 1.8240 - val_acc: 0.3600\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 4s 39us/sample - loss: 1.7837 - acc: 0.3820 - val_loss: 1.8127 - val_acc: 0.3660\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 5s 43us/sample - loss: 1.7767 - acc: 0.3837 - val_loss: 1.7982 - val_acc: 0.3739\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 4s 39us/sample - loss: 1.7721 - acc: 0.3865 - val_loss: 1.8104 - val_acc: 0.3668\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 4s 39us/sample - loss: 1.7674 - acc: 0.3885 - val_loss: 1.8192 - val_acc: 0.3636\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 4s 40us/sample - loss: 1.7618 - acc: 0.3889 - val_loss: 1.8042 - val_acc: 0.3718\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 4s 41us/sample - loss: 1.7579 - acc: 0.3916 - val_loss: 1.8113 - val_acc: 0.3665\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 4s 39us/sample - loss: 1.7548 - acc: 0.3919 - val_loss: 1.7961 - val_acc: 0.3738\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 4s 37us/sample - loss: 1.7499 - acc: 0.3936 - val_loss: 1.8148 - val_acc: 0.3665\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 4s 37us/sample - loss: 1.7469 - acc: 0.3935 - val_loss: 1.8032 - val_acc: 0.3709\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 4s 37us/sample - loss: 1.7437 - acc: 0.3964 - val_loss: 1.7950 - val_acc: 0.3766\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 4s 38us/sample - loss: 1.7383 - acc: 0.3962 - val_loss: 1.8063 - val_acc: 0.3715\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 4s 38us/sample - loss: 1.7365 - acc: 0.3984 - val_loss: 1.8022 - val_acc: 0.3721\n",
            "32889/32889 [==============================] - 1s 29us/sample - loss: 1.8621 - acc: 0.3778\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 9s 87us/sample - loss: 1.9671 - acc: 0.3289 - val_loss: 1.8474 - val_acc: 0.3828\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.8575 - acc: 0.3606 - val_loss: 1.8732 - val_acc: 0.3645\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.8344 - acc: 0.3670 - val_loss: 1.8059 - val_acc: 0.3907\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.8180 - acc: 0.3722 - val_loss: 1.8154 - val_acc: 0.3939\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.8078 - acc: 0.3769 - val_loss: 1.7981 - val_acc: 0.3967\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7989 - acc: 0.3789 - val_loss: 1.8137 - val_acc: 0.3893\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.7899 - acc: 0.3821 - val_loss: 1.8003 - val_acc: 0.3970\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7836 - acc: 0.3850 - val_loss: 1.8190 - val_acc: 0.3891\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.7789 - acc: 0.3864 - val_loss: 1.8050 - val_acc: 0.3969\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.7727 - acc: 0.3873 - val_loss: 1.8058 - val_acc: 0.3953\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7681 - acc: 0.3890 - val_loss: 1.8137 - val_acc: 0.3907\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 4s 41us/sample - loss: 1.7625 - acc: 0.3915 - val_loss: 1.8014 - val_acc: 0.3944\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 1.7588 - acc: 0.3925 - val_loss: 1.7934 - val_acc: 0.3978\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 4s 41us/sample - loss: 1.7549 - acc: 0.3940 - val_loss: 1.7806 - val_acc: 0.4057\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 4s 41us/sample - loss: 1.7510 - acc: 0.3954 - val_loss: 1.8134 - val_acc: 0.3896\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7471 - acc: 0.3963 - val_loss: 1.7995 - val_acc: 0.3943\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 1.7443 - acc: 0.3970 - val_loss: 1.8172 - val_acc: 0.3917\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 4s 42us/sample - loss: 1.7410 - acc: 0.3979 - val_loss: 1.8055 - val_acc: 0.3955\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 4s 41us/sample - loss: 1.7391 - acc: 0.3979 - val_loss: 1.7947 - val_acc: 0.4021\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 5s 43us/sample - loss: 1.7335 - acc: 0.4004 - val_loss: 1.8183 - val_acc: 0.3894\n",
            "32890/32890 [==============================] - 1s 29us/sample - loss: 1.8560 - acc: 0.3650\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 10s 92us/sample - loss: 1.9630 - acc: 0.3337 - val_loss: 1.8614 - val_acc: 0.3670\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 4s 41us/sample - loss: 1.8445 - acc: 0.3684 - val_loss: 1.8260 - val_acc: 0.3820\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 4s 42us/sample - loss: 1.8191 - acc: 0.3756 - val_loss: 1.8040 - val_acc: 0.3897\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 1.8049 - acc: 0.3809 - val_loss: 1.8103 - val_acc: 0.3851\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 4s 43us/sample - loss: 1.7945 - acc: 0.3841 - val_loss: 1.7997 - val_acc: 0.3927\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 4s 41us/sample - loss: 1.7849 - acc: 0.3872 - val_loss: 1.8052 - val_acc: 0.3885\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 1.7773 - acc: 0.3890 - val_loss: 1.8090 - val_acc: 0.3891\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 1.7709 - acc: 0.3913 - val_loss: 1.8118 - val_acc: 0.3878\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 4s 42us/sample - loss: 1.7647 - acc: 0.3946 - val_loss: 1.8123 - val_acc: 0.3884\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 4s 42us/sample - loss: 1.7596 - acc: 0.3950 - val_loss: 1.7831 - val_acc: 0.3987\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7548 - acc: 0.3972 - val_loss: 1.8141 - val_acc: 0.3860\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 1.7506 - acc: 0.3977 - val_loss: 1.7935 - val_acc: 0.3945\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7460 - acc: 0.4004 - val_loss: 1.8073 - val_acc: 0.3927\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 1.7414 - acc: 0.4016 - val_loss: 1.8094 - val_acc: 0.3857\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7382 - acc: 0.4033 - val_loss: 1.8095 - val_acc: 0.3859\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 1.7335 - acc: 0.4041 - val_loss: 1.8192 - val_acc: 0.3848\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 1.7306 - acc: 0.4045 - val_loss: 1.8197 - val_acc: 0.3839\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7270 - acc: 0.4062 - val_loss: 1.8161 - val_acc: 0.3848\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7227 - acc: 0.4080 - val_loss: 1.8089 - val_acc: 0.3879\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 1.7206 - acc: 0.4079 - val_loss: 1.8122 - val_acc: 0.3888\n",
            "32890/32890 [==============================] - 1s 30us/sample - loss: 1.8952 - acc: 0.3399\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 10s 92us/sample - loss: 1.9736 - acc: 0.3229 - val_loss: 1.8536 - val_acc: 0.3703\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.8644 - acc: 0.3562 - val_loss: 1.8232 - val_acc: 0.3835\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.8431 - acc: 0.3637 - val_loss: 1.8420 - val_acc: 0.3749\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.8293 - acc: 0.3671 - val_loss: 1.8329 - val_acc: 0.3773\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 1.8182 - acc: 0.3715 - val_loss: 1.8555 - val_acc: 0.3660\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 1.8093 - acc: 0.3745 - val_loss: 1.8228 - val_acc: 0.3807\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 1.8016 - acc: 0.3766 - val_loss: 1.8342 - val_acc: 0.3764\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7959 - acc: 0.3780 - val_loss: 1.8382 - val_acc: 0.3715\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7900 - acc: 0.3791 - val_loss: 1.8167 - val_acc: 0.3862\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 4s 41us/sample - loss: 1.7841 - acc: 0.3826 - val_loss: 1.8090 - val_acc: 0.3910\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7791 - acc: 0.3830 - val_loss: 1.8460 - val_acc: 0.3721\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7739 - acc: 0.3850 - val_loss: 1.8275 - val_acc: 0.3802\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 1.7696 - acc: 0.3874 - val_loss: 1.8286 - val_acc: 0.3776\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 4s 41us/sample - loss: 1.7657 - acc: 0.3862 - val_loss: 1.8327 - val_acc: 0.3736\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 4s 42us/sample - loss: 1.7610 - acc: 0.3890 - val_loss: 1.8171 - val_acc: 0.3830\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 1.7570 - acc: 0.3916 - val_loss: 1.8319 - val_acc: 0.3846\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 1.7543 - acc: 0.3918 - val_loss: 1.8201 - val_acc: 0.3831\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7492 - acc: 0.3929 - val_loss: 1.8507 - val_acc: 0.3711\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 1.7468 - acc: 0.3935 - val_loss: 1.8163 - val_acc: 0.3858\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 1.7432 - acc: 0.3954 - val_loss: 1.8341 - val_acc: 0.3756\n",
            "32890/32890 [==============================] - 1s 30us/sample - loss: 1.8043 - acc: 0.3798\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 10s 93us/sample - loss: 1.9551 - acc: 0.3293 - val_loss: 1.8675 - val_acc: 0.3718\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 1.8587 - acc: 0.3599 - val_loss: 1.8646 - val_acc: 0.3721\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 1.8364 - acc: 0.3687 - val_loss: 1.8210 - val_acc: 0.3883\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 4s 41us/sample - loss: 1.8223 - acc: 0.3725 - val_loss: 1.8592 - val_acc: 0.3735\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 4s 41us/sample - loss: 1.8112 - acc: 0.3761 - val_loss: 1.8293 - val_acc: 0.3855\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 4s 41us/sample - loss: 1.8027 - acc: 0.3785 - val_loss: 1.8519 - val_acc: 0.3728\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7952 - acc: 0.3802 - val_loss: 1.8339 - val_acc: 0.3829\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7895 - acc: 0.3823 - val_loss: 1.8451 - val_acc: 0.3795\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7832 - acc: 0.3849 - val_loss: 1.8270 - val_acc: 0.3800\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 1.7766 - acc: 0.3873 - val_loss: 1.8336 - val_acc: 0.3883\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 1.7725 - acc: 0.3890 - val_loss: 1.8610 - val_acc: 0.3710\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 1.7670 - acc: 0.3900 - val_loss: 1.8417 - val_acc: 0.3794\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 1.7628 - acc: 0.3918 - val_loss: 1.8367 - val_acc: 0.3823\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 1.7596 - acc: 0.3922 - val_loss: 1.8368 - val_acc: 0.3841\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 4s 42us/sample - loss: 1.7548 - acc: 0.3946 - val_loss: 1.8495 - val_acc: 0.3763\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 4s 39us/sample - loss: 1.7510 - acc: 0.3962 - val_loss: 1.8207 - val_acc: 0.3885\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 1.7470 - acc: 0.3974 - val_loss: 1.8561 - val_acc: 0.3732\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 4s 42us/sample - loss: 1.7438 - acc: 0.3975 - val_loss: 1.8735 - val_acc: 0.3650\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 4s 42us/sample - loss: 1.7402 - acc: 0.4003 - val_loss: 1.8656 - val_acc: 0.3777\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 4s 41us/sample - loss: 1.7365 - acc: 0.4009 - val_loss: 1.8461 - val_acc: 0.3771\n",
            "32890/32890 [==============================] - 1s 29us/sample - loss: 1.8413 - acc: 0.3615\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 10s 90us/sample - loss: 1.9817 - acc: 0.3266 - val_loss: 1.8940 - val_acc: 0.3352\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 4s 40us/sample - loss: 1.8603 - acc: 0.3585 - val_loss: 1.8285 - val_acc: 0.3623\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 4s 39us/sample - loss: 1.8377 - acc: 0.3641 - val_loss: 1.8278 - val_acc: 0.3620\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 4s 39us/sample - loss: 1.8243 - acc: 0.3697 - val_loss: 1.8278 - val_acc: 0.3582\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 4s 40us/sample - loss: 1.8144 - acc: 0.3721 - val_loss: 1.8194 - val_acc: 0.3645\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 4s 42us/sample - loss: 1.8056 - acc: 0.3749 - val_loss: 1.8169 - val_acc: 0.3613\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 4s 40us/sample - loss: 1.7987 - acc: 0.3770 - val_loss: 1.8051 - val_acc: 0.3714\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 4s 40us/sample - loss: 1.7919 - acc: 0.3797 - val_loss: 1.7999 - val_acc: 0.3740\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 4s 42us/sample - loss: 1.7860 - acc: 0.3823 - val_loss: 1.8130 - val_acc: 0.3620\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 4s 42us/sample - loss: 1.7812 - acc: 0.3833 - val_loss: 1.8075 - val_acc: 0.3701\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 4s 41us/sample - loss: 1.7758 - acc: 0.3840 - val_loss: 1.8229 - val_acc: 0.3599\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 4s 41us/sample - loss: 1.7711 - acc: 0.3868 - val_loss: 1.7924 - val_acc: 0.3744\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 4s 40us/sample - loss: 1.7656 - acc: 0.3883 - val_loss: 1.8451 - val_acc: 0.3504\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 4s 39us/sample - loss: 1.7622 - acc: 0.3877 - val_loss: 1.8342 - val_acc: 0.3586\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 4s 40us/sample - loss: 1.7588 - acc: 0.3888 - val_loss: 1.8125 - val_acc: 0.3683\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 4s 40us/sample - loss: 1.7552 - acc: 0.3921 - val_loss: 1.8188 - val_acc: 0.3615\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 4s 40us/sample - loss: 1.7512 - acc: 0.3923 - val_loss: 1.8341 - val_acc: 0.3634\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 4s 39us/sample - loss: 1.7478 - acc: 0.3930 - val_loss: 1.8134 - val_acc: 0.3698\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 4s 40us/sample - loss: 1.7448 - acc: 0.3934 - val_loss: 1.8146 - val_acc: 0.3718\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 4s 42us/sample - loss: 1.7411 - acc: 0.3948 - val_loss: 1.8275 - val_acc: 0.3640\n",
            "32889/32889 [==============================] - 1s 30us/sample - loss: 1.8329 - acc: 0.3878\n",
            "Train on 131559 samples, validate on 32890 samples\n",
            "Epoch 1/20\n",
            "131559/131559 [==============================] - 11s 83us/sample - loss: 1.9843 - acc: 0.3127 - val_loss: 1.8867 - val_acc: 0.3729\n",
            "Epoch 2/20\n",
            "131559/131559 [==============================] - 5s 41us/sample - loss: 1.8705 - acc: 0.3532 - val_loss: 1.8765 - val_acc: 0.3704\n",
            "Epoch 3/20\n",
            "131559/131559 [==============================] - 5s 41us/sample - loss: 1.8468 - acc: 0.3606 - val_loss: 1.8428 - val_acc: 0.3879\n",
            "Epoch 4/20\n",
            "131559/131559 [==============================] - 6s 42us/sample - loss: 1.8303 - acc: 0.3661 - val_loss: 1.8403 - val_acc: 0.3856\n",
            "Epoch 5/20\n",
            "131559/131559 [==============================] - 5s 40us/sample - loss: 1.8187 - acc: 0.3709 - val_loss: 1.8377 - val_acc: 0.3856\n",
            "Epoch 6/20\n",
            "131559/131559 [==============================] - 5s 41us/sample - loss: 1.8095 - acc: 0.3736 - val_loss: 1.8252 - val_acc: 0.3862\n",
            "Epoch 7/20\n",
            "131559/131559 [==============================] - 5s 41us/sample - loss: 1.8024 - acc: 0.3767 - val_loss: 1.8087 - val_acc: 0.3930\n",
            "Epoch 8/20\n",
            "131559/131559 [==============================] - 5s 41us/sample - loss: 1.7962 - acc: 0.3783 - val_loss: 1.8017 - val_acc: 0.3979\n",
            "Epoch 9/20\n",
            "131559/131559 [==============================] - 5s 41us/sample - loss: 1.7907 - acc: 0.3810 - val_loss: 1.8147 - val_acc: 0.3901\n",
            "Epoch 10/20\n",
            "131559/131559 [==============================] - 5s 41us/sample - loss: 1.7854 - acc: 0.3815 - val_loss: 1.8318 - val_acc: 0.3864\n",
            "Epoch 11/20\n",
            "131559/131559 [==============================] - 5s 42us/sample - loss: 1.7814 - acc: 0.3837 - val_loss: 1.8081 - val_acc: 0.3949\n",
            "Epoch 12/20\n",
            "131559/131559 [==============================] - 5s 42us/sample - loss: 1.7771 - acc: 0.3860 - val_loss: 1.8102 - val_acc: 0.3948\n",
            "Epoch 13/20\n",
            "131559/131559 [==============================] - 5s 41us/sample - loss: 1.7725 - acc: 0.3858 - val_loss: 1.8373 - val_acc: 0.3873\n",
            "Epoch 14/20\n",
            "131559/131559 [==============================] - 5s 40us/sample - loss: 1.7694 - acc: 0.3876 - val_loss: 1.7973 - val_acc: 0.3947\n",
            "Epoch 15/20\n",
            "131559/131559 [==============================] - 5s 41us/sample - loss: 1.7666 - acc: 0.3882 - val_loss: 1.8192 - val_acc: 0.3874\n",
            "Epoch 16/20\n",
            "131559/131559 [==============================] - 5s 40us/sample - loss: 1.7639 - acc: 0.3890 - val_loss: 1.8149 - val_acc: 0.3872\n",
            "Epoch 17/20\n",
            "131559/131559 [==============================] - 5s 41us/sample - loss: 1.7602 - acc: 0.3909 - val_loss: 1.8171 - val_acc: 0.3914\n",
            "Epoch 18/20\n",
            "131559/131559 [==============================] - 5s 40us/sample - loss: 1.7570 - acc: 0.3915 - val_loss: 1.8278 - val_acc: 0.3886\n",
            "Epoch 19/20\n",
            "131559/131559 [==============================] - 6s 42us/sample - loss: 1.7552 - acc: 0.3921 - val_loss: 1.8052 - val_acc: 0.3921\n",
            "Epoch 20/20\n",
            "131559/131559 [==============================] - 6s 43us/sample - loss: 1.7518 - acc: 0.3930 - val_loss: 1.7922 - val_acc: 0.3962\n",
            "Best: 0.374274218082428 using {'weight_init': 'normal'}\n",
            "Means: 0.37196956276893617, Stdev: 0.016677869156865997 with: {'weight_init': 'uniform'}\n",
            "Means: 0.3694946229457855, Stdev: 0.01566922119322975 with: {'weight_init': 'lecun_uniform'}\n",
            "Means: 0.374274218082428, Stdev: 0.017493883247537836 with: {'weight_init': 'normal'}\n",
            "Means: 0.12191002666950226, Stdev: 0.023610390345984975 with: {'weight_init': 'zero'}\n",
            "Means: 0.3679683029651642, Stdev: 0.014173102677415889 with: {'weight_init': 'glorot_normal'}\n",
            "Means: 0.37195734977722167, Stdev: 0.014614450829949555 with: {'weight_init': 'glorot_uniform'}\n",
            "Means: 0.36505550146102905, Stdev: 0.014752654742300057 with: {'weight_init': 'he_normal'}\n",
            "Means: 0.3668068587779999, Stdev: 0.016532914148844977 with: {'weight_init': 'he_uniform'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9_rhdaZsqcR",
        "colab_type": "text"
      },
      "source": [
        "### Dropout Rate.*(0.01)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIl5n0Xostlq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function for NN model.\n",
        "def create_model(weight_rate=0.0, weight_constraint=0):\n",
        "      # set the model.\n",
        "    model = Sequential()\n",
        "    # hidden layers.\n",
        "    model.add(Dense(192, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
        "    model.add(Dropout(weight_rate))\n",
        "    model.add(Dense(96, kernel_initializer='normal', activation='relu'))\n",
        "    model.add(Dropout(weight_rate))\n",
        "    model.add(Dense(48, kernel_initializer='normal', activation='relu'))\n",
        "    model.add(Dropout(weight_rate))\n",
        "    model.add(Dense(24, kernel_initializer='normal', activation='relu'))\n",
        "    model.add(Dropout(weight_rate))\n",
        "    model.add(Dense(12, kernel_initializer='normal', activation='softmax'))\n",
        "    # function for top 3.\n",
        "    def top_3_accuracy(y_true, y_pred):\n",
        "        return top_k_categorical_accuracy(y_true, y_pred, k=3)\n",
        "\n",
        "    # set the optimizer.\n",
        "    optimizer = Adam(lr=0.001)\n",
        "    # compile the model.\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                        optimizer=optimizer,\n",
        "                        metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vU6S_9Gwstt5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set the KerasClassifier model.\n",
        "model = KerasClassifier(build_fn=create_model, epochs=20, batch_size=200, validation_split=0.2, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GH7_kEnstrq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set the parameters for weights and dropout rate.\n",
        "weight_constraint = [1, 2]\n",
        "weight_rate = [0.1, 0.2]\n",
        "param_grid = dict(weight_rate=weight_rate, weight_constraint=weight_constraint)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4_OyYdYstp3",
        "colab_type": "code",
        "outputId": "bbfb3934-b98f-4dfe-b157-88da5d232820",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# set the gridsearch.\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
        "grid_result = grid.fit(X_scaled, y_train)\n",
        "# show the results.\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:143: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 2.0419 - acc: 0.2859 - val_loss: 1.8539 - val_acc: 0.3836\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.9211 - acc: 0.3400 - val_loss: 1.8339 - val_acc: 0.3873\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.9015 - acc: 0.3464 - val_loss: 1.8386 - val_acc: 0.3881\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8920 - acc: 0.3517 - val_loss: 1.8286 - val_acc: 0.3859\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8833 - acc: 0.3536 - val_loss: 1.8233 - val_acc: 0.3875\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8743 - acc: 0.3579 - val_loss: 1.8129 - val_acc: 0.3920\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.8678 - acc: 0.3596 - val_loss: 1.8063 - val_acc: 0.3941\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.8618 - acc: 0.3628 - val_loss: 1.8038 - val_acc: 0.3969\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8552 - acc: 0.3645 - val_loss: 1.7956 - val_acc: 0.3981\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8497 - acc: 0.3672 - val_loss: 1.7874 - val_acc: 0.4018\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8456 - acc: 0.3685 - val_loss: 1.7940 - val_acc: 0.4010\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8414 - acc: 0.3699 - val_loss: 1.7861 - val_acc: 0.4056\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8348 - acc: 0.3718 - val_loss: 1.7844 - val_acc: 0.4037\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8347 - acc: 0.3724 - val_loss: 1.7939 - val_acc: 0.3997\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8298 - acc: 0.3753 - val_loss: 1.7888 - val_acc: 0.4021\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8266 - acc: 0.3752 - val_loss: 1.7880 - val_acc: 0.4035\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8229 - acc: 0.3747 - val_loss: 1.7784 - val_acc: 0.4025\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8220 - acc: 0.3765 - val_loss: 1.7841 - val_acc: 0.4032\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.8197 - acc: 0.3801 - val_loss: 1.7826 - val_acc: 0.4019\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8154 - acc: 0.3783 - val_loss: 1.7760 - val_acc: 0.4058\n",
            "32890/32890 [==============================] - 0s 9us/sample - loss: 1.8315 - acc: 0.3680\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 2.0142 - acc: 0.3036 - val_loss: 1.8677 - val_acc: 0.3727\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.9056 - acc: 0.3471 - val_loss: 1.8691 - val_acc: 0.3704\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8845 - acc: 0.3559 - val_loss: 1.8233 - val_acc: 0.3836\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8724 - acc: 0.3608 - val_loss: 1.8307 - val_acc: 0.3829\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8618 - acc: 0.3660 - val_loss: 1.8154 - val_acc: 0.3927\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8529 - acc: 0.3675 - val_loss: 1.8180 - val_acc: 0.3883\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8487 - acc: 0.3704 - val_loss: 1.8175 - val_acc: 0.3891\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8402 - acc: 0.3727 - val_loss: 1.8103 - val_acc: 0.3942\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8342 - acc: 0.3765 - val_loss: 1.7902 - val_acc: 0.3976\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8309 - acc: 0.3778 - val_loss: 1.8017 - val_acc: 0.3933\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8260 - acc: 0.3790 - val_loss: 1.7928 - val_acc: 0.3952\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8202 - acc: 0.3808 - val_loss: 1.8058 - val_acc: 0.3916\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8167 - acc: 0.3806 - val_loss: 1.7885 - val_acc: 0.3968\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8141 - acc: 0.3829 - val_loss: 1.7805 - val_acc: 0.3987\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8112 - acc: 0.3831 - val_loss: 1.7899 - val_acc: 0.3954\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8086 - acc: 0.3851 - val_loss: 1.7905 - val_acc: 0.3969\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8050 - acc: 0.3850 - val_loss: 1.7897 - val_acc: 0.3959\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8008 - acc: 0.3866 - val_loss: 1.7780 - val_acc: 0.4001\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.7989 - acc: 0.3879 - val_loss: 1.7779 - val_acc: 0.3996\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.7961 - acc: 0.3887 - val_loss: 1.7896 - val_acc: 0.3937\n",
            "32890/32890 [==============================] - 0s 7us/sample - loss: 1.8797 - acc: 0.3428\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 2.0405 - acc: 0.2868 - val_loss: 1.8847 - val_acc: 0.3663\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.9317 - acc: 0.3290 - val_loss: 1.8764 - val_acc: 0.3719\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.9115 - acc: 0.3395 - val_loss: 1.8732 - val_acc: 0.3770\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8964 - acc: 0.3463 - val_loss: 1.8504 - val_acc: 0.3790\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8874 - acc: 0.3509 - val_loss: 1.8432 - val_acc: 0.3822\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8785 - acc: 0.3532 - val_loss: 1.8631 - val_acc: 0.3726\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8722 - acc: 0.3565 - val_loss: 1.8306 - val_acc: 0.3855\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8644 - acc: 0.3590 - val_loss: 1.8287 - val_acc: 0.3908\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.8592 - acc: 0.3617 - val_loss: 1.8271 - val_acc: 0.3844\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.8540 - acc: 0.3637 - val_loss: 1.8202 - val_acc: 0.3886\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.8509 - acc: 0.3639 - val_loss: 1.8283 - val_acc: 0.3872\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.8479 - acc: 0.3647 - val_loss: 1.8145 - val_acc: 0.3902\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.8416 - acc: 0.3672 - val_loss: 1.8077 - val_acc: 0.3920\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8381 - acc: 0.3689 - val_loss: 1.8188 - val_acc: 0.3877\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8357 - acc: 0.3697 - val_loss: 1.8166 - val_acc: 0.3858\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8324 - acc: 0.3709 - val_loss: 1.8096 - val_acc: 0.3907\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.8309 - acc: 0.3717 - val_loss: 1.8085 - val_acc: 0.3916\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8276 - acc: 0.3721 - val_loss: 1.8098 - val_acc: 0.3865\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8249 - acc: 0.3730 - val_loss: 1.8007 - val_acc: 0.3895\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8230 - acc: 0.3737 - val_loss: 1.8068 - val_acc: 0.3903\n",
            "32890/32890 [==============================] - 0s 8us/sample - loss: 1.7810 - acc: 0.3962\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 4s 33us/sample - loss: 2.0256 - acc: 0.2998 - val_loss: 1.9077 - val_acc: 0.3585\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.9211 - acc: 0.3383 - val_loss: 1.8857 - val_acc: 0.3667\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.9013 - acc: 0.3470 - val_loss: 1.8659 - val_acc: 0.3724\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.8887 - acc: 0.3527 - val_loss: 1.8660 - val_acc: 0.3729\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.8788 - acc: 0.3551 - val_loss: 1.8527 - val_acc: 0.3762\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.8716 - acc: 0.3586 - val_loss: 1.8575 - val_acc: 0.3694\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.8644 - acc: 0.3596 - val_loss: 1.8344 - val_acc: 0.3840\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.8575 - acc: 0.3642 - val_loss: 1.8334 - val_acc: 0.3845\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.8515 - acc: 0.3656 - val_loss: 1.8186 - val_acc: 0.3909\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.8481 - acc: 0.3675 - val_loss: 1.8297 - val_acc: 0.3825\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.8441 - acc: 0.3688 - val_loss: 1.8324 - val_acc: 0.3819\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.8386 - acc: 0.3708 - val_loss: 1.8188 - val_acc: 0.3871\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8365 - acc: 0.3711 - val_loss: 1.8341 - val_acc: 0.3791\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8332 - acc: 0.3713 - val_loss: 1.8288 - val_acc: 0.3854\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.8305 - acc: 0.3726 - val_loss: 1.8096 - val_acc: 0.3927\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.8279 - acc: 0.3743 - val_loss: 1.8056 - val_acc: 0.3941\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.8230 - acc: 0.3758 - val_loss: 1.8318 - val_acc: 0.3847\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.8222 - acc: 0.3758 - val_loss: 1.8143 - val_acc: 0.3909\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.8185 - acc: 0.3776 - val_loss: 1.8246 - val_acc: 0.3885\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.8171 - acc: 0.3801 - val_loss: 1.8141 - val_acc: 0.3880\n",
            "32890/32890 [==============================] - 0s 8us/sample - loss: 1.8275 - acc: 0.3716\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 4s 33us/sample - loss: 2.0467 - acc: 0.2852 - val_loss: 1.8915 - val_acc: 0.3454\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 3s 30us/sample - loss: 1.9302 - acc: 0.3316 - val_loss: 1.8761 - val_acc: 0.3398\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 3s 30us/sample - loss: 1.9111 - acc: 0.3412 - val_loss: 1.8656 - val_acc: 0.3446\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 3s 30us/sample - loss: 1.8968 - acc: 0.3464 - val_loss: 1.8623 - val_acc: 0.3400\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 3s 29us/sample - loss: 1.8859 - acc: 0.3515 - val_loss: 1.8654 - val_acc: 0.3405\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 3s 31us/sample - loss: 1.8750 - acc: 0.3563 - val_loss: 1.8247 - val_acc: 0.3650\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 3s 31us/sample - loss: 1.8674 - acc: 0.3582 - val_loss: 1.8494 - val_acc: 0.3462\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 3s 30us/sample - loss: 1.8615 - acc: 0.3609 - val_loss: 1.8356 - val_acc: 0.3576\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 3s 31us/sample - loss: 1.8567 - acc: 0.3615 - val_loss: 1.8128 - val_acc: 0.3680\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 3s 30us/sample - loss: 1.8516 - acc: 0.3644 - val_loss: 1.8105 - val_acc: 0.3681\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 3s 29us/sample - loss: 1.8461 - acc: 0.3655 - val_loss: 1.8062 - val_acc: 0.3696\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 3s 29us/sample - loss: 1.8418 - acc: 0.3669 - val_loss: 1.7968 - val_acc: 0.3771\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 3s 29us/sample - loss: 1.8385 - acc: 0.3696 - val_loss: 1.8088 - val_acc: 0.3719\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 3s 31us/sample - loss: 1.8363 - acc: 0.3709 - val_loss: 1.7952 - val_acc: 0.3728\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 3s 31us/sample - loss: 1.8339 - acc: 0.3712 - val_loss: 1.8095 - val_acc: 0.3644\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 3s 30us/sample - loss: 1.8303 - acc: 0.3717 - val_loss: 1.8078 - val_acc: 0.3709\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 3s 30us/sample - loss: 1.8270 - acc: 0.3733 - val_loss: 1.8138 - val_acc: 0.3632\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 3s 30us/sample - loss: 1.8253 - acc: 0.3723 - val_loss: 1.7944 - val_acc: 0.3752\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 3s 30us/sample - loss: 1.8220 - acc: 0.3739 - val_loss: 1.8104 - val_acc: 0.3688\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 3s 30us/sample - loss: 1.8210 - acc: 0.3747 - val_loss: 1.8026 - val_acc: 0.3716\n",
            "32889/32889 [==============================] - 0s 8us/sample - loss: 1.8130 - acc: 0.3968\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 4s 33us/sample - loss: 2.0593 - acc: 0.2856 - val_loss: 1.8757 - val_acc: 0.3744\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.9529 - acc: 0.3307 - val_loss: 1.8605 - val_acc: 0.3822\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.9353 - acc: 0.3370 - val_loss: 1.8392 - val_acc: 0.3890\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.9272 - acc: 0.3411 - val_loss: 1.8303 - val_acc: 0.3902\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.9172 - acc: 0.3445 - val_loss: 1.8346 - val_acc: 0.3923\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.9111 - acc: 0.3488 - val_loss: 1.8236 - val_acc: 0.3885\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.9038 - acc: 0.3496 - val_loss: 1.8324 - val_acc: 0.3918\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.9000 - acc: 0.3533 - val_loss: 1.8182 - val_acc: 0.3982\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8950 - acc: 0.3541 - val_loss: 1.8073 - val_acc: 0.3996\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8896 - acc: 0.3566 - val_loss: 1.8094 - val_acc: 0.3987\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.8831 - acc: 0.3577 - val_loss: 1.8034 - val_acc: 0.4004\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.8806 - acc: 0.3616 - val_loss: 1.8047 - val_acc: 0.4018\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.8752 - acc: 0.3627 - val_loss: 1.8034 - val_acc: 0.3986\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.8748 - acc: 0.3629 - val_loss: 1.7885 - val_acc: 0.4049\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.8691 - acc: 0.3638 - val_loss: 1.7901 - val_acc: 0.4064\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8671 - acc: 0.3654 - val_loss: 1.7946 - val_acc: 0.4059\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.8660 - acc: 0.3650 - val_loss: 1.7828 - val_acc: 0.4098\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8653 - acc: 0.3661 - val_loss: 1.7832 - val_acc: 0.4083\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8615 - acc: 0.3680 - val_loss: 1.7956 - val_acc: 0.4032\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.8589 - acc: 0.3681 - val_loss: 1.7917 - val_acc: 0.4049\n",
            "32890/32890 [==============================] - 0s 9us/sample - loss: 1.8356 - acc: 0.3605\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 4s 33us/sample - loss: 2.0496 - acc: 0.2893 - val_loss: 1.8695 - val_acc: 0.3743\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.9359 - acc: 0.3388 - val_loss: 1.8557 - val_acc: 0.3768\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.9177 - acc: 0.3463 - val_loss: 1.8700 - val_acc: 0.3698\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.9081 - acc: 0.3503 - val_loss: 1.8607 - val_acc: 0.3725\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.8975 - acc: 0.3533 - val_loss: 1.8375 - val_acc: 0.3837\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.8892 - acc: 0.3569 - val_loss: 1.8285 - val_acc: 0.3901\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.8854 - acc: 0.3593 - val_loss: 1.8209 - val_acc: 0.3878\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8770 - acc: 0.3619 - val_loss: 1.8169 - val_acc: 0.3921\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8727 - acc: 0.3649 - val_loss: 1.8115 - val_acc: 0.3923\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8667 - acc: 0.3668 - val_loss: 1.8167 - val_acc: 0.3914\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.8631 - acc: 0.3666 - val_loss: 1.8107 - val_acc: 0.3950\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.8571 - acc: 0.3701 - val_loss: 1.8120 - val_acc: 0.3919\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8579 - acc: 0.3708 - val_loss: 1.8023 - val_acc: 0.3984\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.8536 - acc: 0.3703 - val_loss: 1.8191 - val_acc: 0.3917\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.8524 - acc: 0.3724 - val_loss: 1.8058 - val_acc: 0.3963\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.8495 - acc: 0.3744 - val_loss: 1.8014 - val_acc: 0.4013\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 29us/sample - loss: 1.8468 - acc: 0.3763 - val_loss: 1.8084 - val_acc: 0.3949\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.8431 - acc: 0.3761 - val_loss: 1.8105 - val_acc: 0.3960\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.8431 - acc: 0.3749 - val_loss: 1.8002 - val_acc: 0.4001\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.8404 - acc: 0.3766 - val_loss: 1.8043 - val_acc: 0.3981\n",
            "32890/32890 [==============================] - 0s 8us/sample - loss: 1.8773 - acc: 0.3428\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 2.0614 - acc: 0.2871 - val_loss: 1.8916 - val_acc: 0.3676\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.9609 - acc: 0.3245 - val_loss: 1.8884 - val_acc: 0.3691\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.9427 - acc: 0.3317 - val_loss: 1.8690 - val_acc: 0.3744\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.9317 - acc: 0.3374 - val_loss: 1.8601 - val_acc: 0.3720\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.9225 - acc: 0.3398 - val_loss: 1.8528 - val_acc: 0.3766\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.9139 - acc: 0.3423 - val_loss: 1.8373 - val_acc: 0.3822\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.9079 - acc: 0.3436 - val_loss: 1.8503 - val_acc: 0.3783\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.9034 - acc: 0.3484 - val_loss: 1.8355 - val_acc: 0.3816\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.8991 - acc: 0.3503 - val_loss: 1.8415 - val_acc: 0.3812\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.8915 - acc: 0.3524 - val_loss: 1.8252 - val_acc: 0.3905\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.8871 - acc: 0.3537 - val_loss: 1.8275 - val_acc: 0.3867\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.8849 - acc: 0.3539 - val_loss: 1.8240 - val_acc: 0.3855\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.8808 - acc: 0.3583 - val_loss: 1.8289 - val_acc: 0.3837\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.8800 - acc: 0.3575 - val_loss: 1.8250 - val_acc: 0.3839\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.8782 - acc: 0.3595 - val_loss: 1.8155 - val_acc: 0.3931\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.8740 - acc: 0.3599 - val_loss: 1.8206 - val_acc: 0.3862\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.8715 - acc: 0.3613 - val_loss: 1.8403 - val_acc: 0.3783\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.8688 - acc: 0.3612 - val_loss: 1.8117 - val_acc: 0.3940\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.8704 - acc: 0.3630 - val_loss: 1.8306 - val_acc: 0.3869\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.8674 - acc: 0.3626 - val_loss: 1.8221 - val_acc: 0.3856\n",
            "32890/32890 [==============================] - 0s 9us/sample - loss: 1.8037 - acc: 0.3897\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 2.0675 - acc: 0.2821 - val_loss: 1.9067 - val_acc: 0.3609\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.9499 - acc: 0.3281 - val_loss: 1.8874 - val_acc: 0.3663\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.9327 - acc: 0.3367 - val_loss: 1.8831 - val_acc: 0.3709\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.9199 - acc: 0.3430 - val_loss: 1.8691 - val_acc: 0.3729\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.9136 - acc: 0.3467 - val_loss: 1.8586 - val_acc: 0.3799\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.9068 - acc: 0.3471 - val_loss: 1.8569 - val_acc: 0.3761\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.8997 - acc: 0.3505 - val_loss: 1.8663 - val_acc: 0.3750\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.8954 - acc: 0.3528 - val_loss: 1.8503 - val_acc: 0.3771\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.8902 - acc: 0.3550 - val_loss: 1.8548 - val_acc: 0.3766\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.8871 - acc: 0.3552 - val_loss: 1.8487 - val_acc: 0.3810\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.8832 - acc: 0.3588 - val_loss: 1.8499 - val_acc: 0.3798\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.8821 - acc: 0.3579 - val_loss: 1.8528 - val_acc: 0.3768\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.8764 - acc: 0.3608 - val_loss: 1.8413 - val_acc: 0.3839\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.8739 - acc: 0.3608 - val_loss: 1.8353 - val_acc: 0.3853\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.8709 - acc: 0.3621 - val_loss: 1.8347 - val_acc: 0.3859\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.8709 - acc: 0.3614 - val_loss: 1.8514 - val_acc: 0.3776\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.8675 - acc: 0.3629 - val_loss: 1.8307 - val_acc: 0.3902\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.8646 - acc: 0.3637 - val_loss: 1.8338 - val_acc: 0.3853\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.8612 - acc: 0.3645 - val_loss: 1.8296 - val_acc: 0.3883\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.8602 - acc: 0.3661 - val_loss: 1.8235 - val_acc: 0.3916\n",
            "32890/32890 [==============================] - 0s 9us/sample - loss: 1.8427 - acc: 0.3630\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 4s 38us/sample - loss: 2.0864 - acc: 0.2746 - val_loss: 1.9087 - val_acc: 0.3400\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 3s 32us/sample - loss: 1.9580 - acc: 0.3276 - val_loss: 1.9134 - val_acc: 0.3345\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 3s 32us/sample - loss: 1.9389 - acc: 0.3348 - val_loss: 1.8714 - val_acc: 0.3449\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 3s 32us/sample - loss: 1.9264 - acc: 0.3387 - val_loss: 1.8781 - val_acc: 0.3413\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 3s 33us/sample - loss: 1.9190 - acc: 0.3437 - val_loss: 1.8667 - val_acc: 0.3432\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 3s 32us/sample - loss: 1.9099 - acc: 0.3461 - val_loss: 1.8695 - val_acc: 0.3405\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 3s 32us/sample - loss: 1.9063 - acc: 0.3490 - val_loss: 1.8684 - val_acc: 0.3372\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 3s 32us/sample - loss: 1.9012 - acc: 0.3495 - val_loss: 1.8507 - val_acc: 0.3467\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 3s 32us/sample - loss: 1.8971 - acc: 0.3517 - val_loss: 1.8536 - val_acc: 0.3481\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 3s 32us/sample - loss: 1.8925 - acc: 0.3532 - val_loss: 1.8473 - val_acc: 0.3512\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 3s 31us/sample - loss: 1.8891 - acc: 0.3555 - val_loss: 1.8560 - val_acc: 0.3465\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 3s 31us/sample - loss: 1.8828 - acc: 0.3567 - val_loss: 1.8346 - val_acc: 0.3520\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 3s 32us/sample - loss: 1.8808 - acc: 0.3580 - val_loss: 1.8566 - val_acc: 0.3383\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 3s 32us/sample - loss: 1.8802 - acc: 0.3574 - val_loss: 1.8296 - val_acc: 0.3588\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 3s 32us/sample - loss: 1.8775 - acc: 0.3589 - val_loss: 1.8218 - val_acc: 0.3657\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 3s 33us/sample - loss: 1.8734 - acc: 0.3602 - val_loss: 1.8329 - val_acc: 0.3544\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 4s 34us/sample - loss: 1.8707 - acc: 0.3611 - val_loss: 1.8233 - val_acc: 0.3634\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 4s 35us/sample - loss: 1.8701 - acc: 0.3614 - val_loss: 1.8253 - val_acc: 0.3603\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 4s 34us/sample - loss: 1.8683 - acc: 0.3615 - val_loss: 1.8222 - val_acc: 0.3634\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 3s 32us/sample - loss: 1.8642 - acc: 0.3642 - val_loss: 1.8132 - val_acc: 0.3665\n",
            "32889/32889 [==============================] - 0s 9us/sample - loss: 1.8328 - acc: 0.3960\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 2.0223 - acc: 0.2991 - val_loss: 1.8635 - val_acc: 0.3750\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.9207 - acc: 0.3384 - val_loss: 1.8564 - val_acc: 0.3793\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.9018 - acc: 0.3467 - val_loss: 1.8204 - val_acc: 0.3939\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 30us/sample - loss: 1.8888 - acc: 0.3514 - val_loss: 1.8145 - val_acc: 0.3898\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.8800 - acc: 0.3562 - val_loss: 1.8032 - val_acc: 0.3969\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8707 - acc: 0.3588 - val_loss: 1.8146 - val_acc: 0.3926\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.8631 - acc: 0.3621 - val_loss: 1.7929 - val_acc: 0.4001\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.8551 - acc: 0.3641 - val_loss: 1.7980 - val_acc: 0.4001\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.8508 - acc: 0.3645 - val_loss: 1.7816 - val_acc: 0.4015\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.8464 - acc: 0.3673 - val_loss: 1.7998 - val_acc: 0.3977\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.8437 - acc: 0.3660 - val_loss: 1.7978 - val_acc: 0.3996\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.8394 - acc: 0.3696 - val_loss: 1.7817 - val_acc: 0.4053\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.8351 - acc: 0.3707 - val_loss: 1.7871 - val_acc: 0.3991\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.8315 - acc: 0.3735 - val_loss: 1.7968 - val_acc: 0.3984\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.8303 - acc: 0.3731 - val_loss: 1.7744 - val_acc: 0.4059\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.8259 - acc: 0.3748 - val_loss: 1.7786 - val_acc: 0.4036\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.8218 - acc: 0.3771 - val_loss: 1.7775 - val_acc: 0.4032\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.8214 - acc: 0.3766 - val_loss: 1.7760 - val_acc: 0.4066\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.8181 - acc: 0.3774 - val_loss: 1.7759 - val_acc: 0.4039\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.8154 - acc: 0.3774 - val_loss: 1.7762 - val_acc: 0.4060\n",
            "32890/32890 [==============================] - 0s 9us/sample - loss: 1.8267 - acc: 0.3677\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 2.0221 - acc: 0.3027 - val_loss: 1.8889 - val_acc: 0.3648\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.9091 - acc: 0.3442 - val_loss: 1.8666 - val_acc: 0.3701\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.8872 - acc: 0.3526 - val_loss: 1.8375 - val_acc: 0.3802\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.8734 - acc: 0.3574 - val_loss: 1.8328 - val_acc: 0.3837\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.8652 - acc: 0.3634 - val_loss: 1.8366 - val_acc: 0.3790\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.8535 - acc: 0.3669 - val_loss: 1.8366 - val_acc: 0.3813\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.8482 - acc: 0.3680 - val_loss: 1.8060 - val_acc: 0.3921\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.8412 - acc: 0.3715 - val_loss: 1.8080 - val_acc: 0.3916\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.8347 - acc: 0.3748 - val_loss: 1.8085 - val_acc: 0.3929\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.8302 - acc: 0.3757 - val_loss: 1.8082 - val_acc: 0.3887\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.8244 - acc: 0.3784 - val_loss: 1.7862 - val_acc: 0.3991\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.8229 - acc: 0.3788 - val_loss: 1.7868 - val_acc: 0.4035\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.8175 - acc: 0.3813 - val_loss: 1.7966 - val_acc: 0.3969\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.8148 - acc: 0.3807 - val_loss: 1.7923 - val_acc: 0.3981\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.8093 - acc: 0.3824 - val_loss: 1.7956 - val_acc: 0.3998\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.8077 - acc: 0.3838 - val_loss: 1.7877 - val_acc: 0.3959\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.8060 - acc: 0.3857 - val_loss: 1.7905 - val_acc: 0.3979\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.8040 - acc: 0.3860 - val_loss: 1.7908 - val_acc: 0.3975\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.7997 - acc: 0.3864 - val_loss: 1.7823 - val_acc: 0.3986\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 31us/sample - loss: 1.7973 - acc: 0.3878 - val_loss: 1.7776 - val_acc: 0.4035\n",
            "32890/32890 [==============================] - 0s 9us/sample - loss: 1.8811 - acc: 0.3430\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 4s 38us/sample - loss: 2.0385 - acc: 0.2882 - val_loss: 1.9141 - val_acc: 0.3499\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.9315 - acc: 0.3303 - val_loss: 1.8741 - val_acc: 0.3721\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.9117 - acc: 0.3392 - val_loss: 1.8585 - val_acc: 0.3811\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.8953 - acc: 0.3451 - val_loss: 1.8605 - val_acc: 0.3787\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.8864 - acc: 0.3497 - val_loss: 1.8421 - val_acc: 0.3819\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.8779 - acc: 0.3531 - val_loss: 1.8355 - val_acc: 0.3827\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.8694 - acc: 0.3564 - val_loss: 1.8273 - val_acc: 0.3897\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.8653 - acc: 0.3581 - val_loss: 1.8253 - val_acc: 0.3896\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.8606 - acc: 0.3594 - val_loss: 1.8229 - val_acc: 0.3874\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.8556 - acc: 0.3627 - val_loss: 1.8198 - val_acc: 0.3852\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.8502 - acc: 0.3639 - val_loss: 1.8269 - val_acc: 0.3847\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.8456 - acc: 0.3667 - val_loss: 1.8070 - val_acc: 0.3922\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8427 - acc: 0.3665 - val_loss: 1.8040 - val_acc: 0.3952\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 4s 33us/sample - loss: 1.8394 - acc: 0.3672 - val_loss: 1.8081 - val_acc: 0.3901\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.8362 - acc: 0.3696 - val_loss: 1.8124 - val_acc: 0.3856\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.8330 - acc: 0.3702 - val_loss: 1.7991 - val_acc: 0.3905\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8303 - acc: 0.3717 - val_loss: 1.8025 - val_acc: 0.3915\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8269 - acc: 0.3731 - val_loss: 1.8041 - val_acc: 0.3956\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 4s 33us/sample - loss: 1.8250 - acc: 0.3734 - val_loss: 1.8112 - val_acc: 0.3870\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8227 - acc: 0.3746 - val_loss: 1.8084 - val_acc: 0.3912\n",
            "32890/32890 [==============================] - 0s 9us/sample - loss: 1.7890 - acc: 0.3892\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 4s 41us/sample - loss: 2.0254 - acc: 0.2973 - val_loss: 1.8992 - val_acc: 0.3549\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 4s 33us/sample - loss: 1.9193 - acc: 0.3402 - val_loss: 1.8806 - val_acc: 0.3652\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.8982 - acc: 0.3469 - val_loss: 1.8594 - val_acc: 0.3730\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.8859 - acc: 0.3535 - val_loss: 1.8555 - val_acc: 0.3729\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 4s 33us/sample - loss: 1.8762 - acc: 0.3562 - val_loss: 1.8588 - val_acc: 0.3777\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.8682 - acc: 0.3598 - val_loss: 1.8373 - val_acc: 0.3854\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.8618 - acc: 0.3635 - val_loss: 1.8474 - val_acc: 0.3801\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8588 - acc: 0.3640 - val_loss: 1.8434 - val_acc: 0.3842\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8512 - acc: 0.3675 - val_loss: 1.8333 - val_acc: 0.3807\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8471 - acc: 0.3667 - val_loss: 1.8354 - val_acc: 0.3814\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.8425 - acc: 0.3696 - val_loss: 1.8342 - val_acc: 0.3841\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.8387 - acc: 0.3698 - val_loss: 1.8243 - val_acc: 0.3850\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8340 - acc: 0.3725 - val_loss: 1.8205 - val_acc: 0.3853\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8300 - acc: 0.3733 - val_loss: 1.8318 - val_acc: 0.3799\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8283 - acc: 0.3737 - val_loss: 1.8292 - val_acc: 0.3847\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8253 - acc: 0.3748 - val_loss: 1.8275 - val_acc: 0.3864\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8220 - acc: 0.3767 - val_loss: 1.8129 - val_acc: 0.3916\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.8209 - acc: 0.3766 - val_loss: 1.8181 - val_acc: 0.3899\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.8161 - acc: 0.3793 - val_loss: 1.8251 - val_acc: 0.3841\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8151 - acc: 0.3785 - val_loss: 1.8197 - val_acc: 0.3890\n",
            "32890/32890 [==============================] - 0s 10us/sample - loss: 1.8246 - acc: 0.3742\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 4s 41us/sample - loss: 2.0372 - acc: 0.2914 - val_loss: 1.9035 - val_acc: 0.3358\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 3s 33us/sample - loss: 1.9268 - acc: 0.3357 - val_loss: 1.8740 - val_acc: 0.3463\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 3s 33us/sample - loss: 1.9054 - acc: 0.3442 - val_loss: 1.8723 - val_acc: 0.3390\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 4s 35us/sample - loss: 1.8936 - acc: 0.3482 - val_loss: 1.8678 - val_acc: 0.3471\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 4s 36us/sample - loss: 1.8845 - acc: 0.3516 - val_loss: 1.8611 - val_acc: 0.3474\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 4s 37us/sample - loss: 1.8752 - acc: 0.3553 - val_loss: 1.8483 - val_acc: 0.3511\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 4s 34us/sample - loss: 1.8707 - acc: 0.3557 - val_loss: 1.8509 - val_acc: 0.3510\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 4s 34us/sample - loss: 1.8646 - acc: 0.3587 - val_loss: 1.8205 - val_acc: 0.3636\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 4s 34us/sample - loss: 1.8584 - acc: 0.3621 - val_loss: 1.8372 - val_acc: 0.3550\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 4s 33us/sample - loss: 1.8544 - acc: 0.3627 - val_loss: 1.8245 - val_acc: 0.3558\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 3s 33us/sample - loss: 1.8489 - acc: 0.3659 - val_loss: 1.8195 - val_acc: 0.3611\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 4s 35us/sample - loss: 1.8456 - acc: 0.3655 - val_loss: 1.8104 - val_acc: 0.3685\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 4s 36us/sample - loss: 1.8410 - acc: 0.3681 - val_loss: 1.7955 - val_acc: 0.3743\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 3s 33us/sample - loss: 1.8371 - acc: 0.3702 - val_loss: 1.7926 - val_acc: 0.3777\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 3s 33us/sample - loss: 1.8332 - acc: 0.3708 - val_loss: 1.8049 - val_acc: 0.3694\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 4s 33us/sample - loss: 1.8292 - acc: 0.3718 - val_loss: 1.8010 - val_acc: 0.3702\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 4s 34us/sample - loss: 1.8276 - acc: 0.3734 - val_loss: 1.7868 - val_acc: 0.3784\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 3s 33us/sample - loss: 1.8224 - acc: 0.3742 - val_loss: 1.8041 - val_acc: 0.3694\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 4s 33us/sample - loss: 1.8220 - acc: 0.3739 - val_loss: 1.7990 - val_acc: 0.3736\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 4s 34us/sample - loss: 1.8215 - acc: 0.3746 - val_loss: 1.7894 - val_acc: 0.3808\n",
            "32889/32889 [==============================] - 0s 9us/sample - loss: 1.8288 - acc: 0.3968\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 4s 42us/sample - loss: 2.0582 - acc: 0.2867 - val_loss: 1.8770 - val_acc: 0.3714\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.9556 - acc: 0.3280 - val_loss: 1.8502 - val_acc: 0.3795\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 4s 33us/sample - loss: 1.9371 - acc: 0.3360 - val_loss: 1.8529 - val_acc: 0.3785\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.9271 - acc: 0.3397 - val_loss: 1.8472 - val_acc: 0.3815\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.9190 - acc: 0.3437 - val_loss: 1.8352 - val_acc: 0.3863\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.9098 - acc: 0.3476 - val_loss: 1.8353 - val_acc: 0.3898\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.9045 - acc: 0.3490 - val_loss: 1.8209 - val_acc: 0.3958\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 4s 33us/sample - loss: 1.8983 - acc: 0.3516 - val_loss: 1.8173 - val_acc: 0.3948\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8917 - acc: 0.3549 - val_loss: 1.8114 - val_acc: 0.3969\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 4s 33us/sample - loss: 1.8881 - acc: 0.3565 - val_loss: 1.8049 - val_acc: 0.4027\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8832 - acc: 0.3581 - val_loss: 1.8038 - val_acc: 0.4022\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8788 - acc: 0.3601 - val_loss: 1.7979 - val_acc: 0.4079\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8784 - acc: 0.3610 - val_loss: 1.7950 - val_acc: 0.4063\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8712 - acc: 0.3628 - val_loss: 1.7868 - val_acc: 0.4086\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.8695 - acc: 0.3642 - val_loss: 1.7922 - val_acc: 0.4062\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8685 - acc: 0.3639 - val_loss: 1.8004 - val_acc: 0.4002\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8639 - acc: 0.3662 - val_loss: 1.7963 - val_acc: 0.4068\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8623 - acc: 0.3670 - val_loss: 1.7937 - val_acc: 0.4051\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8625 - acc: 0.3661 - val_loss: 1.7982 - val_acc: 0.4061\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.8604 - acc: 0.3681 - val_loss: 1.7823 - val_acc: 0.4090\n",
            "32890/32890 [==============================] - 0s 9us/sample - loss: 1.8424 - acc: 0.3609\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 4s 42us/sample - loss: 2.0570 - acc: 0.2906 - val_loss: 1.8948 - val_acc: 0.3658\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.9388 - acc: 0.3368 - val_loss: 1.8637 - val_acc: 0.3803\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.9185 - acc: 0.3443 - val_loss: 1.8602 - val_acc: 0.3761\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 4s 33us/sample - loss: 1.9077 - acc: 0.3500 - val_loss: 1.8491 - val_acc: 0.3804\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8983 - acc: 0.3532 - val_loss: 1.8536 - val_acc: 0.3739\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8908 - acc: 0.3558 - val_loss: 1.8335 - val_acc: 0.3811\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 4s 33us/sample - loss: 1.8836 - acc: 0.3616 - val_loss: 1.8378 - val_acc: 0.3867\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.8813 - acc: 0.3598 - val_loss: 1.8134 - val_acc: 0.3929\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.8742 - acc: 0.3640 - val_loss: 1.8405 - val_acc: 0.3804\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.8693 - acc: 0.3640 - val_loss: 1.8332 - val_acc: 0.3847\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.8657 - acc: 0.3666 - val_loss: 1.8257 - val_acc: 0.3868\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.8630 - acc: 0.3683 - val_loss: 1.8127 - val_acc: 0.3950\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.8578 - acc: 0.3701 - val_loss: 1.8075 - val_acc: 0.3974\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.8562 - acc: 0.3712 - val_loss: 1.8147 - val_acc: 0.3902\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8500 - acc: 0.3727 - val_loss: 1.7996 - val_acc: 0.4000\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.8502 - acc: 0.3736 - val_loss: 1.8015 - val_acc: 0.3959\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.8494 - acc: 0.3739 - val_loss: 1.7980 - val_acc: 0.4006\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8462 - acc: 0.3758 - val_loss: 1.8066 - val_acc: 0.3953\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8436 - acc: 0.3771 - val_loss: 1.8008 - val_acc: 0.3993\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8408 - acc: 0.3766 - val_loss: 1.8049 - val_acc: 0.3954\n",
            "32890/32890 [==============================] - 0s 10us/sample - loss: 1.8815 - acc: 0.3419\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 5s 43us/sample - loss: 2.0793 - acc: 0.2775 - val_loss: 1.9060 - val_acc: 0.3688\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.9602 - acc: 0.3242 - val_loss: 1.8641 - val_acc: 0.3807\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.9411 - acc: 0.3319 - val_loss: 1.8826 - val_acc: 0.3693\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.9311 - acc: 0.3360 - val_loss: 1.8613 - val_acc: 0.3779\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.9208 - acc: 0.3414 - val_loss: 1.8611 - val_acc: 0.3720\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.9144 - acc: 0.3449 - val_loss: 1.8468 - val_acc: 0.3832\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.9083 - acc: 0.3460 - val_loss: 1.8425 - val_acc: 0.3837\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.9000 - acc: 0.3485 - val_loss: 1.8429 - val_acc: 0.3763\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.8968 - acc: 0.3501 - val_loss: 1.8405 - val_acc: 0.3813\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.8925 - acc: 0.3528 - val_loss: 1.8393 - val_acc: 0.3850\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.8897 - acc: 0.3534 - val_loss: 1.8306 - val_acc: 0.3883\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.8844 - acc: 0.3559 - val_loss: 1.8242 - val_acc: 0.3874\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.8806 - acc: 0.3565 - val_loss: 1.8181 - val_acc: 0.3900\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.8782 - acc: 0.3577 - val_loss: 1.8252 - val_acc: 0.3845\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 4s 37us/sample - loss: 1.8744 - acc: 0.3585 - val_loss: 1.8217 - val_acc: 0.3885\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.8726 - acc: 0.3605 - val_loss: 1.8309 - val_acc: 0.3875\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.8712 - acc: 0.3614 - val_loss: 1.8188 - val_acc: 0.3898\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.8695 - acc: 0.3622 - val_loss: 1.8288 - val_acc: 0.3859\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.8670 - acc: 0.3620 - val_loss: 1.8280 - val_acc: 0.3858\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.8641 - acc: 0.3639 - val_loss: 1.8236 - val_acc: 0.3852\n",
            "32890/32890 [==============================] - 0s 11us/sample - loss: 1.8058 - acc: 0.3871\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 2.0796 - acc: 0.2799 - val_loss: 1.9359 - val_acc: 0.3502\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.9562 - acc: 0.3267 - val_loss: 1.8963 - val_acc: 0.3629\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.9345 - acc: 0.3376 - val_loss: 1.8810 - val_acc: 0.3672\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.9221 - acc: 0.3406 - val_loss: 1.8743 - val_acc: 0.3725\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.9142 - acc: 0.3443 - val_loss: 1.8800 - val_acc: 0.3669\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.9089 - acc: 0.3478 - val_loss: 1.8694 - val_acc: 0.3773\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.9024 - acc: 0.3486 - val_loss: 1.8600 - val_acc: 0.3737\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.8936 - acc: 0.3530 - val_loss: 1.8569 - val_acc: 0.3744\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.8901 - acc: 0.3551 - val_loss: 1.8541 - val_acc: 0.3826\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 4s 33us/sample - loss: 1.8858 - acc: 0.3567 - val_loss: 1.8585 - val_acc: 0.3782\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.8817 - acc: 0.3584 - val_loss: 1.8432 - val_acc: 0.3885\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.8785 - acc: 0.3599 - val_loss: 1.8451 - val_acc: 0.3898\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 4s 33us/sample - loss: 1.8765 - acc: 0.3601 - val_loss: 1.8425 - val_acc: 0.3875\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 4s 35us/sample - loss: 1.8719 - acc: 0.3624 - val_loss: 1.8335 - val_acc: 0.3853\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 4s 36us/sample - loss: 1.8703 - acc: 0.3632 - val_loss: 1.8443 - val_acc: 0.3832\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 32us/sample - loss: 1.8661 - acc: 0.3638 - val_loss: 1.8392 - val_acc: 0.3857\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 4s 33us/sample - loss: 1.8653 - acc: 0.3639 - val_loss: 1.8309 - val_acc: 0.3920\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 4s 34us/sample - loss: 1.8626 - acc: 0.3657 - val_loss: 1.8265 - val_acc: 0.3893\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.8599 - acc: 0.3669 - val_loss: 1.8215 - val_acc: 0.3972\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 33us/sample - loss: 1.8590 - acc: 0.3659 - val_loss: 1.8256 - val_acc: 0.3885\n",
            "32890/32890 [==============================] - 0s 10us/sample - loss: 1.8432 - acc: 0.3631\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 5s 43us/sample - loss: 2.0652 - acc: 0.2838 - val_loss: 1.8985 - val_acc: 0.3468\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 4s 34us/sample - loss: 1.9564 - acc: 0.3257 - val_loss: 1.8973 - val_acc: 0.3377\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 3s 33us/sample - loss: 1.9360 - acc: 0.3349 - val_loss: 1.8806 - val_acc: 0.3380\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 4s 33us/sample - loss: 1.9253 - acc: 0.3406 - val_loss: 1.8635 - val_acc: 0.3457\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 3s 32us/sample - loss: 1.9156 - acc: 0.3453 - val_loss: 1.8678 - val_acc: 0.3446\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 3s 32us/sample - loss: 1.9092 - acc: 0.3480 - val_loss: 1.8619 - val_acc: 0.3492\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 4s 33us/sample - loss: 1.9027 - acc: 0.3494 - val_loss: 1.8467 - val_acc: 0.3520\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 3s 33us/sample - loss: 1.8960 - acc: 0.3494 - val_loss: 1.8626 - val_acc: 0.3464\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 3s 33us/sample - loss: 1.8923 - acc: 0.3545 - val_loss: 1.8453 - val_acc: 0.3501\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 3s 33us/sample - loss: 1.8902 - acc: 0.3555 - val_loss: 1.8324 - val_acc: 0.3546\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 4s 33us/sample - loss: 1.8831 - acc: 0.3572 - val_loss: 1.8304 - val_acc: 0.3546\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 4s 33us/sample - loss: 1.8829 - acc: 0.3576 - val_loss: 1.8157 - val_acc: 0.3688\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 4s 34us/sample - loss: 1.8759 - acc: 0.3605 - val_loss: 1.8271 - val_acc: 0.3532\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 4s 34us/sample - loss: 1.8737 - acc: 0.3589 - val_loss: 1.8116 - val_acc: 0.3679\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 3s 33us/sample - loss: 1.8719 - acc: 0.3604 - val_loss: 1.8101 - val_acc: 0.3706\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 3s 33us/sample - loss: 1.8699 - acc: 0.3609 - val_loss: 1.8181 - val_acc: 0.3613\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 4s 34us/sample - loss: 1.8673 - acc: 0.3629 - val_loss: 1.8219 - val_acc: 0.3628\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 4s 34us/sample - loss: 1.8670 - acc: 0.3628 - val_loss: 1.8179 - val_acc: 0.3669\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 4s 35us/sample - loss: 1.8646 - acc: 0.3644 - val_loss: 1.8110 - val_acc: 0.3701\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 3s 33us/sample - loss: 1.8612 - acc: 0.3638 - val_loss: 1.8167 - val_acc: 0.3695\n",
            "32889/32889 [==============================] - 0s 10us/sample - loss: 1.8324 - acc: 0.3961\n",
            "Train on 131559 samples, validate on 32890 samples\n",
            "Epoch 1/20\n",
            "131559/131559 [==============================] - 5s 41us/sample - loss: 2.0077 - acc: 0.3019 - val_loss: 1.8952 - val_acc: 0.3714\n",
            "Epoch 2/20\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.9160 - acc: 0.3377 - val_loss: 1.8822 - val_acc: 0.3744\n",
            "Epoch 3/20\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.8979 - acc: 0.3455 - val_loss: 1.8564 - val_acc: 0.3847\n",
            "Epoch 4/20\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.8833 - acc: 0.3507 - val_loss: 1.8502 - val_acc: 0.3864\n",
            "Epoch 5/20\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.8724 - acc: 0.3559 - val_loss: 1.8447 - val_acc: 0.3912\n",
            "Epoch 6/20\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.8618 - acc: 0.3592 - val_loss: 1.8487 - val_acc: 0.3861\n",
            "Epoch 7/20\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.8535 - acc: 0.3618 - val_loss: 1.8352 - val_acc: 0.3919\n",
            "Epoch 8/20\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.8488 - acc: 0.3638 - val_loss: 1.8179 - val_acc: 0.3969\n",
            "Epoch 9/20\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.8426 - acc: 0.3653 - val_loss: 1.8236 - val_acc: 0.3929\n",
            "Epoch 10/20\n",
            "131559/131559 [==============================] - 5s 34us/sample - loss: 1.8386 - acc: 0.3683 - val_loss: 1.8322 - val_acc: 0.3939\n",
            "Epoch 11/20\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.8348 - acc: 0.3700 - val_loss: 1.8223 - val_acc: 0.3987\n",
            "Epoch 12/20\n",
            "131559/131559 [==============================] - 5s 36us/sample - loss: 1.8316 - acc: 0.3702 - val_loss: 1.8124 - val_acc: 0.3973\n",
            "Epoch 13/20\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.8274 - acc: 0.3729 - val_loss: 1.8089 - val_acc: 0.3996\n",
            "Epoch 14/20\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.8255 - acc: 0.3724 - val_loss: 1.8022 - val_acc: 0.4011\n",
            "Epoch 15/20\n",
            "131559/131559 [==============================] - 5s 34us/sample - loss: 1.8216 - acc: 0.3744 - val_loss: 1.8100 - val_acc: 0.3997\n",
            "Epoch 16/20\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.8181 - acc: 0.3761 - val_loss: 1.8140 - val_acc: 0.3968\n",
            "Epoch 17/20\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.8172 - acc: 0.3759 - val_loss: 1.8070 - val_acc: 0.3997\n",
            "Epoch 18/20\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.8148 - acc: 0.3760 - val_loss: 1.8015 - val_acc: 0.4026\n",
            "Epoch 19/20\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.8121 - acc: 0.3777 - val_loss: 1.8138 - val_acc: 0.3992\n",
            "Epoch 20/20\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.8111 - acc: 0.3784 - val_loss: 1.8125 - val_acc: 0.3969\n",
            "Best: 0.37505257725715635 using {'weight_constraint': 1, 'weight_rate': 0.1}\n",
            "Means: 0.37505257725715635, Stdev: 0.020111915741015313 with: {'weight_constraint': 1, 'weight_rate': 0.1}\n",
            "Means: 0.3704007089138031, Stdev: 0.019714134705252035 with: {'weight_constraint': 1, 'weight_rate': 0.2}\n",
            "Means: 0.3741647720336914, Stdev: 0.01870633895768674 with: {'weight_constraint': 2, 'weight_rate': 0.1}\n",
            "Means: 0.36982910633087157, Stdev: 0.019459299657740656 with: {'weight_constraint': 2, 'weight_rate': 0.2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rV3gl4GGMdhy",
        "colab_type": "text"
      },
      "source": [
        "### Batch Size.*(100)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNYIM33-Mbs4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function for NN model.\n",
        "def create_model():\n",
        "      # set the model.\n",
        "    model = Sequential()\n",
        "    # hidden layers.\n",
        "    model.add(Dense(192, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(96, kernel_initializer='normal', activation='relu'))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(48, kernel_initializer='normal', activation='relu'))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(24, kernel_initializer='normal', activation='relu'))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(12, kernel_initializer='normal', activation='softmax'))\n",
        "    # function for top 3.\n",
        "    def top_3_accuracy(y_true, y_pred):\n",
        "        return top_k_categorical_accuracy(y_true, y_pred, k=3)\n",
        "\n",
        "    # set the optimizer.\n",
        "    optimizer = Adam(lr=0.001)\n",
        "    # compile the model.\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                        optimizer=optimizer,\n",
        "                        metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2GHJazLMr5I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set the KerasClassifier model.\n",
        "model = KerasClassifier(build_fn=create_model, epochs=20, validation_split=0.2, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU_3kHZPUULc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set the parameters for batch size.\n",
        "batch_size = [50, 100, 200, 500, 1000]\n",
        "param_grid = dict(batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3RS7HIFNSxV",
        "colab_type": "code",
        "outputId": "6ad0dd42-d420-40c5-ad26-0d5e4dac84c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# set the gridsearch.\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
        "grid_result = grid.fit(X_scaled, y_train)\n",
        "# show the results.\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 10s 99us/sample - loss: 1.9827 - acc: 0.3147 - val_loss: 1.8624 - val_acc: 0.3850\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 9s 85us/sample - loss: 1.9124 - acc: 0.3432 - val_loss: 1.8181 - val_acc: 0.3947\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 9s 85us/sample - loss: 1.8926 - acc: 0.3504 - val_loss: 1.8267 - val_acc: 0.3885\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 9s 84us/sample - loss: 1.8765 - acc: 0.3571 - val_loss: 1.8127 - val_acc: 0.3903\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 11s 104us/sample - loss: 1.8641 - acc: 0.3628 - val_loss: 1.7984 - val_acc: 0.3988\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 9s 89us/sample - loss: 1.8554 - acc: 0.3651 - val_loss: 1.7938 - val_acc: 0.3988\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 9s 86us/sample - loss: 1.8479 - acc: 0.3677 - val_loss: 1.7921 - val_acc: 0.4018\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 9s 87us/sample - loss: 1.8415 - acc: 0.3709 - val_loss: 1.8092 - val_acc: 0.3937\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 9s 89us/sample - loss: 1.8362 - acc: 0.3730 - val_loss: 1.8020 - val_acc: 0.3954\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 9s 87us/sample - loss: 1.8334 - acc: 0.3733 - val_loss: 1.7844 - val_acc: 0.4070\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 9s 88us/sample - loss: 1.8286 - acc: 0.3742 - val_loss: 1.7842 - val_acc: 0.4022\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 9s 86us/sample - loss: 1.8262 - acc: 0.3771 - val_loss: 1.7852 - val_acc: 0.4043\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 9s 86us/sample - loss: 1.8222 - acc: 0.3752 - val_loss: 1.7811 - val_acc: 0.4034\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 9s 86us/sample - loss: 1.8202 - acc: 0.3782 - val_loss: 1.7664 - val_acc: 0.4105\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 9s 84us/sample - loss: 1.8196 - acc: 0.3779 - val_loss: 1.7898 - val_acc: 0.3962\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 9s 85us/sample - loss: 1.8141 - acc: 0.3792 - val_loss: 1.7713 - val_acc: 0.4077\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 9s 84us/sample - loss: 1.8130 - acc: 0.3785 - val_loss: 1.7729 - val_acc: 0.4048\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 9s 85us/sample - loss: 1.8099 - acc: 0.3799 - val_loss: 1.7783 - val_acc: 0.4053\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 9s 85us/sample - loss: 1.8085 - acc: 0.3813 - val_loss: 1.7952 - val_acc: 0.3967\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 9s 87us/sample - loss: 1.8090 - acc: 0.3821 - val_loss: 1.7737 - val_acc: 0.4057\n",
            "32890/32890 [==============================] - 1s 33us/sample - loss: 1.8365 - acc: 0.3648\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 10s 98us/sample - loss: 1.9659 - acc: 0.3225 - val_loss: 1.8549 - val_acc: 0.3760\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 9s 87us/sample - loss: 1.8883 - acc: 0.3522 - val_loss: 1.8291 - val_acc: 0.3775\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 9s 87us/sample - loss: 1.8674 - acc: 0.3624 - val_loss: 1.8059 - val_acc: 0.3886\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 9s 90us/sample - loss: 1.8550 - acc: 0.3673 - val_loss: 1.8113 - val_acc: 0.3897\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 10s 91us/sample - loss: 1.8439 - acc: 0.3706 - val_loss: 1.8027 - val_acc: 0.3928\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 9s 90us/sample - loss: 1.8349 - acc: 0.3748 - val_loss: 1.7889 - val_acc: 0.3974\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 10s 91us/sample - loss: 1.8293 - acc: 0.3772 - val_loss: 1.8071 - val_acc: 0.3889\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 10s 93us/sample - loss: 1.8229 - acc: 0.3793 - val_loss: 1.8025 - val_acc: 0.3923\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 10s 91us/sample - loss: 1.8189 - acc: 0.3810 - val_loss: 1.7951 - val_acc: 0.3984\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 10s 92us/sample - loss: 1.8122 - acc: 0.3834 - val_loss: 1.7789 - val_acc: 0.4029\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 10s 93us/sample - loss: 1.8101 - acc: 0.3831 - val_loss: 1.7873 - val_acc: 0.4001\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 10s 99us/sample - loss: 1.8051 - acc: 0.3844 - val_loss: 1.7924 - val_acc: 0.3937\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 10s 98us/sample - loss: 1.8044 - acc: 0.3872 - val_loss: 1.8022 - val_acc: 0.3939\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 10s 99us/sample - loss: 1.8015 - acc: 0.3861 - val_loss: 1.7986 - val_acc: 0.3896\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 10s 99us/sample - loss: 1.7990 - acc: 0.3876 - val_loss: 1.8003 - val_acc: 0.3938\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 10s 98us/sample - loss: 1.7959 - acc: 0.3879 - val_loss: 1.7938 - val_acc: 0.3910\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 12s 111us/sample - loss: 1.7930 - acc: 0.3888 - val_loss: 1.8061 - val_acc: 0.3882\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 10s 95us/sample - loss: 1.7922 - acc: 0.3887 - val_loss: 1.7982 - val_acc: 0.3930\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 10s 91us/sample - loss: 1.7910 - acc: 0.3892 - val_loss: 1.8141 - val_acc: 0.3867\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 9s 90us/sample - loss: 1.7883 - acc: 0.3904 - val_loss: 1.7960 - val_acc: 0.3941\n",
            "32890/32890 [==============================] - 1s 35us/sample - loss: 1.8760 - acc: 0.3435\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 11s 102us/sample - loss: 1.9827 - acc: 0.3094 - val_loss: 1.8733 - val_acc: 0.3596\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 10s 90us/sample - loss: 1.9167 - acc: 0.3383 - val_loss: 1.8544 - val_acc: 0.3763\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 10s 91us/sample - loss: 1.8972 - acc: 0.3465 - val_loss: 1.8531 - val_acc: 0.3701\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 10s 90us/sample - loss: 1.8829 - acc: 0.3522 - val_loss: 1.8389 - val_acc: 0.3793\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 9s 89us/sample - loss: 1.8726 - acc: 0.3569 - val_loss: 1.8151 - val_acc: 0.3938\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 9s 87us/sample - loss: 1.8646 - acc: 0.3594 - val_loss: 1.8455 - val_acc: 0.3789\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 9s 88us/sample - loss: 1.8572 - acc: 0.3638 - val_loss: 1.8149 - val_acc: 0.3903\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 9s 90us/sample - loss: 1.8514 - acc: 0.3626 - val_loss: 1.8049 - val_acc: 0.3891\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 10s 93us/sample - loss: 1.8458 - acc: 0.3655 - val_loss: 1.8216 - val_acc: 0.3851\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 10s 94us/sample - loss: 1.8412 - acc: 0.3685 - val_loss: 1.8071 - val_acc: 0.3918\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 10s 91us/sample - loss: 1.8374 - acc: 0.3695 - val_loss: 1.8258 - val_acc: 0.3851\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 10s 92us/sample - loss: 1.8348 - acc: 0.3714 - val_loss: 1.8155 - val_acc: 0.3874\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 10s 92us/sample - loss: 1.8302 - acc: 0.3715 - val_loss: 1.8070 - val_acc: 0.3929\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 10s 91us/sample - loss: 1.8282 - acc: 0.3732 - val_loss: 1.8304 - val_acc: 0.3806\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 10s 92us/sample - loss: 1.8245 - acc: 0.3742 - val_loss: 1.8154 - val_acc: 0.3868\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 10s 93us/sample - loss: 1.8251 - acc: 0.3730 - val_loss: 1.8148 - val_acc: 0.3896\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 10s 93us/sample - loss: 1.8217 - acc: 0.3743 - val_loss: 1.7996 - val_acc: 0.3953\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 10s 91us/sample - loss: 1.8182 - acc: 0.3756 - val_loss: 1.8082 - val_acc: 0.3931\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 9s 87us/sample - loss: 1.8184 - acc: 0.3765 - val_loss: 1.8128 - val_acc: 0.3891\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 9s 90us/sample - loss: 1.8144 - acc: 0.3775 - val_loss: 1.8119 - val_acc: 0.3852\n",
            "32890/32890 [==============================] - 1s 36us/sample - loss: 1.7900 - acc: 0.3857\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 12s 110us/sample - loss: 1.9707 - acc: 0.3190 - val_loss: 1.8831 - val_acc: 0.3701\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 11s 100us/sample - loss: 1.9035 - acc: 0.3468 - val_loss: 1.8630 - val_acc: 0.3734\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 10s 98us/sample - loss: 1.8852 - acc: 0.3543 - val_loss: 1.8681 - val_acc: 0.3714\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 10s 97us/sample - loss: 1.8731 - acc: 0.3581 - val_loss: 1.8590 - val_acc: 0.3794\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 10s 96us/sample - loss: 1.8644 - acc: 0.3613 - val_loss: 1.8418 - val_acc: 0.3786\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 11s 102us/sample - loss: 1.8570 - acc: 0.3631 - val_loss: 1.8335 - val_acc: 0.3882\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 11s 103us/sample - loss: 1.8516 - acc: 0.3664 - val_loss: 1.8297 - val_acc: 0.3877\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 11s 109us/sample - loss: 1.8446 - acc: 0.3683 - val_loss: 1.8208 - val_acc: 0.3884\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 10s 99us/sample - loss: 1.8396 - acc: 0.3718 - val_loss: 1.8234 - val_acc: 0.3887\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 10s 96us/sample - loss: 1.8360 - acc: 0.3728 - val_loss: 1.8354 - val_acc: 0.3822\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 10s 97us/sample - loss: 1.8296 - acc: 0.3749 - val_loss: 1.8261 - val_acc: 0.3908\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 10s 97us/sample - loss: 1.8275 - acc: 0.3749 - val_loss: 1.8102 - val_acc: 0.3931\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 10s 96us/sample - loss: 1.8240 - acc: 0.3773 - val_loss: 1.8246 - val_acc: 0.3899\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 10s 97us/sample - loss: 1.8219 - acc: 0.3769 - val_loss: 1.8260 - val_acc: 0.3895\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 10s 98us/sample - loss: 1.8192 - acc: 0.3789 - val_loss: 1.8185 - val_acc: 0.3910\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 10s 94us/sample - loss: 1.8156 - acc: 0.3796 - val_loss: 1.8269 - val_acc: 0.3866\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 10s 93us/sample - loss: 1.8142 - acc: 0.3808 - val_loss: 1.8140 - val_acc: 0.3945\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 10s 95us/sample - loss: 1.8116 - acc: 0.3812 - val_loss: 1.8299 - val_acc: 0.3858\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 10s 94us/sample - loss: 1.8093 - acc: 0.3804 - val_loss: 1.8289 - val_acc: 0.3875\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 10s 94us/sample - loss: 1.8065 - acc: 0.3815 - val_loss: 1.8112 - val_acc: 0.3938\n",
            "32890/32890 [==============================] - 1s 35us/sample - loss: 1.8296 - acc: 0.3716\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 11s 106us/sample - loss: 1.9740 - acc: 0.3162 - val_loss: 1.8864 - val_acc: 0.3314\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 10s 96us/sample - loss: 1.9063 - acc: 0.3441 - val_loss: 1.8736 - val_acc: 0.3484\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 10s 99us/sample - loss: 1.8886 - acc: 0.3508 - val_loss: 1.8650 - val_acc: 0.3367\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 11s 102us/sample - loss: 1.8752 - acc: 0.3564 - val_loss: 1.8414 - val_acc: 0.3582\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 10s 96us/sample - loss: 1.8651 - acc: 0.3596 - val_loss: 1.8502 - val_acc: 0.3509\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 10s 99us/sample - loss: 1.8584 - acc: 0.3615 - val_loss: 1.8215 - val_acc: 0.3643\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 11s 100us/sample - loss: 1.8512 - acc: 0.3652 - val_loss: 1.8343 - val_acc: 0.3587\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 10s 98us/sample - loss: 1.8458 - acc: 0.3666 - val_loss: 1.7951 - val_acc: 0.3691\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 10s 99us/sample - loss: 1.8400 - acc: 0.3682 - val_loss: 1.8101 - val_acc: 0.3659\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 11s 100us/sample - loss: 1.8363 - acc: 0.3709 - val_loss: 1.8055 - val_acc: 0.3664\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 10s 98us/sample - loss: 1.8346 - acc: 0.3684 - val_loss: 1.8154 - val_acc: 0.3655\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 11s 102us/sample - loss: 1.8309 - acc: 0.3710 - val_loss: 1.8034 - val_acc: 0.3716\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 10s 95us/sample - loss: 1.8273 - acc: 0.3730 - val_loss: 1.7911 - val_acc: 0.3765\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 10s 96us/sample - loss: 1.8253 - acc: 0.3748 - val_loss: 1.8047 - val_acc: 0.3676\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 11s 100us/sample - loss: 1.8234 - acc: 0.3737 - val_loss: 1.8049 - val_acc: 0.3679\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 11s 101us/sample - loss: 1.8197 - acc: 0.3764 - val_loss: 1.7839 - val_acc: 0.3816\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 11s 108us/sample - loss: 1.8174 - acc: 0.3776 - val_loss: 1.7891 - val_acc: 0.3738\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 11s 108us/sample - loss: 1.8160 - acc: 0.3775 - val_loss: 1.7884 - val_acc: 0.3745\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 10s 99us/sample - loss: 1.8138 - acc: 0.3773 - val_loss: 1.8003 - val_acc: 0.3741\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 10s 99us/sample - loss: 1.8127 - acc: 0.3779 - val_loss: 1.7866 - val_acc: 0.3763\n",
            "32889/32889 [==============================] - 1s 37us/sample - loss: 1.8212 - acc: 0.3960\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 8s 74us/sample - loss: 1.9934 - acc: 0.3093 - val_loss: 1.8531 - val_acc: 0.3827\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 6s 57us/sample - loss: 1.9138 - acc: 0.3406 - val_loss: 1.8381 - val_acc: 0.3828\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 6s 56us/sample - loss: 1.8948 - acc: 0.3499 - val_loss: 1.8247 - val_acc: 0.3910\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 6s 57us/sample - loss: 1.8816 - acc: 0.3538 - val_loss: 1.8081 - val_acc: 0.3929\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 6s 59us/sample - loss: 1.8710 - acc: 0.3598 - val_loss: 1.7977 - val_acc: 0.3966\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 6s 57us/sample - loss: 1.8605 - acc: 0.3620 - val_loss: 1.8040 - val_acc: 0.3973\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 6s 60us/sample - loss: 1.8540 - acc: 0.3648 - val_loss: 1.7897 - val_acc: 0.4027\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 6s 58us/sample - loss: 1.8466 - acc: 0.3684 - val_loss: 1.7979 - val_acc: 0.3982\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 6s 59us/sample - loss: 1.8428 - acc: 0.3701 - val_loss: 1.7875 - val_acc: 0.4026\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 6s 59us/sample - loss: 1.8366 - acc: 0.3708 - val_loss: 1.7871 - val_acc: 0.4048\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 6s 58us/sample - loss: 1.8309 - acc: 0.3738 - val_loss: 1.7654 - val_acc: 0.4114\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 6s 55us/sample - loss: 1.8305 - acc: 0.3742 - val_loss: 1.7833 - val_acc: 0.4033\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 6s 57us/sample - loss: 1.8252 - acc: 0.3758 - val_loss: 1.7807 - val_acc: 0.4047\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 6s 58us/sample - loss: 1.8213 - acc: 0.3758 - val_loss: 1.7866 - val_acc: 0.3987\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 6s 58us/sample - loss: 1.8213 - acc: 0.3768 - val_loss: 1.7859 - val_acc: 0.4006\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 6s 56us/sample - loss: 1.8161 - acc: 0.3773 - val_loss: 1.7887 - val_acc: 0.3967\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 6s 57us/sample - loss: 1.8138 - acc: 0.3792 - val_loss: 1.7747 - val_acc: 0.4065\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 6s 56us/sample - loss: 1.8133 - acc: 0.3793 - val_loss: 1.7870 - val_acc: 0.3964\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 6s 57us/sample - loss: 1.8103 - acc: 0.3816 - val_loss: 1.7722 - val_acc: 0.4059\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 6s 56us/sample - loss: 1.8075 - acc: 0.3797 - val_loss: 1.7644 - val_acc: 0.4089\n",
            "32890/32890 [==============================] - 1s 19us/sample - loss: 1.8408 - acc: 0.3641\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 8s 73us/sample - loss: 1.9937 - acc: 0.3080 - val_loss: 1.8683 - val_acc: 0.3746\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 6s 60us/sample - loss: 1.8959 - acc: 0.3499 - val_loss: 1.8447 - val_acc: 0.3817\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 6s 61us/sample - loss: 1.8784 - acc: 0.3580 - val_loss: 1.8335 - val_acc: 0.3836\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 7s 62us/sample - loss: 1.8655 - acc: 0.3641 - val_loss: 1.8231 - val_acc: 0.3904\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 6s 60us/sample - loss: 1.8541 - acc: 0.3667 - val_loss: 1.8188 - val_acc: 0.3894\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 6s 61us/sample - loss: 1.8461 - acc: 0.3711 - val_loss: 1.8225 - val_acc: 0.3830\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 7s 62us/sample - loss: 1.8371 - acc: 0.3731 - val_loss: 1.8075 - val_acc: 0.3952\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 6s 61us/sample - loss: 1.8303 - acc: 0.3759 - val_loss: 1.8158 - val_acc: 0.3845\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 6s 61us/sample - loss: 1.8268 - acc: 0.3783 - val_loss: 1.8038 - val_acc: 0.3941\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 6s 59us/sample - loss: 1.8204 - acc: 0.3806 - val_loss: 1.7930 - val_acc: 0.3970\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 6s 59us/sample - loss: 1.8142 - acc: 0.3811 - val_loss: 1.8023 - val_acc: 0.3901\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 6s 60us/sample - loss: 1.8122 - acc: 0.3818 - val_loss: 1.7960 - val_acc: 0.3973\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 6s 62us/sample - loss: 1.8089 - acc: 0.3829 - val_loss: 1.7972 - val_acc: 0.3951\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 6s 59us/sample - loss: 1.8054 - acc: 0.3853 - val_loss: 1.7895 - val_acc: 0.3971\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 7s 63us/sample - loss: 1.8020 - acc: 0.3873 - val_loss: 1.7970 - val_acc: 0.3936\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 6s 61us/sample - loss: 1.7982 - acc: 0.3868 - val_loss: 1.7758 - val_acc: 0.4020\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 7s 62us/sample - loss: 1.7967 - acc: 0.3875 - val_loss: 1.7777 - val_acc: 0.3985\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 6s 61us/sample - loss: 1.7940 - acc: 0.3882 - val_loss: 1.7928 - val_acc: 0.3924\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 6s 60us/sample - loss: 1.7932 - acc: 0.3895 - val_loss: 1.8006 - val_acc: 0.3926\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 6s 60us/sample - loss: 1.7912 - acc: 0.3895 - val_loss: 1.7823 - val_acc: 0.4010\n",
            "32890/32890 [==============================] - 1s 20us/sample - loss: 1.8662 - acc: 0.3480\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 9s 83us/sample - loss: 2.0141 - acc: 0.2962 - val_loss: 1.8930 - val_acc: 0.3631\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 7s 67us/sample - loss: 1.9210 - acc: 0.3350 - val_loss: 1.8583 - val_acc: 0.3797\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 7s 68us/sample - loss: 1.9029 - acc: 0.3443 - val_loss: 1.8400 - val_acc: 0.3855\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 7s 62us/sample - loss: 1.8866 - acc: 0.3518 - val_loss: 1.8511 - val_acc: 0.3764\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 7s 62us/sample - loss: 1.8761 - acc: 0.3541 - val_loss: 1.8542 - val_acc: 0.3785\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 7s 62us/sample - loss: 1.8691 - acc: 0.3584 - val_loss: 1.8262 - val_acc: 0.3851\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 6s 61us/sample - loss: 1.8633 - acc: 0.3599 - val_loss: 1.8275 - val_acc: 0.3841\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 7s 63us/sample - loss: 1.8549 - acc: 0.3626 - val_loss: 1.8279 - val_acc: 0.3842\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 7s 62us/sample - loss: 1.8501 - acc: 0.3655 - val_loss: 1.8234 - val_acc: 0.3884\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 7s 62us/sample - loss: 1.8453 - acc: 0.3671 - val_loss: 1.8054 - val_acc: 0.3920\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 7s 63us/sample - loss: 1.8420 - acc: 0.3681 - val_loss: 1.8020 - val_acc: 0.3944\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 6s 61us/sample - loss: 1.8396 - acc: 0.3691 - val_loss: 1.8102 - val_acc: 0.3897\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 6s 59us/sample - loss: 1.8359 - acc: 0.3689 - val_loss: 1.8090 - val_acc: 0.3884\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 6s 61us/sample - loss: 1.8320 - acc: 0.3709 - val_loss: 1.8030 - val_acc: 0.3907\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 6s 61us/sample - loss: 1.8283 - acc: 0.3705 - val_loss: 1.8080 - val_acc: 0.3852\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 6s 62us/sample - loss: 1.8262 - acc: 0.3724 - val_loss: 1.8012 - val_acc: 0.3927\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 7s 63us/sample - loss: 1.8243 - acc: 0.3744 - val_loss: 1.7953 - val_acc: 0.3910\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 7s 64us/sample - loss: 1.8226 - acc: 0.3745 - val_loss: 1.7901 - val_acc: 0.3947\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 7s 64us/sample - loss: 1.8194 - acc: 0.3763 - val_loss: 1.7984 - val_acc: 0.3907\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 7s 64us/sample - loss: 1.8187 - acc: 0.3760 - val_loss: 1.7997 - val_acc: 0.3942\n",
            "32890/32890 [==============================] - 1s 22us/sample - loss: 1.7857 - acc: 0.3898\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 8s 80us/sample - loss: 1.9860 - acc: 0.3124 - val_loss: 1.9202 - val_acc: 0.3579\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 7s 64us/sample - loss: 1.9097 - acc: 0.3431 - val_loss: 1.8738 - val_acc: 0.3775\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 7s 64us/sample - loss: 1.8919 - acc: 0.3519 - val_loss: 1.8436 - val_acc: 0.3836\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 7s 64us/sample - loss: 1.8786 - acc: 0.3557 - val_loss: 1.8774 - val_acc: 0.3770\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 7s 64us/sample - loss: 1.8692 - acc: 0.3594 - val_loss: 1.8335 - val_acc: 0.3866\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 7s 64us/sample - loss: 1.8603 - acc: 0.3614 - val_loss: 1.8428 - val_acc: 0.3770\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 7s 64us/sample - loss: 1.8521 - acc: 0.3655 - val_loss: 1.8360 - val_acc: 0.3842\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 7s 64us/sample - loss: 1.8452 - acc: 0.3670 - val_loss: 1.8354 - val_acc: 0.3808\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 7s 63us/sample - loss: 1.8414 - acc: 0.3698 - val_loss: 1.8052 - val_acc: 0.3949\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 7s 62us/sample - loss: 1.8350 - acc: 0.3715 - val_loss: 1.8296 - val_acc: 0.3872\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 7s 62us/sample - loss: 1.8311 - acc: 0.3727 - val_loss: 1.8052 - val_acc: 0.3944\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 7s 62us/sample - loss: 1.8284 - acc: 0.3745 - val_loss: 1.8271 - val_acc: 0.3808\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 7s 62us/sample - loss: 1.8266 - acc: 0.3749 - val_loss: 1.8269 - val_acc: 0.3898\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 6s 60us/sample - loss: 1.8219 - acc: 0.3760 - val_loss: 1.8141 - val_acc: 0.3896\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 6s 60us/sample - loss: 1.8181 - acc: 0.3759 - val_loss: 1.8157 - val_acc: 0.3906\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 6s 58us/sample - loss: 1.8174 - acc: 0.3776 - val_loss: 1.8014 - val_acc: 0.3947\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 6s 61us/sample - loss: 1.8123 - acc: 0.3800 - val_loss: 1.8300 - val_acc: 0.3826\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 6s 59us/sample - loss: 1.8128 - acc: 0.3789 - val_loss: 1.8123 - val_acc: 0.3933\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 6s 60us/sample - loss: 1.8097 - acc: 0.3813 - val_loss: 1.8047 - val_acc: 0.3911\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 6s 59us/sample - loss: 1.8083 - acc: 0.3815 - val_loss: 1.8095 - val_acc: 0.3912\n",
            "32890/32890 [==============================] - 1s 20us/sample - loss: 1.8188 - acc: 0.3705\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 8s 75us/sample - loss: 2.0044 - acc: 0.3053 - val_loss: 1.8838 - val_acc: 0.3383\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 6s 61us/sample - loss: 1.9126 - acc: 0.3419 - val_loss: 1.8906 - val_acc: 0.3242\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 6s 61us/sample - loss: 1.8950 - acc: 0.3489 - val_loss: 1.8841 - val_acc: 0.3245\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 6s 60us/sample - loss: 1.8827 - acc: 0.3530 - val_loss: 1.8557 - val_acc: 0.3463\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 6s 60us/sample - loss: 1.8700 - acc: 0.3566 - val_loss: 1.8413 - val_acc: 0.3477\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 7s 67us/sample - loss: 1.8629 - acc: 0.3604 - val_loss: 1.8193 - val_acc: 0.3599\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 6s 58us/sample - loss: 1.8558 - acc: 0.3622 - val_loss: 1.8249 - val_acc: 0.3604\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 6s 60us/sample - loss: 1.8500 - acc: 0.3656 - val_loss: 1.8123 - val_acc: 0.3652\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 7s 66us/sample - loss: 1.8442 - acc: 0.3669 - val_loss: 1.8037 - val_acc: 0.3720\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 6s 60us/sample - loss: 1.8392 - acc: 0.3675 - val_loss: 1.8207 - val_acc: 0.3688\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 6s 58us/sample - loss: 1.8355 - acc: 0.3703 - val_loss: 1.8127 - val_acc: 0.3699\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 6s 59us/sample - loss: 1.8309 - acc: 0.3729 - val_loss: 1.7892 - val_acc: 0.3761\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 6s 58us/sample - loss: 1.8280 - acc: 0.3731 - val_loss: 1.8023 - val_acc: 0.3739\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 6s 61us/sample - loss: 1.8244 - acc: 0.3748 - val_loss: 1.7919 - val_acc: 0.3710\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 7s 66us/sample - loss: 1.8212 - acc: 0.3742 - val_loss: 1.8095 - val_acc: 0.3680\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 6s 57us/sample - loss: 1.8194 - acc: 0.3754 - val_loss: 1.7847 - val_acc: 0.3758\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 6s 58us/sample - loss: 1.8173 - acc: 0.3764 - val_loss: 1.7971 - val_acc: 0.3742\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 6s 58us/sample - loss: 1.8149 - acc: 0.3786 - val_loss: 1.7881 - val_acc: 0.3760\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 7s 62us/sample - loss: 1.8126 - acc: 0.3780 - val_loss: 1.7865 - val_acc: 0.3801\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 6s 59us/sample - loss: 1.8107 - acc: 0.3779 - val_loss: 1.8110 - val_acc: 0.3628\n",
            "32889/32889 [==============================] - 1s 21us/sample - loss: 1.8195 - acc: 0.3904\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 6s 58us/sample - loss: 2.0322 - acc: 0.2962 - val_loss: 1.8661 - val_acc: 0.3766\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 4s 42us/sample - loss: 1.9190 - acc: 0.3398 - val_loss: 1.8513 - val_acc: 0.3869\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 5s 43us/sample - loss: 1.9030 - acc: 0.3445 - val_loss: 1.8304 - val_acc: 0.3876\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 4s 42us/sample - loss: 1.8897 - acc: 0.3499 - val_loss: 1.8184 - val_acc: 0.3917\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.8796 - acc: 0.3570 - val_loss: 1.8161 - val_acc: 0.3938\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 4s 42us/sample - loss: 1.8710 - acc: 0.3577 - val_loss: 1.8035 - val_acc: 0.3946\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 1.8629 - acc: 0.3594 - val_loss: 1.8044 - val_acc: 0.3947\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 4s 41us/sample - loss: 1.8574 - acc: 0.3630 - val_loss: 1.7934 - val_acc: 0.3994\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 4s 41us/sample - loss: 1.8511 - acc: 0.3664 - val_loss: 1.7888 - val_acc: 0.4021\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 4s 42us/sample - loss: 1.8477 - acc: 0.3686 - val_loss: 1.7821 - val_acc: 0.4072\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 4s 42us/sample - loss: 1.8432 - acc: 0.3689 - val_loss: 1.7907 - val_acc: 0.4038\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 4s 41us/sample - loss: 1.8377 - acc: 0.3699 - val_loss: 1.7799 - val_acc: 0.4072\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 4s 41us/sample - loss: 1.8355 - acc: 0.3715 - val_loss: 1.7843 - val_acc: 0.4022\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 4s 42us/sample - loss: 1.8315 - acc: 0.3745 - val_loss: 1.7809 - val_acc: 0.4062\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 4s 42us/sample - loss: 1.8276 - acc: 0.3752 - val_loss: 1.7786 - val_acc: 0.4069\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 4s 42us/sample - loss: 1.8263 - acc: 0.3745 - val_loss: 1.7850 - val_acc: 0.4019\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 1.8228 - acc: 0.3769 - val_loss: 1.7799 - val_acc: 0.4062\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 4s 41us/sample - loss: 1.8203 - acc: 0.3778 - val_loss: 1.7792 - val_acc: 0.4032\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 4s 41us/sample - loss: 1.8204 - acc: 0.3765 - val_loss: 1.7738 - val_acc: 0.4082\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 4s 42us/sample - loss: 1.8158 - acc: 0.3791 - val_loss: 1.7826 - val_acc: 0.4046\n",
            "32890/32890 [==============================] - 0s 14us/sample - loss: 1.8376 - acc: 0.3634\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 6s 59us/sample - loss: 2.0314 - acc: 0.2969 - val_loss: 1.8722 - val_acc: 0.3751\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 5s 43us/sample - loss: 1.9083 - acc: 0.3436 - val_loss: 1.8471 - val_acc: 0.3802\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 4s 41us/sample - loss: 1.8885 - acc: 0.3528 - val_loss: 1.8298 - val_acc: 0.3890\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 4s 41us/sample - loss: 1.8749 - acc: 0.3579 - val_loss: 1.8302 - val_acc: 0.3882\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.8643 - acc: 0.3640 - val_loss: 1.8250 - val_acc: 0.3865\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 5s 43us/sample - loss: 1.8539 - acc: 0.3673 - val_loss: 1.8094 - val_acc: 0.3930\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 4s 43us/sample - loss: 1.8471 - acc: 0.3696 - val_loss: 1.8075 - val_acc: 0.3925\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.8400 - acc: 0.3726 - val_loss: 1.7997 - val_acc: 0.3964\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.8337 - acc: 0.3761 - val_loss: 1.7865 - val_acc: 0.3975\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 4s 42us/sample - loss: 1.8301 - acc: 0.3770 - val_loss: 1.7927 - val_acc: 0.3950\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.8268 - acc: 0.3778 - val_loss: 1.7840 - val_acc: 0.4011\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 4s 42us/sample - loss: 1.8220 - acc: 0.3800 - val_loss: 1.7868 - val_acc: 0.4000\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 4s 42us/sample - loss: 1.8185 - acc: 0.3819 - val_loss: 1.8070 - val_acc: 0.3869\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 5s 43us/sample - loss: 1.8153 - acc: 0.3812 - val_loss: 1.7882 - val_acc: 0.3949\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.8112 - acc: 0.3847 - val_loss: 1.7904 - val_acc: 0.3941\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 4s 42us/sample - loss: 1.8094 - acc: 0.3847 - val_loss: 1.7878 - val_acc: 0.3961\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 4s 42us/sample - loss: 1.8066 - acc: 0.3850 - val_loss: 1.7973 - val_acc: 0.3940\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 4s 42us/sample - loss: 1.8043 - acc: 0.3851 - val_loss: 1.8000 - val_acc: 0.3939\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 4s 42us/sample - loss: 1.8030 - acc: 0.3863 - val_loss: 1.7964 - val_acc: 0.3902\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 4s 43us/sample - loss: 1.8016 - acc: 0.3851 - val_loss: 1.7856 - val_acc: 0.3978\n",
            "32890/32890 [==============================] - 0s 15us/sample - loss: 1.8707 - acc: 0.3441\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 6s 61us/sample - loss: 2.0320 - acc: 0.2939 - val_loss: 1.9020 - val_acc: 0.3614\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 5s 43us/sample - loss: 1.9322 - acc: 0.3303 - val_loss: 1.8918 - val_acc: 0.3594\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 5s 43us/sample - loss: 1.9167 - acc: 0.3371 - val_loss: 1.8759 - val_acc: 0.3712\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 4s 42us/sample - loss: 1.9024 - acc: 0.3436 - val_loss: 1.8533 - val_acc: 0.3782\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 5s 47us/sample - loss: 1.8920 - acc: 0.3493 - val_loss: 1.8492 - val_acc: 0.3804\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 5s 45us/sample - loss: 1.8832 - acc: 0.3523 - val_loss: 1.8421 - val_acc: 0.3769\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.8757 - acc: 0.3579 - val_loss: 1.8272 - val_acc: 0.3844\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 5s 45us/sample - loss: 1.8686 - acc: 0.3562 - val_loss: 1.8248 - val_acc: 0.3804\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 5s 48us/sample - loss: 1.8633 - acc: 0.3615 - val_loss: 1.8200 - val_acc: 0.3844\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 5s 47us/sample - loss: 1.8592 - acc: 0.3627 - val_loss: 1.8250 - val_acc: 0.3838\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.8533 - acc: 0.3635 - val_loss: 1.8026 - val_acc: 0.3952\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 5s 43us/sample - loss: 1.8503 - acc: 0.3649 - val_loss: 1.8116 - val_acc: 0.3870\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 5s 45us/sample - loss: 1.8483 - acc: 0.3662 - val_loss: 1.8084 - val_acc: 0.3915\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 5s 43us/sample - loss: 1.8428 - acc: 0.3687 - val_loss: 1.8122 - val_acc: 0.3861\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 5s 43us/sample - loss: 1.8385 - acc: 0.3687 - val_loss: 1.7974 - val_acc: 0.3970\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 5s 45us/sample - loss: 1.8369 - acc: 0.3695 - val_loss: 1.8072 - val_acc: 0.3857\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 5s 45us/sample - loss: 1.8359 - acc: 0.3696 - val_loss: 1.8176 - val_acc: 0.3842\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.8332 - acc: 0.3725 - val_loss: 1.8011 - val_acc: 0.3916\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.8299 - acc: 0.3726 - val_loss: 1.7987 - val_acc: 0.3915\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 5s 45us/sample - loss: 1.8284 - acc: 0.3723 - val_loss: 1.8046 - val_acc: 0.3891\n",
            "32890/32890 [==============================] - 0s 14us/sample - loss: 1.7893 - acc: 0.3899\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 6s 61us/sample - loss: 2.0330 - acc: 0.2988 - val_loss: 1.8959 - val_acc: 0.3667\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.9125 - acc: 0.3443 - val_loss: 1.8723 - val_acc: 0.3702\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.8934 - acc: 0.3506 - val_loss: 1.8505 - val_acc: 0.3804\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 5s 43us/sample - loss: 1.8823 - acc: 0.3545 - val_loss: 1.8500 - val_acc: 0.3779\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 5s 43us/sample - loss: 1.8747 - acc: 0.3567 - val_loss: 1.8568 - val_acc: 0.3762\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 5s 43us/sample - loss: 1.8682 - acc: 0.3606 - val_loss: 1.8454 - val_acc: 0.3804\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 5s 43us/sample - loss: 1.8625 - acc: 0.3631 - val_loss: 1.8305 - val_acc: 0.3868\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 5s 43us/sample - loss: 1.8568 - acc: 0.3638 - val_loss: 1.8225 - val_acc: 0.3912\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 4s 42us/sample - loss: 1.8515 - acc: 0.3654 - val_loss: 1.8465 - val_acc: 0.3777\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 4s 42us/sample - loss: 1.8482 - acc: 0.3657 - val_loss: 1.8172 - val_acc: 0.3899\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 4s 42us/sample - loss: 1.8432 - acc: 0.3680 - val_loss: 1.8382 - val_acc: 0.3882\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 4s 42us/sample - loss: 1.8403 - acc: 0.3701 - val_loss: 1.8382 - val_acc: 0.3804\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 4s 42us/sample - loss: 1.8354 - acc: 0.3713 - val_loss: 1.8184 - val_acc: 0.3932\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.8326 - acc: 0.3727 - val_loss: 1.8276 - val_acc: 0.3878\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 5s 43us/sample - loss: 1.8295 - acc: 0.3711 - val_loss: 1.8184 - val_acc: 0.3909\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 5s 43us/sample - loss: 1.8251 - acc: 0.3760 - val_loss: 1.8268 - val_acc: 0.3875\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 5s 44us/sample - loss: 1.8225 - acc: 0.3757 - val_loss: 1.8371 - val_acc: 0.3818\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 4s 43us/sample - loss: 1.8183 - acc: 0.3769 - val_loss: 1.8236 - val_acc: 0.3908\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 4s 40us/sample - loss: 1.8193 - acc: 0.3787 - val_loss: 1.8090 - val_acc: 0.3937\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 4s 42us/sample - loss: 1.8165 - acc: 0.3791 - val_loss: 1.8153 - val_acc: 0.3917\n",
            "32890/32890 [==============================] - 0s 13us/sample - loss: 1.8193 - acc: 0.3749\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 6s 59us/sample - loss: 2.0550 - acc: 0.2849 - val_loss: 1.9236 - val_acc: 0.3167\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 4s 43us/sample - loss: 1.9309 - acc: 0.3340 - val_loss: 1.8674 - val_acc: 0.3553\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 4s 42us/sample - loss: 1.9080 - acc: 0.3434 - val_loss: 1.8500 - val_acc: 0.3546\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 4s 42us/sample - loss: 1.8947 - acc: 0.3501 - val_loss: 1.8645 - val_acc: 0.3420\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 5s 44us/sample - loss: 1.8841 - acc: 0.3527 - val_loss: 1.8279 - val_acc: 0.3588\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 4s 41us/sample - loss: 1.8756 - acc: 0.3569 - val_loss: 1.8357 - val_acc: 0.3565\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 4s 42us/sample - loss: 1.8675 - acc: 0.3595 - val_loss: 1.8260 - val_acc: 0.3633\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 4s 41us/sample - loss: 1.8611 - acc: 0.3610 - val_loss: 1.8371 - val_acc: 0.3546\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 4s 42us/sample - loss: 1.8562 - acc: 0.3627 - val_loss: 1.8036 - val_acc: 0.3751\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 4s 41us/sample - loss: 1.8533 - acc: 0.3644 - val_loss: 1.8178 - val_acc: 0.3655\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 4s 41us/sample - loss: 1.8470 - acc: 0.3662 - val_loss: 1.8137 - val_acc: 0.3668\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 4s 42us/sample - loss: 1.8430 - acc: 0.3681 - val_loss: 1.8164 - val_acc: 0.3687\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 4s 42us/sample - loss: 1.8395 - acc: 0.3696 - val_loss: 1.8141 - val_acc: 0.3742\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 4s 42us/sample - loss: 1.8376 - acc: 0.3704 - val_loss: 1.8127 - val_acc: 0.3691\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 4s 41us/sample - loss: 1.8336 - acc: 0.3708 - val_loss: 1.7940 - val_acc: 0.3740\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 4s 41us/sample - loss: 1.8329 - acc: 0.3708 - val_loss: 1.8107 - val_acc: 0.3755\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 4s 41us/sample - loss: 1.8290 - acc: 0.3725 - val_loss: 1.7928 - val_acc: 0.3788\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 5s 43us/sample - loss: 1.8272 - acc: 0.3731 - val_loss: 1.8014 - val_acc: 0.3749\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 5s 44us/sample - loss: 1.8241 - acc: 0.3746 - val_loss: 1.8000 - val_acc: 0.3761\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 4s 42us/sample - loss: 1.8229 - acc: 0.3741 - val_loss: 1.8049 - val_acc: 0.3735\n",
            "32889/32889 [==============================] - 0s 14us/sample - loss: 1.8370 - acc: 0.3897\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 5s 45us/sample - loss: 2.1257 - acc: 0.2580 - val_loss: 1.8946 - val_acc: 0.3583\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9369 - acc: 0.3323 - val_loss: 1.8439 - val_acc: 0.3853\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9136 - acc: 0.3417 - val_loss: 1.8435 - val_acc: 0.3848\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.9009 - acc: 0.3464 - val_loss: 1.8350 - val_acc: 0.3871\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8913 - acc: 0.3507 - val_loss: 1.8279 - val_acc: 0.3907\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8838 - acc: 0.3541 - val_loss: 1.8197 - val_acc: 0.3897\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8770 - acc: 0.3553 - val_loss: 1.8175 - val_acc: 0.3891\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8715 - acc: 0.3580 - val_loss: 1.7995 - val_acc: 0.3987\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8672 - acc: 0.3596 - val_loss: 1.7962 - val_acc: 0.3994\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8608 - acc: 0.3632 - val_loss: 1.8015 - val_acc: 0.3948\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8558 - acc: 0.3640 - val_loss: 1.8043 - val_acc: 0.3948\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8515 - acc: 0.3657 - val_loss: 1.7899 - val_acc: 0.4007\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8480 - acc: 0.3673 - val_loss: 1.7973 - val_acc: 0.3975\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8450 - acc: 0.3680 - val_loss: 1.7935 - val_acc: 0.3998\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8418 - acc: 0.3687 - val_loss: 1.7858 - val_acc: 0.4047\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8391 - acc: 0.3690 - val_loss: 1.7925 - val_acc: 0.3985\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8358 - acc: 0.3709 - val_loss: 1.7894 - val_acc: 0.4003\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8351 - acc: 0.3717 - val_loss: 1.7871 - val_acc: 0.4046\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8337 - acc: 0.3725 - val_loss: 1.7723 - val_acc: 0.4094\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8290 - acc: 0.3746 - val_loss: 1.7868 - val_acc: 0.4026\n",
            "32890/32890 [==============================] - 0s 8us/sample - loss: 1.8358 - acc: 0.3666\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 5s 48us/sample - loss: 2.1020 - acc: 0.2640 - val_loss: 1.9019 - val_acc: 0.3621\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.9250 - acc: 0.3382 - val_loss: 1.8842 - val_acc: 0.3658\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.9006 - acc: 0.3478 - val_loss: 1.8533 - val_acc: 0.3768\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8866 - acc: 0.3543 - val_loss: 1.8404 - val_acc: 0.3834\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8772 - acc: 0.3569 - val_loss: 1.8287 - val_acc: 0.3879\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8688 - acc: 0.3629 - val_loss: 1.8376 - val_acc: 0.3827\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8615 - acc: 0.3638 - val_loss: 1.8296 - val_acc: 0.3864\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8545 - acc: 0.3662 - val_loss: 1.8179 - val_acc: 0.3886\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8477 - acc: 0.3687 - val_loss: 1.8097 - val_acc: 0.3923\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8434 - acc: 0.3709 - val_loss: 1.8091 - val_acc: 0.3922\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8384 - acc: 0.3732 - val_loss: 1.8075 - val_acc: 0.3941\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8355 - acc: 0.3735 - val_loss: 1.8039 - val_acc: 0.3932\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 25us/sample - loss: 1.8290 - acc: 0.3750 - val_loss: 1.8134 - val_acc: 0.3909\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8260 - acc: 0.3786 - val_loss: 1.8071 - val_acc: 0.3931\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8254 - acc: 0.3778 - val_loss: 1.7873 - val_acc: 0.3986\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8220 - acc: 0.3790 - val_loss: 1.8011 - val_acc: 0.3934\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8168 - acc: 0.3813 - val_loss: 1.7912 - val_acc: 0.3942\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8158 - acc: 0.3818 - val_loss: 1.7967 - val_acc: 0.3941\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8146 - acc: 0.3816 - val_loss: 1.7880 - val_acc: 0.3977\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8147 - acc: 0.3811 - val_loss: 1.7909 - val_acc: 0.3958\n",
            "32890/32890 [==============================] - 0s 9us/sample - loss: 1.8858 - acc: 0.3412\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 5s 47us/sample - loss: 2.1111 - acc: 0.2667 - val_loss: 1.9104 - val_acc: 0.3603\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.9467 - acc: 0.3227 - val_loss: 1.8773 - val_acc: 0.3698\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9250 - acc: 0.3316 - val_loss: 1.8819 - val_acc: 0.3670\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9136 - acc: 0.3385 - val_loss: 1.8631 - val_acc: 0.3766\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9022 - acc: 0.3449 - val_loss: 1.8558 - val_acc: 0.3764\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8953 - acc: 0.3470 - val_loss: 1.8667 - val_acc: 0.3723\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8861 - acc: 0.3500 - val_loss: 1.8502 - val_acc: 0.3786\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8825 - acc: 0.3529 - val_loss: 1.8408 - val_acc: 0.3850\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8762 - acc: 0.3543 - val_loss: 1.8237 - val_acc: 0.3903\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8693 - acc: 0.3569 - val_loss: 1.8203 - val_acc: 0.3880\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8685 - acc: 0.3556 - val_loss: 1.8346 - val_acc: 0.3839\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8624 - acc: 0.3591 - val_loss: 1.8314 - val_acc: 0.3829\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8579 - acc: 0.3611 - val_loss: 1.8258 - val_acc: 0.3870\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8545 - acc: 0.3624 - val_loss: 1.8138 - val_acc: 0.3882\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8510 - acc: 0.3649 - val_loss: 1.8255 - val_acc: 0.3862\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8483 - acc: 0.3659 - val_loss: 1.8267 - val_acc: 0.3845\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8461 - acc: 0.3657 - val_loss: 1.8063 - val_acc: 0.3930\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.8433 - acc: 0.3651 - val_loss: 1.8200 - val_acc: 0.3866\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8416 - acc: 0.3677 - val_loss: 1.8091 - val_acc: 0.3898\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8381 - acc: 0.3702 - val_loss: 1.8056 - val_acc: 0.3921\n",
            "32890/32890 [==============================] - 0s 8us/sample - loss: 1.7911 - acc: 0.3912\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 5s 48us/sample - loss: 2.1465 - acc: 0.2507 - val_loss: 1.9267 - val_acc: 0.3501\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 3s 28us/sample - loss: 1.9429 - acc: 0.3314 - val_loss: 1.8870 - val_acc: 0.3685\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.9168 - acc: 0.3398 - val_loss: 1.8768 - val_acc: 0.3735\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.9038 - acc: 0.3474 - val_loss: 1.8639 - val_acc: 0.3737\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8935 - acc: 0.3507 - val_loss: 1.8626 - val_acc: 0.3749\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8850 - acc: 0.3534 - val_loss: 1.8487 - val_acc: 0.3834\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8787 - acc: 0.3556 - val_loss: 1.8580 - val_acc: 0.3799\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8734 - acc: 0.3582 - val_loss: 1.8499 - val_acc: 0.3789\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8677 - acc: 0.3598 - val_loss: 1.8449 - val_acc: 0.3820\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8633 - acc: 0.3610 - val_loss: 1.8431 - val_acc: 0.3828\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8614 - acc: 0.3625 - val_loss: 1.8490 - val_acc: 0.3816\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8554 - acc: 0.3647 - val_loss: 1.8452 - val_acc: 0.3826\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8519 - acc: 0.3650 - val_loss: 1.8392 - val_acc: 0.3843\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8480 - acc: 0.3675 - val_loss: 1.8340 - val_acc: 0.3850\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8450 - acc: 0.3680 - val_loss: 1.8273 - val_acc: 0.3870\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8431 - acc: 0.3684 - val_loss: 1.8368 - val_acc: 0.3793\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8395 - acc: 0.3691 - val_loss: 1.8389 - val_acc: 0.3839\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8365 - acc: 0.3716 - val_loss: 1.8234 - val_acc: 0.3907\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 3s 26us/sample - loss: 1.8346 - acc: 0.3715 - val_loss: 1.8212 - val_acc: 0.3908\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 3s 27us/sample - loss: 1.8328 - acc: 0.3728 - val_loss: 1.8196 - val_acc: 0.3913\n",
            "32890/32890 [==============================] - 0s 9us/sample - loss: 1.8373 - acc: 0.3699\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 5s 48us/sample - loss: 2.1384 - acc: 0.2521 - val_loss: 1.9184 - val_acc: 0.3367\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.9426 - acc: 0.3296 - val_loss: 1.8843 - val_acc: 0.3511\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.9183 - acc: 0.3401 - val_loss: 1.8846 - val_acc: 0.3451\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.9049 - acc: 0.3453 - val_loss: 1.8673 - val_acc: 0.3489\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8940 - acc: 0.3493 - val_loss: 1.8738 - val_acc: 0.3417\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.8875 - acc: 0.3503 - val_loss: 1.8548 - val_acc: 0.3494\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8814 - acc: 0.3535 - val_loss: 1.8465 - val_acc: 0.3553\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8750 - acc: 0.3545 - val_loss: 1.8348 - val_acc: 0.3611\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.8707 - acc: 0.3564 - val_loss: 1.8178 - val_acc: 0.3671\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8653 - acc: 0.3583 - val_loss: 1.8261 - val_acc: 0.3604\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8618 - acc: 0.3614 - val_loss: 1.8274 - val_acc: 0.3616\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8574 - acc: 0.3622 - val_loss: 1.8248 - val_acc: 0.3623\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8543 - acc: 0.3628 - val_loss: 1.8206 - val_acc: 0.3650\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8509 - acc: 0.3644 - val_loss: 1.8192 - val_acc: 0.3661\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8456 - acc: 0.3666 - val_loss: 1.8217 - val_acc: 0.3677\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 3s 26us/sample - loss: 1.8427 - acc: 0.3670 - val_loss: 1.8095 - val_acc: 0.3670\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.8417 - acc: 0.3674 - val_loss: 1.8034 - val_acc: 0.3708\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.8387 - acc: 0.3691 - val_loss: 1.7991 - val_acc: 0.3744\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.8368 - acc: 0.3701 - val_loss: 1.8058 - val_acc: 0.3681\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 3s 27us/sample - loss: 1.8334 - acc: 0.3712 - val_loss: 1.8024 - val_acc: 0.3699\n",
            "32889/32889 [==============================] - 0s 10us/sample - loss: 1.8343 - acc: 0.3895\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 4s 41us/sample - loss: 2.2435 - acc: 0.2062 - val_loss: 1.9430 - val_acc: 0.3604\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.9732 - acc: 0.3204 - val_loss: 1.8806 - val_acc: 0.3698\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.9381 - acc: 0.3338 - val_loss: 1.8455 - val_acc: 0.3825\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.9193 - acc: 0.3401 - val_loss: 1.8351 - val_acc: 0.3890\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.9096 - acc: 0.3434 - val_loss: 1.8344 - val_acc: 0.3873\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.9009 - acc: 0.3466 - val_loss: 1.8212 - val_acc: 0.3899\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8942 - acc: 0.3482 - val_loss: 1.8282 - val_acc: 0.3874\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8890 - acc: 0.3521 - val_loss: 1.8184 - val_acc: 0.3913\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8833 - acc: 0.3527 - val_loss: 1.8047 - val_acc: 0.3954\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8777 - acc: 0.3559 - val_loss: 1.8069 - val_acc: 0.3962\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8719 - acc: 0.3590 - val_loss: 1.8030 - val_acc: 0.3974\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8674 - acc: 0.3601 - val_loss: 1.7978 - val_acc: 0.3989\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8633 - acc: 0.3625 - val_loss: 1.8021 - val_acc: 0.3946\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8587 - acc: 0.3636 - val_loss: 1.8067 - val_acc: 0.3913\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8573 - acc: 0.3636 - val_loss: 1.7948 - val_acc: 0.3964\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8531 - acc: 0.3646 - val_loss: 1.7900 - val_acc: 0.4004\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8498 - acc: 0.3673 - val_loss: 1.7877 - val_acc: 0.4031\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8470 - acc: 0.3659 - val_loss: 1.7859 - val_acc: 0.4020\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8438 - acc: 0.3685 - val_loss: 1.7881 - val_acc: 0.4019\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8406 - acc: 0.3696 - val_loss: 1.7812 - val_acc: 0.4029\n",
            "32890/32890 [==============================] - 0s 6us/sample - loss: 1.8515 - acc: 0.3602\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 4s 41us/sample - loss: 2.1883 - acc: 0.2467 - val_loss: 1.9154 - val_acc: 0.3493\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.9424 - acc: 0.3296 - val_loss: 1.8794 - val_acc: 0.3699\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.9173 - acc: 0.3398 - val_loss: 1.8637 - val_acc: 0.3752\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.9029 - acc: 0.3465 - val_loss: 1.8581 - val_acc: 0.3739\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8964 - acc: 0.3491 - val_loss: 1.8562 - val_acc: 0.3742\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8880 - acc: 0.3519 - val_loss: 1.8431 - val_acc: 0.3812\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8828 - acc: 0.3549 - val_loss: 1.8418 - val_acc: 0.3809\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8767 - acc: 0.3583 - val_loss: 1.8295 - val_acc: 0.3860\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8719 - acc: 0.3598 - val_loss: 1.8328 - val_acc: 0.3835\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8666 - acc: 0.3611 - val_loss: 1.8233 - val_acc: 0.3850\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8625 - acc: 0.3647 - val_loss: 1.8226 - val_acc: 0.3847\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8570 - acc: 0.3654 - val_loss: 1.8245 - val_acc: 0.3875\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8542 - acc: 0.3671 - val_loss: 1.8131 - val_acc: 0.3897\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8512 - acc: 0.3685 - val_loss: 1.8128 - val_acc: 0.3935\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8484 - acc: 0.3688 - val_loss: 1.8166 - val_acc: 0.3896\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8432 - acc: 0.3707 - val_loss: 1.8088 - val_acc: 0.3909\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8409 - acc: 0.3718 - val_loss: 1.8105 - val_acc: 0.3924\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8355 - acc: 0.3735 - val_loss: 1.8005 - val_acc: 0.3941\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8367 - acc: 0.3739 - val_loss: 1.8065 - val_acc: 0.3936\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8320 - acc: 0.3758 - val_loss: 1.8062 - val_acc: 0.3952\n",
            "32890/32890 [==============================] - 0s 6us/sample - loss: 1.8935 - acc: 0.3392\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 4s 42us/sample - loss: 2.2339 - acc: 0.2327 - val_loss: 1.9559 - val_acc: 0.3355\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.9731 - acc: 0.3119 - val_loss: 1.9072 - val_acc: 0.3592\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.9431 - acc: 0.3250 - val_loss: 1.8859 - val_acc: 0.3691\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.9285 - acc: 0.3314 - val_loss: 1.8661 - val_acc: 0.3768\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.9163 - acc: 0.3372 - val_loss: 1.8625 - val_acc: 0.3775\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.9100 - acc: 0.3385 - val_loss: 1.8580 - val_acc: 0.3799\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.9011 - acc: 0.3422 - val_loss: 1.8559 - val_acc: 0.3767\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8941 - acc: 0.3469 - val_loss: 1.8426 - val_acc: 0.3842\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8882 - acc: 0.3483 - val_loss: 1.8373 - val_acc: 0.3843\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8823 - acc: 0.3510 - val_loss: 1.8268 - val_acc: 0.3920\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8783 - acc: 0.3519 - val_loss: 1.8331 - val_acc: 0.3842\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8753 - acc: 0.3540 - val_loss: 1.8398 - val_acc: 0.3823\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8713 - acc: 0.3559 - val_loss: 1.8382 - val_acc: 0.3808\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 21us/sample - loss: 1.8674 - acc: 0.3570 - val_loss: 1.8271 - val_acc: 0.3856\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8640 - acc: 0.3577 - val_loss: 1.8263 - val_acc: 0.3854\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8611 - acc: 0.3604 - val_loss: 1.8329 - val_acc: 0.3839\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8596 - acc: 0.3620 - val_loss: 1.8324 - val_acc: 0.3842\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8558 - acc: 0.3631 - val_loss: 1.8132 - val_acc: 0.3913\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8525 - acc: 0.3636 - val_loss: 1.8258 - val_acc: 0.3841\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8506 - acc: 0.3643 - val_loss: 1.8216 - val_acc: 0.3858\n",
            "32890/32890 [==============================] - 0s 6us/sample - loss: 1.8060 - acc: 0.3858\n",
            "Train on 105247 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105247/105247 [==============================] - 4s 42us/sample - loss: 2.2465 - acc: 0.2000 - val_loss: 1.9614 - val_acc: 0.3210\n",
            "Epoch 2/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.9687 - acc: 0.3172 - val_loss: 1.9262 - val_acc: 0.3462\n",
            "Epoch 3/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.9354 - acc: 0.3319 - val_loss: 1.8927 - val_acc: 0.3687\n",
            "Epoch 4/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.9185 - acc: 0.3389 - val_loss: 1.8894 - val_acc: 0.3658\n",
            "Epoch 5/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.9111 - acc: 0.3417 - val_loss: 1.8704 - val_acc: 0.3761\n",
            "Epoch 6/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.9031 - acc: 0.3448 - val_loss: 1.8788 - val_acc: 0.3704\n",
            "Epoch 7/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8951 - acc: 0.3486 - val_loss: 1.8672 - val_acc: 0.3761\n",
            "Epoch 8/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8918 - acc: 0.3511 - val_loss: 1.8557 - val_acc: 0.3756\n",
            "Epoch 9/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8846 - acc: 0.3533 - val_loss: 1.8626 - val_acc: 0.3731\n",
            "Epoch 10/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8807 - acc: 0.3550 - val_loss: 1.8529 - val_acc: 0.3777\n",
            "Epoch 11/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8759 - acc: 0.3573 - val_loss: 1.8504 - val_acc: 0.3764\n",
            "Epoch 12/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8735 - acc: 0.3574 - val_loss: 1.8511 - val_acc: 0.3784\n",
            "Epoch 13/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8698 - acc: 0.3591 - val_loss: 1.8412 - val_acc: 0.3798\n",
            "Epoch 14/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8656 - acc: 0.3600 - val_loss: 1.8330 - val_acc: 0.3831\n",
            "Epoch 15/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8619 - acc: 0.3621 - val_loss: 1.8325 - val_acc: 0.3828\n",
            "Epoch 16/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8596 - acc: 0.3626 - val_loss: 1.8416 - val_acc: 0.3801\n",
            "Epoch 17/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8559 - acc: 0.3647 - val_loss: 1.8350 - val_acc: 0.3843\n",
            "Epoch 18/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8544 - acc: 0.3639 - val_loss: 1.8217 - val_acc: 0.3878\n",
            "Epoch 19/20\n",
            "105247/105247 [==============================] - 2s 19us/sample - loss: 1.8526 - acc: 0.3661 - val_loss: 1.8294 - val_acc: 0.3856\n",
            "Epoch 20/20\n",
            "105247/105247 [==============================] - 2s 20us/sample - loss: 1.8481 - acc: 0.3656 - val_loss: 1.8251 - val_acc: 0.3870\n",
            "32890/32890 [==============================] - 0s 6us/sample - loss: 1.8473 - acc: 0.3619\n",
            "Train on 105248 samples, validate on 26312 samples\n",
            "Epoch 1/20\n",
            "105248/105248 [==============================] - 5s 43us/sample - loss: 2.2402 - acc: 0.2216 - val_loss: 1.9549 - val_acc: 0.3132\n",
            "Epoch 2/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.9731 - acc: 0.3167 - val_loss: 1.9061 - val_acc: 0.3386\n",
            "Epoch 3/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.9452 - acc: 0.3278 - val_loss: 1.8912 - val_acc: 0.3465\n",
            "Epoch 4/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.9274 - acc: 0.3348 - val_loss: 1.8762 - val_acc: 0.3439\n",
            "Epoch 5/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.9158 - acc: 0.3401 - val_loss: 1.8710 - val_acc: 0.3444\n",
            "Epoch 6/20\n",
            "105248/105248 [==============================] - 2s 19us/sample - loss: 1.9060 - acc: 0.3436 - val_loss: 1.8712 - val_acc: 0.3473\n",
            "Epoch 7/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8958 - acc: 0.3487 - val_loss: 1.8538 - val_acc: 0.3513\n",
            "Epoch 8/20\n",
            "105248/105248 [==============================] - 2s 19us/sample - loss: 1.8924 - acc: 0.3501 - val_loss: 1.8614 - val_acc: 0.3459\n",
            "Epoch 9/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8874 - acc: 0.3506 - val_loss: 1.8641 - val_acc: 0.3475\n",
            "Epoch 10/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8817 - acc: 0.3533 - val_loss: 1.8437 - val_acc: 0.3531\n",
            "Epoch 11/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8797 - acc: 0.3543 - val_loss: 1.8519 - val_acc: 0.3489\n",
            "Epoch 12/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8738 - acc: 0.3565 - val_loss: 1.8485 - val_acc: 0.3498\n",
            "Epoch 13/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8713 - acc: 0.3588 - val_loss: 1.8332 - val_acc: 0.3602\n",
            "Epoch 14/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8671 - acc: 0.3601 - val_loss: 1.8298 - val_acc: 0.3620\n",
            "Epoch 15/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8638 - acc: 0.3593 - val_loss: 1.8271 - val_acc: 0.3617\n",
            "Epoch 16/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8591 - acc: 0.3621 - val_loss: 1.8293 - val_acc: 0.3640\n",
            "Epoch 17/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8567 - acc: 0.3640 - val_loss: 1.8182 - val_acc: 0.3663\n",
            "Epoch 18/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8547 - acc: 0.3634 - val_loss: 1.8226 - val_acc: 0.3638\n",
            "Epoch 19/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8508 - acc: 0.3647 - val_loss: 1.8267 - val_acc: 0.3630\n",
            "Epoch 20/20\n",
            "105248/105248 [==============================] - 2s 20us/sample - loss: 1.8490 - acc: 0.3650 - val_loss: 1.8076 - val_acc: 0.3686\n",
            "32889/32889 [==============================] - 0s 6us/sample - loss: 1.8352 - acc: 0.3893\n",
            "Train on 131559 samples, validate on 32890 samples\n",
            "Epoch 1/20\n",
            "131559/131559 [==============================] - 13s 98us/sample - loss: 1.9834 - acc: 0.3116 - val_loss: 1.8801 - val_acc: 0.3715\n",
            "Epoch 2/20\n",
            "131559/131559 [==============================] - 12s 89us/sample - loss: 1.9041 - acc: 0.3435 - val_loss: 1.8780 - val_acc: 0.3767\n",
            "Epoch 3/20\n",
            "131559/131559 [==============================] - 10s 78us/sample - loss: 1.8852 - acc: 0.3507 - val_loss: 1.8493 - val_acc: 0.3901\n",
            "Epoch 4/20\n",
            "131559/131559 [==============================] - 10s 74us/sample - loss: 1.8710 - acc: 0.3562 - val_loss: 1.8251 - val_acc: 0.3952\n",
            "Epoch 5/20\n",
            "131559/131559 [==============================] - 9s 72us/sample - loss: 1.8596 - acc: 0.3611 - val_loss: 1.8351 - val_acc: 0.3908\n",
            "Epoch 6/20\n",
            "131559/131559 [==============================] - 9s 72us/sample - loss: 1.8510 - acc: 0.3636 - val_loss: 1.8278 - val_acc: 0.3943\n",
            "Epoch 7/20\n",
            "131559/131559 [==============================] - 9s 71us/sample - loss: 1.8439 - acc: 0.3665 - val_loss: 1.8319 - val_acc: 0.3904\n",
            "Epoch 8/20\n",
            "131559/131559 [==============================] - 10s 75us/sample - loss: 1.8386 - acc: 0.3691 - val_loss: 1.8292 - val_acc: 0.3922\n",
            "Epoch 9/20\n",
            "131559/131559 [==============================] - 9s 71us/sample - loss: 1.8329 - acc: 0.3711 - val_loss: 1.8276 - val_acc: 0.3911\n",
            "Epoch 10/20\n",
            "131559/131559 [==============================] - 10s 76us/sample - loss: 1.8290 - acc: 0.3721 - val_loss: 1.8304 - val_acc: 0.3929\n",
            "Epoch 11/20\n",
            "131559/131559 [==============================] - 10s 75us/sample - loss: 1.8254 - acc: 0.3737 - val_loss: 1.8051 - val_acc: 0.3969\n",
            "Epoch 12/20\n",
            "131559/131559 [==============================] - 10s 73us/sample - loss: 1.8211 - acc: 0.3743 - val_loss: 1.8101 - val_acc: 0.3971\n",
            "Epoch 13/20\n",
            "131559/131559 [==============================] - 10s 76us/sample - loss: 1.8202 - acc: 0.3743 - val_loss: 1.8147 - val_acc: 0.3947\n",
            "Epoch 14/20\n",
            "131559/131559 [==============================] - 10s 79us/sample - loss: 1.8162 - acc: 0.3761 - val_loss: 1.8224 - val_acc: 0.3926\n",
            "Epoch 15/20\n",
            "131559/131559 [==============================] - 9s 66us/sample - loss: 1.8147 - acc: 0.3766 - val_loss: 1.8162 - val_acc: 0.3959\n",
            "Epoch 16/20\n",
            "131559/131559 [==============================] - 9s 67us/sample - loss: 1.8105 - acc: 0.3779 - val_loss: 1.8087 - val_acc: 0.3994\n",
            "Epoch 17/20\n",
            "131559/131559 [==============================] - 9s 71us/sample - loss: 1.8097 - acc: 0.3779 - val_loss: 1.8161 - val_acc: 0.3962\n",
            "Epoch 18/20\n",
            "131559/131559 [==============================] - 9s 68us/sample - loss: 1.8079 - acc: 0.3792 - val_loss: 1.8100 - val_acc: 0.3971\n",
            "Epoch 19/20\n",
            "131559/131559 [==============================] - 9s 70us/sample - loss: 1.8054 - acc: 0.3808 - val_loss: 1.8107 - val_acc: 0.3975\n",
            "Epoch 20/20\n",
            "131559/131559 [==============================] - 10s 74us/sample - loss: 1.8041 - acc: 0.3789 - val_loss: 1.8057 - val_acc: 0.3997\n",
            "Best: 0.3725654661655426 using {'batch_size': 100}\n",
            "Means: 0.37231617569923403, Stdev: 0.018026276912008463 with: {'batch_size': 50}\n",
            "Means: 0.3725654661655426, Stdev: 0.016091393459418245 with: {'batch_size': 100}\n",
            "Means: 0.37238911390304563, Stdev: 0.017306123358360667 with: {'batch_size': 200}\n",
            "Means: 0.371701979637146, Stdev: 0.018197259159078594 with: {'batch_size': 500}\n",
            "Means: 0.3672811806201935, Stdev: 0.018418627979277905 with: {'batch_size': 1000}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxyXN-50P8FG",
        "colab_type": "text"
      },
      "source": [
        "### Epochs.*(500)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twmuWaKXQAFm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function for NN model.\n",
        "def create_model():\n",
        "      # set the model.\n",
        "    model = Sequential()\n",
        "    # hidden layers.\n",
        "    model.add(Dense(192, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(96, kernel_initializer='normal', activation='relu'))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(48, kernel_initializer='normal', activation='relu'))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(24, kernel_initializer='normal', activation='relu'))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(12, kernel_initializer='normal', activation='softmax'))\n",
        "    # function for top 3.\n",
        "    def top_3_accuracy(y_true, y_pred):\n",
        "        return top_k_categorical_accuracy(y_true, y_pred, k=3)\n",
        "\n",
        "    # set the optimizer.\n",
        "    optimizer = Adam(lr=0.001)\n",
        "    # compile the model.\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                        optimizer=optimizer,\n",
        "                        metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kMLQiYaQAKc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set the KerasClassifier model.\n",
        "model = KerasClassifier(build_fn=create_model, batch_size=100, validation_split=0.2, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wg_TyH6qQAIN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set the parameters for epochs.\n",
        "epochs = [100, 200, 500]\n",
        "param_grid = dict(epochs=epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-j_N4WbQ3sB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set the gridsearch.\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
        "# fit the model.\n",
        "grid_result = grid.fit(X_scaled, y_train)\n",
        "# show the results.\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LPcC1xetEJ8",
        "colab_type": "text"
      },
      "source": [
        "### Final Model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGn6XHZvtGwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function for NN model.\n",
        "def create_model():\n",
        "      # set the model.\n",
        "    model = Sequential()\n",
        "    # hidden layers.\n",
        "    model.add(Dense(192, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(96, kernel_initializer='normal', activation='relu'))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(48, kernel_initializer='normal', activation='relu'))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(24, kernel_initializer='normal', activation='relu'))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(12, kernel_initializer='normal', activation='softmax'))\n",
        "    # function for top 3.\n",
        "    def top_3_accuracy(y_true, y_pred):\n",
        "        return top_k_categorical_accuracy(y_true, y_pred, k=3)\n",
        "\n",
        "    # set the optimizer.\n",
        "    optimizer = Adam(lr=0.001)\n",
        "    # compile the model.\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                        optimizer=optimizer,\n",
        "                        metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jy_Wyu3NtG1G",
        "colab_type": "code",
        "outputId": "beb7ed0a-8329-441c-c90d-d1be562684a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# set the model.\n",
        "model = create_model()\n",
        "# fit the model, create history.\n",
        "history = model.fit(X_scaled, y_train, validation_split=0.2, epochs=500, batch_size=100)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:143: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Train on 131559 samples, validate on 32890 samples\n",
            "Epoch 1/500\n",
            "131559/131559 [==============================] - 7s 50us/sample - loss: 1.9785 - acc: 0.3146 - val_loss: 1.9059 - val_acc: 0.3668\n",
            "Epoch 2/500\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.9014 - acc: 0.3443 - val_loss: 1.8726 - val_acc: 0.3730\n",
            "Epoch 3/500\n",
            "131559/131559 [==============================] - 5s 36us/sample - loss: 1.8817 - acc: 0.3531 - val_loss: 1.8435 - val_acc: 0.3887\n",
            "Epoch 4/500\n",
            "131559/131559 [==============================] - 5s 36us/sample - loss: 1.8670 - acc: 0.3581 - val_loss: 1.8509 - val_acc: 0.3770\n",
            "Epoch 5/500\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.8545 - acc: 0.3614 - val_loss: 1.8227 - val_acc: 0.3968\n",
            "Epoch 6/500\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.8479 - acc: 0.3641 - val_loss: 1.8155 - val_acc: 0.3983\n",
            "Epoch 7/500\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.8402 - acc: 0.3673 - val_loss: 1.8237 - val_acc: 0.3983\n",
            "Epoch 8/500\n",
            "131559/131559 [==============================] - 5s 36us/sample - loss: 1.8385 - acc: 0.3676 - val_loss: 1.8306 - val_acc: 0.3939\n",
            "Epoch 9/500\n",
            "131559/131559 [==============================] - 5s 36us/sample - loss: 1.8315 - acc: 0.3705 - val_loss: 1.8209 - val_acc: 0.3969\n",
            "Epoch 10/500\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.8277 - acc: 0.3705 - val_loss: 1.8160 - val_acc: 0.3973\n",
            "Epoch 11/500\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.8243 - acc: 0.3727 - val_loss: 1.8233 - val_acc: 0.3948\n",
            "Epoch 12/500\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.8228 - acc: 0.3728 - val_loss: 1.8226 - val_acc: 0.3937\n",
            "Epoch 13/500\n",
            "131559/131559 [==============================] - 5s 36us/sample - loss: 1.8207 - acc: 0.3740 - val_loss: 1.8099 - val_acc: 0.3983\n",
            "Epoch 14/500\n",
            "131559/131559 [==============================] - 5s 36us/sample - loss: 1.8172 - acc: 0.3751 - val_loss: 1.8170 - val_acc: 0.3949\n",
            "Epoch 15/500\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.8147 - acc: 0.3755 - val_loss: 1.8274 - val_acc: 0.3920\n",
            "Epoch 16/500\n",
            "131559/131559 [==============================] - 5s 34us/sample - loss: 1.8105 - acc: 0.3764 - val_loss: 1.8127 - val_acc: 0.3954\n",
            "Epoch 17/500\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.8101 - acc: 0.3769 - val_loss: 1.8117 - val_acc: 0.3972\n",
            "Epoch 18/500\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.8081 - acc: 0.3780 - val_loss: 1.8048 - val_acc: 0.3967\n",
            "Epoch 19/500\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.8068 - acc: 0.3794 - val_loss: 1.8094 - val_acc: 0.3979\n",
            "Epoch 20/500\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.8061 - acc: 0.3792 - val_loss: 1.8099 - val_acc: 0.4003\n",
            "Epoch 21/500\n",
            "131559/131559 [==============================] - 5s 38us/sample - loss: 1.8034 - acc: 0.3794 - val_loss: 1.8006 - val_acc: 0.3991\n",
            "Epoch 22/500\n",
            "131559/131559 [==============================] - 5s 37us/sample - loss: 1.7996 - acc: 0.3793 - val_loss: 1.7950 - val_acc: 0.4043\n",
            "Epoch 23/500\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.8020 - acc: 0.3796 - val_loss: 1.8081 - val_acc: 0.4013\n",
            "Epoch 24/500\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.7988 - acc: 0.3813 - val_loss: 1.8191 - val_acc: 0.3955\n",
            "Epoch 25/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7963 - acc: 0.3810 - val_loss: 1.8103 - val_acc: 0.4001\n",
            "Epoch 26/500\n",
            "131559/131559 [==============================] - 5s 34us/sample - loss: 1.7969 - acc: 0.3824 - val_loss: 1.8096 - val_acc: 0.3970\n",
            "Epoch 27/500\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.7949 - acc: 0.3818 - val_loss: 1.8042 - val_acc: 0.3994\n",
            "Epoch 28/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7934 - acc: 0.3818 - val_loss: 1.8149 - val_acc: 0.3980\n",
            "Epoch 29/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7940 - acc: 0.3827 - val_loss: 1.8014 - val_acc: 0.4007\n",
            "Epoch 30/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7918 - acc: 0.3835 - val_loss: 1.8020 - val_acc: 0.3983\n",
            "Epoch 31/500\n",
            "131559/131559 [==============================] - 5s 34us/sample - loss: 1.7912 - acc: 0.3838 - val_loss: 1.8100 - val_acc: 0.3966\n",
            "Epoch 32/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7898 - acc: 0.3850 - val_loss: 1.7985 - val_acc: 0.3993\n",
            "Epoch 33/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7894 - acc: 0.3837 - val_loss: 1.7907 - val_acc: 0.4014\n",
            "Epoch 34/500\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.7890 - acc: 0.3846 - val_loss: 1.8013 - val_acc: 0.4027\n",
            "Epoch 35/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7870 - acc: 0.3855 - val_loss: 1.7949 - val_acc: 0.4029\n",
            "Epoch 36/500\n",
            "131559/131559 [==============================] - 5s 34us/sample - loss: 1.7883 - acc: 0.3847 - val_loss: 1.8108 - val_acc: 0.3990\n",
            "Epoch 37/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7846 - acc: 0.3849 - val_loss: 1.8116 - val_acc: 0.3968\n",
            "Epoch 38/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7847 - acc: 0.3871 - val_loss: 1.8032 - val_acc: 0.4020\n",
            "Epoch 39/500\n",
            "131559/131559 [==============================] - 5s 34us/sample - loss: 1.7862 - acc: 0.3854 - val_loss: 1.7943 - val_acc: 0.4031\n",
            "Epoch 40/500\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.7846 - acc: 0.3861 - val_loss: 1.7976 - val_acc: 0.4027\n",
            "Epoch 41/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7831 - acc: 0.3867 - val_loss: 1.8104 - val_acc: 0.3971\n",
            "Epoch 42/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7829 - acc: 0.3863 - val_loss: 1.7993 - val_acc: 0.4019\n",
            "Epoch 43/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7811 - acc: 0.3879 - val_loss: 1.7960 - val_acc: 0.4033\n",
            "Epoch 44/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7811 - acc: 0.3877 - val_loss: 1.7983 - val_acc: 0.4038\n",
            "Epoch 45/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7798 - acc: 0.3874 - val_loss: 1.8059 - val_acc: 0.3983\n",
            "Epoch 46/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7790 - acc: 0.3880 - val_loss: 1.7957 - val_acc: 0.3991\n",
            "Epoch 47/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7770 - acc: 0.3883 - val_loss: 1.8017 - val_acc: 0.3981\n",
            "Epoch 48/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7781 - acc: 0.3893 - val_loss: 1.7994 - val_acc: 0.4021\n",
            "Epoch 49/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7784 - acc: 0.3891 - val_loss: 1.8044 - val_acc: 0.4018\n",
            "Epoch 50/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7765 - acc: 0.3880 - val_loss: 1.8090 - val_acc: 0.4000\n",
            "Epoch 51/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7766 - acc: 0.3884 - val_loss: 1.8020 - val_acc: 0.3970\n",
            "Epoch 52/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7771 - acc: 0.3882 - val_loss: 1.7978 - val_acc: 0.4021\n",
            "Epoch 53/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7767 - acc: 0.3886 - val_loss: 1.8237 - val_acc: 0.3938\n",
            "Epoch 54/500\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.7741 - acc: 0.3900 - val_loss: 1.7957 - val_acc: 0.4053\n",
            "Epoch 55/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7740 - acc: 0.3896 - val_loss: 1.8035 - val_acc: 0.3995\n",
            "Epoch 56/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7745 - acc: 0.3883 - val_loss: 1.8032 - val_acc: 0.4015\n",
            "Epoch 57/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7745 - acc: 0.3899 - val_loss: 1.7913 - val_acc: 0.4054\n",
            "Epoch 58/500\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.7725 - acc: 0.3889 - val_loss: 1.8233 - val_acc: 0.3893\n",
            "Epoch 59/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7728 - acc: 0.3910 - val_loss: 1.8018 - val_acc: 0.4011\n",
            "Epoch 60/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7714 - acc: 0.3910 - val_loss: 1.8047 - val_acc: 0.3999\n",
            "Epoch 61/500\n",
            "131559/131559 [==============================] - 5s 34us/sample - loss: 1.7699 - acc: 0.3895 - val_loss: 1.8114 - val_acc: 0.4000\n",
            "Epoch 62/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7710 - acc: 0.3915 - val_loss: 1.8038 - val_acc: 0.4006\n",
            "Epoch 63/500\n",
            "131559/131559 [==============================] - 5s 34us/sample - loss: 1.7708 - acc: 0.3899 - val_loss: 1.7969 - val_acc: 0.4028\n",
            "Epoch 64/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7700 - acc: 0.3903 - val_loss: 1.8090 - val_acc: 0.4006\n",
            "Epoch 65/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7705 - acc: 0.3905 - val_loss: 1.8184 - val_acc: 0.3942\n",
            "Epoch 66/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7704 - acc: 0.3916 - val_loss: 1.8072 - val_acc: 0.3989\n",
            "Epoch 67/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7695 - acc: 0.3902 - val_loss: 1.7952 - val_acc: 0.4043\n",
            "Epoch 68/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7682 - acc: 0.3910 - val_loss: 1.8069 - val_acc: 0.3970\n",
            "Epoch 69/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7701 - acc: 0.3915 - val_loss: 1.8037 - val_acc: 0.4036\n",
            "Epoch 70/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7685 - acc: 0.3904 - val_loss: 1.8056 - val_acc: 0.4018\n",
            "Epoch 71/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7693 - acc: 0.3909 - val_loss: 1.8128 - val_acc: 0.3976\n",
            "Epoch 72/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7687 - acc: 0.3902 - val_loss: 1.8039 - val_acc: 0.4025\n",
            "Epoch 73/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7674 - acc: 0.3909 - val_loss: 1.7953 - val_acc: 0.4029\n",
            "Epoch 74/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7662 - acc: 0.3925 - val_loss: 1.8088 - val_acc: 0.3985\n",
            "Epoch 75/500\n",
            "131559/131559 [==============================] - 5s 34us/sample - loss: 1.7671 - acc: 0.3909 - val_loss: 1.8109 - val_acc: 0.3988\n",
            "Epoch 76/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7674 - acc: 0.3928 - val_loss: 1.8040 - val_acc: 0.4007\n",
            "Epoch 77/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7662 - acc: 0.3910 - val_loss: 1.7970 - val_acc: 0.4030\n",
            "Epoch 78/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7658 - acc: 0.3922 - val_loss: 1.8054 - val_acc: 0.4007\n",
            "Epoch 79/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7664 - acc: 0.3918 - val_loss: 1.8051 - val_acc: 0.3986\n",
            "Epoch 80/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7644 - acc: 0.3933 - val_loss: 1.8080 - val_acc: 0.4041\n",
            "Epoch 81/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7639 - acc: 0.3928 - val_loss: 1.8014 - val_acc: 0.4010\n",
            "Epoch 82/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7644 - acc: 0.3916 - val_loss: 1.8032 - val_acc: 0.3989\n",
            "Epoch 83/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7648 - acc: 0.3929 - val_loss: 1.8048 - val_acc: 0.3989\n",
            "Epoch 84/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7634 - acc: 0.3940 - val_loss: 1.8092 - val_acc: 0.3971\n",
            "Epoch 85/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7626 - acc: 0.3937 - val_loss: 1.8093 - val_acc: 0.4021\n",
            "Epoch 86/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7620 - acc: 0.3924 - val_loss: 1.8066 - val_acc: 0.4004\n",
            "Epoch 87/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7621 - acc: 0.3923 - val_loss: 1.8119 - val_acc: 0.3988\n",
            "Epoch 88/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7619 - acc: 0.3925 - val_loss: 1.8115 - val_acc: 0.3980\n",
            "Epoch 89/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7625 - acc: 0.3922 - val_loss: 1.7960 - val_acc: 0.4036\n",
            "Epoch 90/500\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.7621 - acc: 0.3931 - val_loss: 1.8146 - val_acc: 0.4005\n",
            "Epoch 91/500\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.7615 - acc: 0.3936 - val_loss: 1.8049 - val_acc: 0.3980\n",
            "Epoch 92/500\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.7622 - acc: 0.3940 - val_loss: 1.8112 - val_acc: 0.3994\n",
            "Epoch 93/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7619 - acc: 0.3931 - val_loss: 1.8154 - val_acc: 0.3974\n",
            "Epoch 94/500\n",
            "131559/131559 [==============================] - 5s 34us/sample - loss: 1.7599 - acc: 0.3941 - val_loss: 1.8005 - val_acc: 0.4024\n",
            "Epoch 95/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7606 - acc: 0.3935 - val_loss: 1.8099 - val_acc: 0.4019\n",
            "Epoch 96/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7617 - acc: 0.3927 - val_loss: 1.8073 - val_acc: 0.4019\n",
            "Epoch 97/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7607 - acc: 0.3940 - val_loss: 1.8277 - val_acc: 0.3933\n",
            "Epoch 98/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7576 - acc: 0.3946 - val_loss: 1.8123 - val_acc: 0.3992\n",
            "Epoch 99/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7612 - acc: 0.3939 - val_loss: 1.8061 - val_acc: 0.4031\n",
            "Epoch 100/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7594 - acc: 0.3936 - val_loss: 1.8031 - val_acc: 0.4048\n",
            "Epoch 101/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7583 - acc: 0.3938 - val_loss: 1.8165 - val_acc: 0.3992\n",
            "Epoch 102/500\n",
            "131559/131559 [==============================] - 5s 34us/sample - loss: 1.7596 - acc: 0.3947 - val_loss: 1.8015 - val_acc: 0.4037\n",
            "Epoch 103/500\n",
            "131559/131559 [==============================] - 5s 34us/sample - loss: 1.7592 - acc: 0.3938 - val_loss: 1.8149 - val_acc: 0.3976\n",
            "Epoch 104/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7608 - acc: 0.3934 - val_loss: 1.8027 - val_acc: 0.4009\n",
            "Epoch 105/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7593 - acc: 0.3935 - val_loss: 1.8107 - val_acc: 0.4008\n",
            "Epoch 106/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7595 - acc: 0.3946 - val_loss: 1.8126 - val_acc: 0.4014\n",
            "Epoch 107/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7585 - acc: 0.3946 - val_loss: 1.8149 - val_acc: 0.3978\n",
            "Epoch 108/500\n",
            "131559/131559 [==============================] - 5s 34us/sample - loss: 1.7587 - acc: 0.3945 - val_loss: 1.8055 - val_acc: 0.4004\n",
            "Epoch 109/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7571 - acc: 0.3949 - val_loss: 1.8101 - val_acc: 0.4010\n",
            "Epoch 110/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7583 - acc: 0.3936 - val_loss: 1.8063 - val_acc: 0.3995\n",
            "Epoch 111/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7558 - acc: 0.3954 - val_loss: 1.8073 - val_acc: 0.4031\n",
            "Epoch 112/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7572 - acc: 0.3952 - val_loss: 1.7983 - val_acc: 0.3997\n",
            "Epoch 113/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7565 - acc: 0.3937 - val_loss: 1.8086 - val_acc: 0.3985\n",
            "Epoch 114/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7555 - acc: 0.3954 - val_loss: 1.8081 - val_acc: 0.4011\n",
            "Epoch 115/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7556 - acc: 0.3958 - val_loss: 1.8128 - val_acc: 0.3974\n",
            "Epoch 116/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7585 - acc: 0.3944 - val_loss: 1.8022 - val_acc: 0.4046\n",
            "Epoch 117/500\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.7547 - acc: 0.3955 - val_loss: 1.8045 - val_acc: 0.4024\n",
            "Epoch 118/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7560 - acc: 0.3946 - val_loss: 1.8073 - val_acc: 0.4004\n",
            "Epoch 119/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7548 - acc: 0.3960 - val_loss: 1.7997 - val_acc: 0.4060\n",
            "Epoch 120/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7554 - acc: 0.3948 - val_loss: 1.8149 - val_acc: 0.3978\n",
            "Epoch 121/500\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.7552 - acc: 0.3953 - val_loss: 1.8197 - val_acc: 0.3992\n",
            "Epoch 122/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7555 - acc: 0.3954 - val_loss: 1.8027 - val_acc: 0.4022\n",
            "Epoch 123/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7541 - acc: 0.3965 - val_loss: 1.8193 - val_acc: 0.3984\n",
            "Epoch 124/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7554 - acc: 0.3956 - val_loss: 1.8073 - val_acc: 0.4001\n",
            "Epoch 125/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7550 - acc: 0.3962 - val_loss: 1.8018 - val_acc: 0.4018\n",
            "Epoch 126/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7552 - acc: 0.3961 - val_loss: 1.8175 - val_acc: 0.4031\n",
            "Epoch 127/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7548 - acc: 0.3948 - val_loss: 1.8024 - val_acc: 0.4019\n",
            "Epoch 128/500\n",
            "131559/131559 [==============================] - 5s 34us/sample - loss: 1.7541 - acc: 0.3958 - val_loss: 1.8121 - val_acc: 0.3998\n",
            "Epoch 129/500\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.7542 - acc: 0.3972 - val_loss: 1.8217 - val_acc: 0.3968\n",
            "Epoch 130/500\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.7531 - acc: 0.3969 - val_loss: 1.8195 - val_acc: 0.3961\n",
            "Epoch 131/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7529 - acc: 0.3949 - val_loss: 1.8006 - val_acc: 0.4037\n",
            "Epoch 132/500\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.7552 - acc: 0.3948 - val_loss: 1.8142 - val_acc: 0.3962\n",
            "Epoch 133/500\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.7538 - acc: 0.3954 - val_loss: 1.8179 - val_acc: 0.3954\n",
            "Epoch 134/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7530 - acc: 0.3962 - val_loss: 1.8156 - val_acc: 0.3969\n",
            "Epoch 135/500\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.7530 - acc: 0.3960 - val_loss: 1.8176 - val_acc: 0.3984\n",
            "Epoch 136/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7525 - acc: 0.3965 - val_loss: 1.8103 - val_acc: 0.3980\n",
            "Epoch 137/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7520 - acc: 0.3954 - val_loss: 1.8179 - val_acc: 0.3978\n",
            "Epoch 138/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7530 - acc: 0.3960 - val_loss: 1.8225 - val_acc: 0.3969\n",
            "Epoch 139/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7522 - acc: 0.3957 - val_loss: 1.8048 - val_acc: 0.4046\n",
            "Epoch 140/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7530 - acc: 0.3966 - val_loss: 1.8158 - val_acc: 0.4010\n",
            "Epoch 141/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7518 - acc: 0.3964 - val_loss: 1.8081 - val_acc: 0.3996\n",
            "Epoch 142/500\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.7531 - acc: 0.3965 - val_loss: 1.8168 - val_acc: 0.3983\n",
            "Epoch 143/500\n",
            "131559/131559 [==============================] - 5s 34us/sample - loss: 1.7532 - acc: 0.3971 - val_loss: 1.8207 - val_acc: 0.3928\n",
            "Epoch 144/500\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.7529 - acc: 0.3964 - val_loss: 1.8172 - val_acc: 0.3981\n",
            "Epoch 145/500\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.7533 - acc: 0.3965 - val_loss: 1.8086 - val_acc: 0.3984\n",
            "Epoch 146/500\n",
            "131559/131559 [==============================] - 5s 34us/sample - loss: 1.7498 - acc: 0.3974 - val_loss: 1.8137 - val_acc: 0.3987\n",
            "Epoch 147/500\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.7514 - acc: 0.3970 - val_loss: 1.8193 - val_acc: 0.3943\n",
            "Epoch 148/500\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.7504 - acc: 0.3973 - val_loss: 1.8241 - val_acc: 0.3996\n",
            "Epoch 149/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7511 - acc: 0.3968 - val_loss: 1.8087 - val_acc: 0.4009\n",
            "Epoch 150/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7512 - acc: 0.3963 - val_loss: 1.8138 - val_acc: 0.3995\n",
            "Epoch 151/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7515 - acc: 0.3958 - val_loss: 1.8115 - val_acc: 0.3996\n",
            "Epoch 152/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7495 - acc: 0.3967 - val_loss: 1.8178 - val_acc: 0.3981\n",
            "Epoch 153/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7498 - acc: 0.3956 - val_loss: 1.8291 - val_acc: 0.3935\n",
            "Epoch 154/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7501 - acc: 0.3966 - val_loss: 1.8246 - val_acc: 0.3951\n",
            "Epoch 155/500\n",
            "131559/131559 [==============================] - 5s 34us/sample - loss: 1.7498 - acc: 0.3965 - val_loss: 1.8208 - val_acc: 0.3973\n",
            "Epoch 156/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7510 - acc: 0.3965 - val_loss: 1.8270 - val_acc: 0.3917\n",
            "Epoch 157/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7487 - acc: 0.3957 - val_loss: 1.8177 - val_acc: 0.3995\n",
            "Epoch 158/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7508 - acc: 0.3974 - val_loss: 1.8138 - val_acc: 0.3985\n",
            "Epoch 159/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7498 - acc: 0.3980 - val_loss: 1.8167 - val_acc: 0.3962\n",
            "Epoch 160/500\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.7508 - acc: 0.3962 - val_loss: 1.8245 - val_acc: 0.3974\n",
            "Epoch 161/500\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.7491 - acc: 0.3972 - val_loss: 1.8256 - val_acc: 0.3960\n",
            "Epoch 162/500\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.7497 - acc: 0.3980 - val_loss: 1.8195 - val_acc: 0.3978\n",
            "Epoch 163/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7492 - acc: 0.3979 - val_loss: 1.8196 - val_acc: 0.4011\n",
            "Epoch 164/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7495 - acc: 0.3971 - val_loss: 1.8149 - val_acc: 0.3997\n",
            "Epoch 165/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7503 - acc: 0.3973 - val_loss: 1.8073 - val_acc: 0.4004\n",
            "Epoch 166/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7495 - acc: 0.3967 - val_loss: 1.8051 - val_acc: 0.4042\n",
            "Epoch 167/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7503 - acc: 0.3964 - val_loss: 1.8126 - val_acc: 0.3985\n",
            "Epoch 168/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7498 - acc: 0.3983 - val_loss: 1.8088 - val_acc: 0.4006\n",
            "Epoch 169/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7495 - acc: 0.3983 - val_loss: 1.7997 - val_acc: 0.4012\n",
            "Epoch 170/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7490 - acc: 0.3982 - val_loss: 1.8164 - val_acc: 0.3992\n",
            "Epoch 171/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7478 - acc: 0.3976 - val_loss: 1.8119 - val_acc: 0.3977\n",
            "Epoch 172/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7482 - acc: 0.3989 - val_loss: 1.8149 - val_acc: 0.3990\n",
            "Epoch 173/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7479 - acc: 0.3983 - val_loss: 1.8215 - val_acc: 0.3961\n",
            "Epoch 174/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7496 - acc: 0.3981 - val_loss: 1.8126 - val_acc: 0.3993\n",
            "Epoch 175/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7482 - acc: 0.3970 - val_loss: 1.8016 - val_acc: 0.4032\n",
            "Epoch 176/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7483 - acc: 0.3978 - val_loss: 1.8188 - val_acc: 0.3962\n",
            "Epoch 177/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7463 - acc: 0.3995 - val_loss: 1.8167 - val_acc: 0.3996\n",
            "Epoch 178/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7482 - acc: 0.3986 - val_loss: 1.8210 - val_acc: 0.3987\n",
            "Epoch 179/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7480 - acc: 0.3993 - val_loss: 1.8100 - val_acc: 0.3993\n",
            "Epoch 180/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7452 - acc: 0.4003 - val_loss: 1.8122 - val_acc: 0.4003\n",
            "Epoch 181/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7460 - acc: 0.3982 - val_loss: 1.8130 - val_acc: 0.3992\n",
            "Epoch 182/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7483 - acc: 0.3980 - val_loss: 1.8164 - val_acc: 0.3975\n",
            "Epoch 183/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7471 - acc: 0.3986 - val_loss: 1.8141 - val_acc: 0.3964\n",
            "Epoch 184/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7468 - acc: 0.3985 - val_loss: 1.8225 - val_acc: 0.3947\n",
            "Epoch 185/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7484 - acc: 0.3987 - val_loss: 1.8065 - val_acc: 0.4008\n",
            "Epoch 186/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7482 - acc: 0.3973 - val_loss: 1.8197 - val_acc: 0.3956\n",
            "Epoch 187/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7460 - acc: 0.3994 - val_loss: 1.8135 - val_acc: 0.3986\n",
            "Epoch 188/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7462 - acc: 0.3975 - val_loss: 1.8012 - val_acc: 0.4009\n",
            "Epoch 189/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7452 - acc: 0.3984 - val_loss: 1.8184 - val_acc: 0.3948\n",
            "Epoch 190/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7449 - acc: 0.3992 - val_loss: 1.8174 - val_acc: 0.3988\n",
            "Epoch 191/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7478 - acc: 0.3978 - val_loss: 1.8197 - val_acc: 0.3982\n",
            "Epoch 192/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7455 - acc: 0.3996 - val_loss: 1.8160 - val_acc: 0.3943\n",
            "Epoch 193/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7474 - acc: 0.3975 - val_loss: 1.8293 - val_acc: 0.3927\n",
            "Epoch 194/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7454 - acc: 0.3993 - val_loss: 1.8116 - val_acc: 0.3994\n",
            "Epoch 195/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7454 - acc: 0.3989 - val_loss: 1.8112 - val_acc: 0.4018\n",
            "Epoch 196/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7465 - acc: 0.3962 - val_loss: 1.8194 - val_acc: 0.3961\n",
            "Epoch 197/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7448 - acc: 0.3985 - val_loss: 1.8140 - val_acc: 0.3983\n",
            "Epoch 198/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7460 - acc: 0.3987 - val_loss: 1.8188 - val_acc: 0.3992\n",
            "Epoch 199/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7462 - acc: 0.3985 - val_loss: 1.8125 - val_acc: 0.3996\n",
            "Epoch 200/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7446 - acc: 0.3989 - val_loss: 1.8117 - val_acc: 0.3974\n",
            "Epoch 201/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7465 - acc: 0.3986 - val_loss: 1.8223 - val_acc: 0.3910\n",
            "Epoch 202/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7464 - acc: 0.3985 - val_loss: 1.8190 - val_acc: 0.3977\n",
            "Epoch 203/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7446 - acc: 0.3987 - val_loss: 1.8217 - val_acc: 0.3929\n",
            "Epoch 204/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7439 - acc: 0.3985 - val_loss: 1.8174 - val_acc: 0.3969\n",
            "Epoch 205/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7452 - acc: 0.3978 - val_loss: 1.8141 - val_acc: 0.3986\n",
            "Epoch 206/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7456 - acc: 0.3984 - val_loss: 1.8210 - val_acc: 0.3967\n",
            "Epoch 207/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7459 - acc: 0.3990 - val_loss: 1.8234 - val_acc: 0.3971\n",
            "Epoch 208/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7435 - acc: 0.3989 - val_loss: 1.8218 - val_acc: 0.3985\n",
            "Epoch 209/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7450 - acc: 0.3987 - val_loss: 1.8135 - val_acc: 0.3985\n",
            "Epoch 210/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7440 - acc: 0.3984 - val_loss: 1.8154 - val_acc: 0.3989\n",
            "Epoch 211/500\n",
            "131559/131559 [==============================] - 5s 34us/sample - loss: 1.7458 - acc: 0.3979 - val_loss: 1.8098 - val_acc: 0.4018\n",
            "Epoch 212/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7455 - acc: 0.3996 - val_loss: 1.8249 - val_acc: 0.3967\n",
            "Epoch 213/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7462 - acc: 0.3979 - val_loss: 1.8175 - val_acc: 0.3985\n",
            "Epoch 214/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7431 - acc: 0.3998 - val_loss: 1.8200 - val_acc: 0.3943\n",
            "Epoch 215/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7458 - acc: 0.3992 - val_loss: 1.8067 - val_acc: 0.4033\n",
            "Epoch 216/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7446 - acc: 0.3985 - val_loss: 1.8103 - val_acc: 0.3974\n",
            "Epoch 217/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7445 - acc: 0.3989 - val_loss: 1.8352 - val_acc: 0.3899\n",
            "Epoch 218/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7435 - acc: 0.3994 - val_loss: 1.8145 - val_acc: 0.3998\n",
            "Epoch 219/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7457 - acc: 0.3984 - val_loss: 1.8209 - val_acc: 0.3946\n",
            "Epoch 220/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7427 - acc: 0.3993 - val_loss: 1.8123 - val_acc: 0.4013\n",
            "Epoch 221/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7421 - acc: 0.3998 - val_loss: 1.8171 - val_acc: 0.4008\n",
            "Epoch 222/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7436 - acc: 0.3988 - val_loss: 1.8036 - val_acc: 0.4001\n",
            "Epoch 223/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7455 - acc: 0.3988 - val_loss: 1.8152 - val_acc: 0.3976\n",
            "Epoch 224/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7443 - acc: 0.3994 - val_loss: 1.8155 - val_acc: 0.3992\n",
            "Epoch 225/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7426 - acc: 0.4007 - val_loss: 1.8164 - val_acc: 0.3991\n",
            "Epoch 226/500\n",
            "131559/131559 [==============================] - 5s 34us/sample - loss: 1.7417 - acc: 0.3988 - val_loss: 1.8190 - val_acc: 0.3980\n",
            "Epoch 227/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7433 - acc: 0.3989 - val_loss: 1.8172 - val_acc: 0.3946\n",
            "Epoch 228/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7428 - acc: 0.3992 - val_loss: 1.8160 - val_acc: 0.3966\n",
            "Epoch 229/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7422 - acc: 0.3997 - val_loss: 1.8089 - val_acc: 0.3981\n",
            "Epoch 230/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7431 - acc: 0.3994 - val_loss: 1.8106 - val_acc: 0.3991\n",
            "Epoch 231/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7433 - acc: 0.3987 - val_loss: 1.8257 - val_acc: 0.3986\n",
            "Epoch 232/500\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.7427 - acc: 0.3982 - val_loss: 1.8191 - val_acc: 0.3954\n",
            "Epoch 233/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7423 - acc: 0.4006 - val_loss: 1.8116 - val_acc: 0.3978\n",
            "Epoch 234/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7424 - acc: 0.3993 - val_loss: 1.8157 - val_acc: 0.4005\n",
            "Epoch 235/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7416 - acc: 0.4000 - val_loss: 1.8222 - val_acc: 0.3935\n",
            "Epoch 236/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7413 - acc: 0.3996 - val_loss: 1.8162 - val_acc: 0.3974\n",
            "Epoch 237/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7419 - acc: 0.3994 - val_loss: 1.8156 - val_acc: 0.3992\n",
            "Epoch 238/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7442 - acc: 0.3997 - val_loss: 1.8134 - val_acc: 0.3954\n",
            "Epoch 239/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7423 - acc: 0.3999 - val_loss: 1.8104 - val_acc: 0.4007\n",
            "Epoch 240/500\n",
            "131559/131559 [==============================] - 5s 34us/sample - loss: 1.7419 - acc: 0.3996 - val_loss: 1.8198 - val_acc: 0.3960\n",
            "Epoch 241/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7428 - acc: 0.4000 - val_loss: 1.8168 - val_acc: 0.3982\n",
            "Epoch 242/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7416 - acc: 0.3993 - val_loss: 1.8121 - val_acc: 0.4006\n",
            "Epoch 243/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7415 - acc: 0.3984 - val_loss: 1.8115 - val_acc: 0.4009\n",
            "Epoch 244/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7412 - acc: 0.3999 - val_loss: 1.8111 - val_acc: 0.3964\n",
            "Epoch 245/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7417 - acc: 0.3992 - val_loss: 1.8109 - val_acc: 0.3959\n",
            "Epoch 246/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7404 - acc: 0.4002 - val_loss: 1.8013 - val_acc: 0.4050\n",
            "Epoch 247/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7432 - acc: 0.3999 - val_loss: 1.8215 - val_acc: 0.3915\n",
            "Epoch 248/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7418 - acc: 0.4006 - val_loss: 1.8213 - val_acc: 0.3971\n",
            "Epoch 249/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7402 - acc: 0.3997 - val_loss: 1.8128 - val_acc: 0.4017\n",
            "Epoch 250/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7426 - acc: 0.4011 - val_loss: 1.8140 - val_acc: 0.3993\n",
            "Epoch 251/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7432 - acc: 0.4002 - val_loss: 1.8116 - val_acc: 0.4017\n",
            "Epoch 252/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7417 - acc: 0.4006 - val_loss: 1.8142 - val_acc: 0.3981\n",
            "Epoch 253/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7398 - acc: 0.3999 - val_loss: 1.8144 - val_acc: 0.4007\n",
            "Epoch 254/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7398 - acc: 0.4011 - val_loss: 1.8322 - val_acc: 0.3939\n",
            "Epoch 255/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7406 - acc: 0.3993 - val_loss: 1.8289 - val_acc: 0.3969\n",
            "Epoch 256/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7416 - acc: 0.3995 - val_loss: 1.8170 - val_acc: 0.3974\n",
            "Epoch 257/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7405 - acc: 0.3999 - val_loss: 1.8131 - val_acc: 0.3968\n",
            "Epoch 258/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7419 - acc: 0.3994 - val_loss: 1.8294 - val_acc: 0.3993\n",
            "Epoch 259/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7428 - acc: 0.4011 - val_loss: 1.8197 - val_acc: 0.3988\n",
            "Epoch 260/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7403 - acc: 0.4007 - val_loss: 1.8223 - val_acc: 0.3972\n",
            "Epoch 261/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7402 - acc: 0.4002 - val_loss: 1.8164 - val_acc: 0.3972\n",
            "Epoch 262/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7413 - acc: 0.4006 - val_loss: 1.8361 - val_acc: 0.3937\n",
            "Epoch 263/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7414 - acc: 0.4005 - val_loss: 1.8253 - val_acc: 0.3932\n",
            "Epoch 264/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7407 - acc: 0.4001 - val_loss: 1.8194 - val_acc: 0.4012\n",
            "Epoch 265/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7410 - acc: 0.4002 - val_loss: 1.8075 - val_acc: 0.4008\n",
            "Epoch 266/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7426 - acc: 0.4002 - val_loss: 1.8157 - val_acc: 0.4002\n",
            "Epoch 267/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7408 - acc: 0.4002 - val_loss: 1.8177 - val_acc: 0.3992\n",
            "Epoch 268/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7412 - acc: 0.4013 - val_loss: 1.8228 - val_acc: 0.4000\n",
            "Epoch 269/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7426 - acc: 0.4002 - val_loss: 1.8245 - val_acc: 0.3952\n",
            "Epoch 270/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7423 - acc: 0.4002 - val_loss: 1.8313 - val_acc: 0.3941\n",
            "Epoch 271/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7386 - acc: 0.4011 - val_loss: 1.8227 - val_acc: 0.3943\n",
            "Epoch 272/500\n",
            "131559/131559 [==============================] - 5s 36us/sample - loss: 1.7400 - acc: 0.3996 - val_loss: 1.8174 - val_acc: 0.4005\n",
            "Epoch 273/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7399 - acc: 0.4015 - val_loss: 1.8317 - val_acc: 0.3943\n",
            "Epoch 274/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7407 - acc: 0.4012 - val_loss: 1.8238 - val_acc: 0.3970\n",
            "Epoch 275/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7394 - acc: 0.4004 - val_loss: 1.8159 - val_acc: 0.4012\n",
            "Epoch 276/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7406 - acc: 0.4005 - val_loss: 1.8213 - val_acc: 0.3999\n",
            "Epoch 277/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7405 - acc: 0.4001 - val_loss: 1.8204 - val_acc: 0.3978\n",
            "Epoch 278/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7391 - acc: 0.4003 - val_loss: 1.8180 - val_acc: 0.4000\n",
            "Epoch 279/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7374 - acc: 0.4007 - val_loss: 1.8278 - val_acc: 0.3950\n",
            "Epoch 280/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7392 - acc: 0.4016 - val_loss: 1.8220 - val_acc: 0.3973\n",
            "Epoch 281/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7392 - acc: 0.4011 - val_loss: 1.8225 - val_acc: 0.3973\n",
            "Epoch 282/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7404 - acc: 0.3994 - val_loss: 1.8138 - val_acc: 0.3997\n",
            "Epoch 283/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7397 - acc: 0.4013 - val_loss: 1.8103 - val_acc: 0.3968\n",
            "Epoch 284/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7391 - acc: 0.4007 - val_loss: 1.8165 - val_acc: 0.3994\n",
            "Epoch 285/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7384 - acc: 0.4001 - val_loss: 1.8147 - val_acc: 0.4025\n",
            "Epoch 286/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7389 - acc: 0.3997 - val_loss: 1.8318 - val_acc: 0.3874\n",
            "Epoch 287/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7391 - acc: 0.4012 - val_loss: 1.8161 - val_acc: 0.3981\n",
            "Epoch 288/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7393 - acc: 0.4009 - val_loss: 1.8184 - val_acc: 0.4003\n",
            "Epoch 289/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7389 - acc: 0.4014 - val_loss: 1.8165 - val_acc: 0.3992\n",
            "Epoch 290/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7402 - acc: 0.3993 - val_loss: 1.8242 - val_acc: 0.3979\n",
            "Epoch 291/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7382 - acc: 0.4017 - val_loss: 1.8132 - val_acc: 0.3993\n",
            "Epoch 292/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7379 - acc: 0.4003 - val_loss: 1.8194 - val_acc: 0.3974\n",
            "Epoch 293/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7378 - acc: 0.4006 - val_loss: 1.8195 - val_acc: 0.3979\n",
            "Epoch 294/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7402 - acc: 0.4011 - val_loss: 1.8171 - val_acc: 0.3975\n",
            "Epoch 295/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7374 - acc: 0.4005 - val_loss: 1.8186 - val_acc: 0.3981\n",
            "Epoch 296/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7384 - acc: 0.4007 - val_loss: 1.8120 - val_acc: 0.3989\n",
            "Epoch 297/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7381 - acc: 0.4001 - val_loss: 1.8222 - val_acc: 0.3960\n",
            "Epoch 298/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7401 - acc: 0.4016 - val_loss: 1.8077 - val_acc: 0.4021\n",
            "Epoch 299/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7370 - acc: 0.4015 - val_loss: 1.8224 - val_acc: 0.3977\n",
            "Epoch 300/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7383 - acc: 0.4005 - val_loss: 1.8158 - val_acc: 0.4005\n",
            "Epoch 301/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7387 - acc: 0.4008 - val_loss: 1.8227 - val_acc: 0.3987\n",
            "Epoch 302/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7374 - acc: 0.4009 - val_loss: 1.8138 - val_acc: 0.4024\n",
            "Epoch 303/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7385 - acc: 0.4013 - val_loss: 1.8195 - val_acc: 0.3975\n",
            "Epoch 304/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7386 - acc: 0.4005 - val_loss: 1.8205 - val_acc: 0.4015\n",
            "Epoch 305/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7365 - acc: 0.4024 - val_loss: 1.8308 - val_acc: 0.3965\n",
            "Epoch 306/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7368 - acc: 0.4007 - val_loss: 1.8185 - val_acc: 0.4005\n",
            "Epoch 307/500\n",
            "131559/131559 [==============================] - 4s 31us/sample - loss: 1.7389 - acc: 0.4006 - val_loss: 1.8101 - val_acc: 0.4027\n",
            "Epoch 308/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7372 - acc: 0.4008 - val_loss: 1.8153 - val_acc: 0.3972\n",
            "Epoch 309/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7393 - acc: 0.4015 - val_loss: 1.8237 - val_acc: 0.3957\n",
            "Epoch 310/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7376 - acc: 0.4024 - val_loss: 1.8200 - val_acc: 0.3972\n",
            "Epoch 311/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7390 - acc: 0.4010 - val_loss: 1.8184 - val_acc: 0.3978\n",
            "Epoch 312/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7377 - acc: 0.4003 - val_loss: 1.8209 - val_acc: 0.3969\n",
            "Epoch 313/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7367 - acc: 0.4020 - val_loss: 1.8209 - val_acc: 0.3978\n",
            "Epoch 314/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7373 - acc: 0.4016 - val_loss: 1.8303 - val_acc: 0.3946\n",
            "Epoch 315/500\n",
            "131559/131559 [==============================] - 4s 31us/sample - loss: 1.7365 - acc: 0.4020 - val_loss: 1.8214 - val_acc: 0.3988\n",
            "Epoch 316/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7373 - acc: 0.4010 - val_loss: 1.8103 - val_acc: 0.4006\n",
            "Epoch 317/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7352 - acc: 0.4013 - val_loss: 1.8264 - val_acc: 0.3964\n",
            "Epoch 318/500\n",
            "131559/131559 [==============================] - 4s 31us/sample - loss: 1.7362 - acc: 0.4019 - val_loss: 1.8268 - val_acc: 0.3975\n",
            "Epoch 319/500\n",
            "131559/131559 [==============================] - 4s 31us/sample - loss: 1.7366 - acc: 0.3992 - val_loss: 1.8116 - val_acc: 0.4003\n",
            "Epoch 320/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7364 - acc: 0.4008 - val_loss: 1.8235 - val_acc: 0.3948\n",
            "Epoch 321/500\n",
            "131559/131559 [==============================] - 4s 31us/sample - loss: 1.7366 - acc: 0.4005 - val_loss: 1.8287 - val_acc: 0.3948\n",
            "Epoch 322/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7358 - acc: 0.4010 - val_loss: 1.8251 - val_acc: 0.3991\n",
            "Epoch 323/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7371 - acc: 0.4016 - val_loss: 1.8194 - val_acc: 0.3979\n",
            "Epoch 324/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7374 - acc: 0.4026 - val_loss: 1.8206 - val_acc: 0.3970\n",
            "Epoch 325/500\n",
            "131559/131559 [==============================] - 4s 31us/sample - loss: 1.7373 - acc: 0.4014 - val_loss: 1.8201 - val_acc: 0.4004\n",
            "Epoch 326/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7360 - acc: 0.4013 - val_loss: 1.8306 - val_acc: 0.3954\n",
            "Epoch 327/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7368 - acc: 0.4022 - val_loss: 1.8186 - val_acc: 0.3992\n",
            "Epoch 328/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7356 - acc: 0.4021 - val_loss: 1.8244 - val_acc: 0.3981\n",
            "Epoch 329/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7359 - acc: 0.4007 - val_loss: 1.8245 - val_acc: 0.3988\n",
            "Epoch 330/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7374 - acc: 0.4018 - val_loss: 1.8144 - val_acc: 0.3971\n",
            "Epoch 331/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7367 - acc: 0.4022 - val_loss: 1.8193 - val_acc: 0.3989\n",
            "Epoch 332/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7356 - acc: 0.4022 - val_loss: 1.8213 - val_acc: 0.3972\n",
            "Epoch 333/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7360 - acc: 0.4021 - val_loss: 1.8211 - val_acc: 0.3980\n",
            "Epoch 334/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7369 - acc: 0.4017 - val_loss: 1.8294 - val_acc: 0.3956\n",
            "Epoch 335/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7359 - acc: 0.4014 - val_loss: 1.8304 - val_acc: 0.3957\n",
            "Epoch 336/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7359 - acc: 0.4029 - val_loss: 1.8213 - val_acc: 0.4024\n",
            "Epoch 337/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7354 - acc: 0.4011 - val_loss: 1.8228 - val_acc: 0.3986\n",
            "Epoch 338/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7343 - acc: 0.4026 - val_loss: 1.8243 - val_acc: 0.3946\n",
            "Epoch 339/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7347 - acc: 0.4016 - val_loss: 1.8332 - val_acc: 0.3903\n",
            "Epoch 340/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7347 - acc: 0.4028 - val_loss: 1.8185 - val_acc: 0.4018\n",
            "Epoch 341/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7358 - acc: 0.4020 - val_loss: 1.8196 - val_acc: 0.3976\n",
            "Epoch 342/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7373 - acc: 0.4010 - val_loss: 1.8207 - val_acc: 0.3976\n",
            "Epoch 343/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7360 - acc: 0.4018 - val_loss: 1.8254 - val_acc: 0.3985\n",
            "Epoch 344/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7357 - acc: 0.4028 - val_loss: 1.8209 - val_acc: 0.3974\n",
            "Epoch 345/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7357 - acc: 0.4014 - val_loss: 1.8254 - val_acc: 0.3964\n",
            "Epoch 346/500\n",
            "131559/131559 [==============================] - 4s 31us/sample - loss: 1.7356 - acc: 0.4017 - val_loss: 1.8216 - val_acc: 0.3967\n",
            "Epoch 347/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7364 - acc: 0.4006 - val_loss: 1.8168 - val_acc: 0.4002\n",
            "Epoch 348/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7353 - acc: 0.4011 - val_loss: 1.8156 - val_acc: 0.3982\n",
            "Epoch 349/500\n",
            "131559/131559 [==============================] - 4s 31us/sample - loss: 1.7359 - acc: 0.4014 - val_loss: 1.8365 - val_acc: 0.3914\n",
            "Epoch 350/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7348 - acc: 0.4016 - val_loss: 1.8126 - val_acc: 0.4017\n",
            "Epoch 351/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7358 - acc: 0.4017 - val_loss: 1.8271 - val_acc: 0.3978\n",
            "Epoch 352/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7359 - acc: 0.4023 - val_loss: 1.8336 - val_acc: 0.3925\n",
            "Epoch 353/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7351 - acc: 0.4016 - val_loss: 1.8249 - val_acc: 0.3979\n",
            "Epoch 354/500\n",
            "131559/131559 [==============================] - 4s 31us/sample - loss: 1.7361 - acc: 0.4007 - val_loss: 1.8196 - val_acc: 0.3999\n",
            "Epoch 355/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7357 - acc: 0.4017 - val_loss: 1.8283 - val_acc: 0.3955\n",
            "Epoch 356/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7362 - acc: 0.4013 - val_loss: 1.8246 - val_acc: 0.3931\n",
            "Epoch 357/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7351 - acc: 0.4021 - val_loss: 1.8142 - val_acc: 0.3990\n",
            "Epoch 358/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7364 - acc: 0.4013 - val_loss: 1.8232 - val_acc: 0.3957\n",
            "Epoch 359/500\n",
            "131559/131559 [==============================] - 4s 31us/sample - loss: 1.7332 - acc: 0.4034 - val_loss: 1.8127 - val_acc: 0.3984\n",
            "Epoch 360/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7368 - acc: 0.4023 - val_loss: 1.8177 - val_acc: 0.3997\n",
            "Epoch 361/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7344 - acc: 0.4018 - val_loss: 1.8228 - val_acc: 0.3990\n",
            "Epoch 362/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7343 - acc: 0.4023 - val_loss: 1.8247 - val_acc: 0.3982\n",
            "Epoch 363/500\n",
            "131559/131559 [==============================] - 4s 31us/sample - loss: 1.7348 - acc: 0.4013 - val_loss: 1.8190 - val_acc: 0.3998\n",
            "Epoch 364/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7342 - acc: 0.4028 - val_loss: 1.8219 - val_acc: 0.3992\n",
            "Epoch 365/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7357 - acc: 0.4016 - val_loss: 1.8306 - val_acc: 0.3951\n",
            "Epoch 366/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7367 - acc: 0.4018 - val_loss: 1.8229 - val_acc: 0.3991\n",
            "Epoch 367/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7365 - acc: 0.4025 - val_loss: 1.8257 - val_acc: 0.3951\n",
            "Epoch 368/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7335 - acc: 0.4018 - val_loss: 1.8173 - val_acc: 0.3998\n",
            "Epoch 369/500\n",
            "131559/131559 [==============================] - 4s 31us/sample - loss: 1.7357 - acc: 0.4014 - val_loss: 1.8270 - val_acc: 0.3950\n",
            "Epoch 370/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7353 - acc: 0.4019 - val_loss: 1.8282 - val_acc: 0.3959\n",
            "Epoch 371/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7347 - acc: 0.4028 - val_loss: 1.8255 - val_acc: 0.3979\n",
            "Epoch 372/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7329 - acc: 0.4035 - val_loss: 1.8152 - val_acc: 0.3990\n",
            "Epoch 373/500\n",
            "131559/131559 [==============================] - 4s 31us/sample - loss: 1.7334 - acc: 0.4028 - val_loss: 1.8296 - val_acc: 0.3948\n",
            "Epoch 374/500\n",
            "131559/131559 [==============================] - 4s 31us/sample - loss: 1.7336 - acc: 0.4022 - val_loss: 1.8200 - val_acc: 0.3984\n",
            "Epoch 375/500\n",
            "131559/131559 [==============================] - 4s 31us/sample - loss: 1.7315 - acc: 0.4042 - val_loss: 1.8161 - val_acc: 0.3981\n",
            "Epoch 376/500\n",
            "131559/131559 [==============================] - 4s 31us/sample - loss: 1.7342 - acc: 0.4016 - val_loss: 1.8191 - val_acc: 0.3979\n",
            "Epoch 377/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7341 - acc: 0.4013 - val_loss: 1.8234 - val_acc: 0.3945\n",
            "Epoch 378/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7338 - acc: 0.4025 - val_loss: 1.8233 - val_acc: 0.3966\n",
            "Epoch 379/500\n",
            "131559/131559 [==============================] - 5s 35us/sample - loss: 1.7336 - acc: 0.4014 - val_loss: 1.8221 - val_acc: 0.3957\n",
            "Epoch 380/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7343 - acc: 0.4030 - val_loss: 1.8315 - val_acc: 0.3943\n",
            "Epoch 381/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7333 - acc: 0.4023 - val_loss: 1.8189 - val_acc: 0.3960\n",
            "Epoch 382/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7328 - acc: 0.4023 - val_loss: 1.8189 - val_acc: 0.3978\n",
            "Epoch 383/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7339 - acc: 0.4009 - val_loss: 1.8144 - val_acc: 0.3999\n",
            "Epoch 384/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7326 - acc: 0.4029 - val_loss: 1.8191 - val_acc: 0.3990\n",
            "Epoch 385/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7340 - acc: 0.4025 - val_loss: 1.8166 - val_acc: 0.4003\n",
            "Epoch 386/500\n",
            "131559/131559 [==============================] - 4s 31us/sample - loss: 1.7328 - acc: 0.4034 - val_loss: 1.8321 - val_acc: 0.3944\n",
            "Epoch 387/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7339 - acc: 0.4021 - val_loss: 1.8184 - val_acc: 0.3992\n",
            "Epoch 388/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7352 - acc: 0.4005 - val_loss: 1.8230 - val_acc: 0.3987\n",
            "Epoch 389/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7316 - acc: 0.4031 - val_loss: 1.8265 - val_acc: 0.3971\n",
            "Epoch 390/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7342 - acc: 0.4022 - val_loss: 1.8184 - val_acc: 0.4012\n",
            "Epoch 391/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7339 - acc: 0.4022 - val_loss: 1.8241 - val_acc: 0.3956\n",
            "Epoch 392/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7324 - acc: 0.4015 - val_loss: 1.8213 - val_acc: 0.3996\n",
            "Epoch 393/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7344 - acc: 0.4014 - val_loss: 1.8291 - val_acc: 0.3955\n",
            "Epoch 394/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7353 - acc: 0.4018 - val_loss: 1.8314 - val_acc: 0.3921\n",
            "Epoch 395/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7344 - acc: 0.4012 - val_loss: 1.8277 - val_acc: 0.3927\n",
            "Epoch 396/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7345 - acc: 0.4023 - val_loss: 1.8158 - val_acc: 0.3982\n",
            "Epoch 397/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7334 - acc: 0.4023 - val_loss: 1.8235 - val_acc: 0.3994\n",
            "Epoch 398/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7329 - acc: 0.4022 - val_loss: 1.8200 - val_acc: 0.3977\n",
            "Epoch 399/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7354 - acc: 0.4030 - val_loss: 1.8326 - val_acc: 0.3952\n",
            "Epoch 400/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7323 - acc: 0.4033 - val_loss: 1.8187 - val_acc: 0.3983\n",
            "Epoch 401/500\n",
            "131559/131559 [==============================] - 4s 31us/sample - loss: 1.7333 - acc: 0.4037 - val_loss: 1.8191 - val_acc: 0.3998\n",
            "Epoch 402/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7336 - acc: 0.4026 - val_loss: 1.8201 - val_acc: 0.3991\n",
            "Epoch 403/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7328 - acc: 0.4037 - val_loss: 1.8328 - val_acc: 0.3948\n",
            "Epoch 404/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7342 - acc: 0.4012 - val_loss: 1.8144 - val_acc: 0.4014\n",
            "Epoch 405/500\n",
            "131559/131559 [==============================] - 4s 31us/sample - loss: 1.7335 - acc: 0.4022 - val_loss: 1.8298 - val_acc: 0.3950\n",
            "Epoch 406/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7329 - acc: 0.4033 - val_loss: 1.8162 - val_acc: 0.3991\n",
            "Epoch 407/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7310 - acc: 0.4034 - val_loss: 1.8249 - val_acc: 0.3964\n",
            "Epoch 408/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7315 - acc: 0.4021 - val_loss: 1.8306 - val_acc: 0.3965\n",
            "Epoch 409/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7332 - acc: 0.4020 - val_loss: 1.8129 - val_acc: 0.4006\n",
            "Epoch 410/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7313 - acc: 0.4029 - val_loss: 1.8036 - val_acc: 0.4047\n",
            "Epoch 411/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7305 - acc: 0.4026 - val_loss: 1.8309 - val_acc: 0.3925\n",
            "Epoch 412/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7325 - acc: 0.4040 - val_loss: 1.8247 - val_acc: 0.4004\n",
            "Epoch 413/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7336 - acc: 0.4019 - val_loss: 1.8284 - val_acc: 0.3932\n",
            "Epoch 414/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7319 - acc: 0.4042 - val_loss: 1.8195 - val_acc: 0.3963\n",
            "Epoch 415/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7322 - acc: 0.4030 - val_loss: 1.8151 - val_acc: 0.4009\n",
            "Epoch 416/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7334 - acc: 0.4035 - val_loss: 1.8258 - val_acc: 0.3982\n",
            "Epoch 417/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7315 - acc: 0.4029 - val_loss: 1.8211 - val_acc: 0.3978\n",
            "Epoch 418/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7310 - acc: 0.4025 - val_loss: 1.8261 - val_acc: 0.3969\n",
            "Epoch 419/500\n",
            "131559/131559 [==============================] - 4s 31us/sample - loss: 1.7331 - acc: 0.4023 - val_loss: 1.8216 - val_acc: 0.3990\n",
            "Epoch 420/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7319 - acc: 0.4025 - val_loss: 1.8291 - val_acc: 0.3949\n",
            "Epoch 421/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7333 - acc: 0.4018 - val_loss: 1.8231 - val_acc: 0.3988\n",
            "Epoch 422/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7318 - acc: 0.4029 - val_loss: 1.8222 - val_acc: 0.3973\n",
            "Epoch 423/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7311 - acc: 0.4026 - val_loss: 1.8265 - val_acc: 0.3979\n",
            "Epoch 424/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7326 - acc: 0.4027 - val_loss: 1.8219 - val_acc: 0.3969\n",
            "Epoch 425/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7314 - acc: 0.4030 - val_loss: 1.8187 - val_acc: 0.4002\n",
            "Epoch 426/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7311 - acc: 0.4038 - val_loss: 1.8384 - val_acc: 0.3913\n",
            "Epoch 427/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7318 - acc: 0.4035 - val_loss: 1.8156 - val_acc: 0.3998\n",
            "Epoch 428/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7335 - acc: 0.4024 - val_loss: 1.8415 - val_acc: 0.3897\n",
            "Epoch 429/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7308 - acc: 0.4026 - val_loss: 1.8259 - val_acc: 0.3978\n",
            "Epoch 430/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7330 - acc: 0.4024 - val_loss: 1.8266 - val_acc: 0.3974\n",
            "Epoch 431/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7312 - acc: 0.4029 - val_loss: 1.8123 - val_acc: 0.3998\n",
            "Epoch 432/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7326 - acc: 0.4028 - val_loss: 1.8204 - val_acc: 0.3972\n",
            "Epoch 433/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7309 - acc: 0.4034 - val_loss: 1.8265 - val_acc: 0.3982\n",
            "Epoch 434/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7332 - acc: 0.4035 - val_loss: 1.8164 - val_acc: 0.3988\n",
            "Epoch 435/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7309 - acc: 0.4030 - val_loss: 1.8206 - val_acc: 0.3987\n",
            "Epoch 436/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7331 - acc: 0.4018 - val_loss: 1.8327 - val_acc: 0.3972\n",
            "Epoch 437/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7324 - acc: 0.4043 - val_loss: 1.8217 - val_acc: 0.4012\n",
            "Epoch 438/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7323 - acc: 0.4035 - val_loss: 1.8312 - val_acc: 0.3946\n",
            "Epoch 439/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7319 - acc: 0.4016 - val_loss: 1.8230 - val_acc: 0.3978\n",
            "Epoch 440/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7339 - acc: 0.4017 - val_loss: 1.8315 - val_acc: 0.3957\n",
            "Epoch 441/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7312 - acc: 0.4027 - val_loss: 1.8143 - val_acc: 0.3989\n",
            "Epoch 442/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7293 - acc: 0.4030 - val_loss: 1.8166 - val_acc: 0.3998\n",
            "Epoch 443/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7304 - acc: 0.4034 - val_loss: 1.8349 - val_acc: 0.3944\n",
            "Epoch 444/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7309 - acc: 0.4025 - val_loss: 1.8277 - val_acc: 0.3978\n",
            "Epoch 445/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7319 - acc: 0.4026 - val_loss: 1.8166 - val_acc: 0.3988\n",
            "Epoch 446/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7319 - acc: 0.4026 - val_loss: 1.8189 - val_acc: 0.3994\n",
            "Epoch 447/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7306 - acc: 0.4041 - val_loss: 1.8143 - val_acc: 0.4030\n",
            "Epoch 448/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7313 - acc: 0.4031 - val_loss: 1.8335 - val_acc: 0.3955\n",
            "Epoch 449/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7305 - acc: 0.4045 - val_loss: 1.8327 - val_acc: 0.3975\n",
            "Epoch 450/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7315 - acc: 0.4035 - val_loss: 1.8150 - val_acc: 0.3973\n",
            "Epoch 451/500\n",
            "131559/131559 [==============================] - 5s 34us/sample - loss: 1.7307 - acc: 0.4032 - val_loss: 1.8210 - val_acc: 0.3993\n",
            "Epoch 452/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7334 - acc: 0.4042 - val_loss: 1.8263 - val_acc: 0.3953\n",
            "Epoch 453/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7322 - acc: 0.4028 - val_loss: 1.8149 - val_acc: 0.3988\n",
            "Epoch 454/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7308 - acc: 0.4038 - val_loss: 1.8208 - val_acc: 0.3973\n",
            "Epoch 455/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7310 - acc: 0.4029 - val_loss: 1.8220 - val_acc: 0.3962\n",
            "Epoch 456/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7315 - acc: 0.4031 - val_loss: 1.8190 - val_acc: 0.3979\n",
            "Epoch 457/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7319 - acc: 0.4028 - val_loss: 1.8203 - val_acc: 0.3982\n",
            "Epoch 458/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7321 - acc: 0.4040 - val_loss: 1.8273 - val_acc: 0.3994\n",
            "Epoch 459/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7316 - acc: 0.4044 - val_loss: 1.8272 - val_acc: 0.3982\n",
            "Epoch 460/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7319 - acc: 0.4024 - val_loss: 1.8188 - val_acc: 0.4016\n",
            "Epoch 461/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7286 - acc: 0.4041 - val_loss: 1.8215 - val_acc: 0.3984\n",
            "Epoch 462/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7323 - acc: 0.4029 - val_loss: 1.8197 - val_acc: 0.3973\n",
            "Epoch 463/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7311 - acc: 0.4043 - val_loss: 1.8343 - val_acc: 0.3946\n",
            "Epoch 464/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7312 - acc: 0.4051 - val_loss: 1.8395 - val_acc: 0.3937\n",
            "Epoch 465/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7297 - acc: 0.4047 - val_loss: 1.8186 - val_acc: 0.4011\n",
            "Epoch 466/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7286 - acc: 0.4051 - val_loss: 1.8225 - val_acc: 0.3965\n",
            "Epoch 467/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7325 - acc: 0.4038 - val_loss: 1.8158 - val_acc: 0.4002\n",
            "Epoch 468/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7291 - acc: 0.4041 - val_loss: 1.8297 - val_acc: 0.3970\n",
            "Epoch 469/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7315 - acc: 0.4043 - val_loss: 1.8241 - val_acc: 0.3957\n",
            "Epoch 470/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7319 - acc: 0.4038 - val_loss: 1.8200 - val_acc: 0.3978\n",
            "Epoch 471/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7300 - acc: 0.4034 - val_loss: 1.8303 - val_acc: 0.3910\n",
            "Epoch 472/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7307 - acc: 0.4040 - val_loss: 1.8277 - val_acc: 0.3932\n",
            "Epoch 473/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7318 - acc: 0.4039 - val_loss: 1.8253 - val_acc: 0.3981\n",
            "Epoch 474/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7308 - acc: 0.4031 - val_loss: 1.8293 - val_acc: 0.3943\n",
            "Epoch 475/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7325 - acc: 0.4025 - val_loss: 1.8264 - val_acc: 0.3943\n",
            "Epoch 476/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7311 - acc: 0.4029 - val_loss: 1.8228 - val_acc: 0.3974\n",
            "Epoch 477/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7296 - acc: 0.4030 - val_loss: 1.8193 - val_acc: 0.4002\n",
            "Epoch 478/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7293 - acc: 0.4050 - val_loss: 1.8250 - val_acc: 0.4000\n",
            "Epoch 479/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7292 - acc: 0.4035 - val_loss: 1.8213 - val_acc: 0.3988\n",
            "Epoch 480/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7303 - acc: 0.4039 - val_loss: 1.8257 - val_acc: 0.3943\n",
            "Epoch 481/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7281 - acc: 0.4042 - val_loss: 1.8375 - val_acc: 0.3954\n",
            "Epoch 482/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7301 - acc: 0.4047 - val_loss: 1.8345 - val_acc: 0.3957\n",
            "Epoch 483/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7310 - acc: 0.4043 - val_loss: 1.8277 - val_acc: 0.3974\n",
            "Epoch 484/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7312 - acc: 0.4036 - val_loss: 1.8224 - val_acc: 0.3984\n",
            "Epoch 485/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7303 - acc: 0.4027 - val_loss: 1.8318 - val_acc: 0.3951\n",
            "Epoch 486/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7291 - acc: 0.4041 - val_loss: 1.8241 - val_acc: 0.3991\n",
            "Epoch 487/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7288 - acc: 0.4046 - val_loss: 1.8226 - val_acc: 0.3962\n",
            "Epoch 488/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7297 - acc: 0.4030 - val_loss: 1.8251 - val_acc: 0.3971\n",
            "Epoch 489/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7317 - acc: 0.4041 - val_loss: 1.8224 - val_acc: 0.4003\n",
            "Epoch 490/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7306 - acc: 0.4031 - val_loss: 1.8202 - val_acc: 0.3977\n",
            "Epoch 491/500\n",
            "131559/131559 [==============================] - 5s 34us/sample - loss: 1.7287 - acc: 0.4046 - val_loss: 1.8198 - val_acc: 0.3996\n",
            "Epoch 492/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7307 - acc: 0.4036 - val_loss: 1.8274 - val_acc: 0.3939\n",
            "Epoch 493/500\n",
            "131559/131559 [==============================] - 5s 34us/sample - loss: 1.7305 - acc: 0.4027 - val_loss: 1.8158 - val_acc: 0.3994\n",
            "Epoch 494/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7309 - acc: 0.4029 - val_loss: 1.8211 - val_acc: 0.3986\n",
            "Epoch 495/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7315 - acc: 0.4023 - val_loss: 1.8232 - val_acc: 0.3974\n",
            "Epoch 496/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7303 - acc: 0.4047 - val_loss: 1.8240 - val_acc: 0.3955\n",
            "Epoch 497/500\n",
            "131559/131559 [==============================] - 4s 34us/sample - loss: 1.7319 - acc: 0.4041 - val_loss: 1.8293 - val_acc: 0.3939\n",
            "Epoch 498/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7288 - acc: 0.4037 - val_loss: 1.8213 - val_acc: 0.3978\n",
            "Epoch 499/500\n",
            "131559/131559 [==============================] - 4s 32us/sample - loss: 1.7301 - acc: 0.4055 - val_loss: 1.8108 - val_acc: 0.4011\n",
            "Epoch 500/500\n",
            "131559/131559 [==============================] - 4s 33us/sample - loss: 1.7301 - acc: 0.4058 - val_loss: 1.8242 - val_acc: 0.3987\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBzYKigwyTbL",
        "colab_type": "code",
        "outputId": "15e5ac2d-383c-4327-cf35-b8d99839210b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# final model accuracy score.\n",
        "scores = model.evaluate(X_scaled, y_train)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "164449/164449 [==============================] - 9s 55us/sample - loss: 1.6750 - acc: 0.4241\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31wXbvqDuif_",
        "colab_type": "code",
        "outputId": "6c7b3059-0507-4ba8-f5b5-104706202136",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "# plot the model loss.\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
        "ax.plot(np.sqrt(history.history['loss']), 'r', label='train')\n",
        "ax.plot(np.sqrt(history.history['val_loss']), 'b' ,label='val')\n",
        "ax.set_xlabel(r'Epoch', fontsize=20)\n",
        "ax.set_ylabel(r'Loss', fontsize=20)\n",
        "ax.legend()\n",
        "ax.tick_params(labelsize=20)\n",
        "\n",
        "# plot the model accuracy.\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
        "ax.plot(np.sqrt(history.history['acc']), 'r', label='train')\n",
        "ax.plot(np.sqrt(history.history['val_acc']), 'b' ,label='val')\n",
        "ax.set_xlabel(r'Epoch', fontsize=20)\n",
        "ax.set_ylabel(r'Accuracy', fontsize=20)\n",
        "ax.legend()\n",
        "ax.tick_params(labelsize=20)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoYAAAGFCAYAAABg9jJKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3SYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAocAAAGJCAYAAAD49pkPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEmS1jMcSpIkaT3DoSRJktb7f5b+b0UGfv7SAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}